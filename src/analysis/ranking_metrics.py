#!/usr/bin/env python
# coding: utf-8

"""
ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€æ§˜ã€…ãªå…¬å¹³æ€§æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
ç‰¹ã«éœ²å‡ºåº¦ï¼ˆExposureï¼‰ã€æ©Ÿä¼šå‡ç­‰æ€§ï¼ˆEqual Opportunityï¼‰ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°å®‰å®šæ€§ãªã©ã®
æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
"""

import os
import json
import collections
import csv
import datetime
import argparse
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import trange, tqdm
from dotenv import load_dotenv
import boto3
from src.utils.storage_utils import save_json, get_s3_client, ensure_dir, get_today_str, load_json
from src.utils.rank_utils import compute_tau, rbo
from src.utils.metrics_utils import gini_coefficient, statistical_parity_gap, equal_opportunity_ratio

# ãƒ‰ãƒ¡ã‚¤ãƒ³é–¢é€£ã®æ©Ÿèƒ½
from src.utils import extract_domain
from src.categories import get_categories

import re
import unicodedata

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# -----------------------------
# 0. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# -----------------------------
TOP_K        = 5                       # ä¸Šä½ k ä½ã‚’å¯¾è±¡ã¨ã™ã‚‹
EXPOSURE_WTS = {1: 5, 2: 4, 3: 3, 4: 2, 5: 1}      # 1ä½=5pt, 2ä½=4pt, 3ä½=3pt, 4ä½=2pt, 5ä½=1pt

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—
AWS_ACCESS_KEY = os.environ.get("AWS_ACCESS_KEY")
AWS_SECRET_KEY = os.environ.get("AWS_SECRET_KEY")
AWS_REGION = os.environ.get("AWS_REGION", "ap-northeast-1")
S3_BUCKET_NAME = os.environ.get("S3_BUCKET_NAME")

# ã‚«ãƒ†ã‚´ãƒªåˆ¥å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
def load_market_shares():
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰
    default_market_shares = {
        "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹": {
            "AWS": 0.32, "Azure": 0.23, "Google Cloud": 0.10,
            "IBM Cloud": 0.04, "Oracle Cloud": 0.03
        },
        "æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³": {
            "Google": 0.85, "Bing": 0.07, "Yahoo! Japan": 0.03, "Baidu": 0.01,
            "DuckDuckGo": 0.01
        }
    }

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
    market_shares_path = "src/data/market_shares.json"
    try:
        if os.path.exists(market_shares_path):
            with open(market_shares_path, "r", encoding="utf-8") as f:
                market_shares = json.load(f)
                print(f"å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ {market_shares_path} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                return market_shares
    except Exception as e:
        print(f"å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
    return default_market_shares

# å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
_MARKET_SHARES_CACHE = None

def get_market_shares():
    """å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    global _MARKET_SHARES_CACHE
    if _MARKET_SHARES_CACHE is None:
        _MARKET_SHARES_CACHE = load_market_shares()
    return _MARKET_SHARES_CACHE

# -----------------------------
# S3æ“ä½œãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------
def download_from_s3(s3_key):
    """S3ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å†…å®¹ã‚’è¿”ã™"""
    s3_client = get_s3_client()

    try:
        response = s3_client.get_object(
            Bucket=S3_BUCKET_NAME,
            Key=s3_key
        )

        content = response['Body'].read().decode('utf-8')
        return content
    except Exception as e:
        print(f"S3ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ ({s3_key}): {e}")
        return None

# -----------------------------
# 1. ä¸Šä½å‡ºç¾ç¢ºç‡ã¨éœ²å‡ºåº¦
# -----------------------------
def topk_probabilities(runs: list[list[str]], k: int = TOP_K):
    """å„ãƒ©ãƒ³ãŒä¸Šä½kä½å†…ã«ç¾ã‚Œã‚‹ç¢ºç‡ã‚’è¨ˆç®—"""
    counter = collections.Counter()
    for lst in runs:
        counter.update(lst[:k])
    n_runs = len(runs)
    return {c: counter[c] / n_runs for c in counter}

def exposure_index(runs: list[list[str]], wts=EXPOSURE_WTS):
    """å„ãƒ©ãƒ³ã‚¯ã®éœ²å‡ºåº¦æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆãƒ©ãƒ³ã‚¯é‡ã¿ä»˜ãï¼‰"""
    expo = collections.Counter()
    for lst in runs:
        for rank, name in enumerate(lst, 1):
            if rank in wts:
                expo[name] += wts[rank]
    total = sum(expo.values()) or 1
    return {c: expo[c] / total for c in expo}

def rank_distribution(runs: list[list[str]], max_rank: int = 5):
    """å„ãƒ©ãƒ³ã‚¯ã®é †ä½åˆ†å¸ƒã‚’è¨ˆç®—"""
    # å…¨ã¦ã®ã‚µãƒ¼ãƒ“ã‚¹åã‚’æŠ½å‡º
    all_services = set()
    for run in runs:
        all_services.update(run[:max_rank])

    # çµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸ã‚’åˆæœŸåŒ–
    dist = {service: [0.0] * max_rank for service in all_services}

    # å„ãƒ©ãƒ³ã‚¯ã®é †ä½åˆ†å¸ƒã‚’è¨ˆç®—
    n_runs = len(runs)
    for run in runs:
        for rank, service in enumerate(run[:max_rank]):
            if rank < max_rank:
                dist[service][rank] += 1 / n_runs

    return dist

# -----------------------------
# 2. æŒ‡æ¨™ (SPã€EOã€Correlationã€Gini)
# -----------------------------
def kendall_tau_correlation(ranked_runs, market_share):
    """Kendallã®Ï„é †ä½ç›¸é–¢ä¿‚æ•°ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨å¸‚å ´ã‚·ã‚§ã‚¢ã®ç›¸é–¢æ€§ï¼‰"""
    # å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨ˆç®—
    rank_counts = collections.defaultdict(list)
    for run in ranked_runs:
        for rank, service in enumerate(run):
            rank_counts[service].append(rank)

    avg_ranks = {service: np.mean(ranks) for service, ranks in rank_counts.items()}

    # å…±é€šã®ã‚µãƒ¼ãƒ“ã‚¹ã®ã¿ã‚’æŠ½å‡º
    common_services = set(avg_ranks.keys()) & set(market_share.keys())
    if len(common_services) < 2:
        return 0.0  # ç›¸é–¢ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒå¿…è¦

    # é †ä½ã¨å¸‚å ´ã‚·ã‚§ã‚¢ã®ãƒšã‚¢ã‚’ä½œæˆ
    x = [avg_ranks[service] for service in common_services]
    y = [-market_share[service] for service in common_services]  # ã‚·ã‚§ã‚¢ãŒå¤§ãã„ã»ã©é †ä½ãŒä¸Šä½ãªã®ã§è² å·

    # Kendallã®Ï„ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
    tau, _ = stats.kendalltau(x, y)
    return tau

def calculate_ranking_stability(rankings):
    """
    ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã®å®‰å®šæ€§ã‚’è¨ˆç®—

    Parameters
    ----------
    rankings : list[list[str]]
        è¤‡æ•°å›ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ

    Returns
    -------
    dict
        stability_score: å¹³å‡å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆ-1ã€œ1ï¼‰
        pairwise_stability: ãƒšã‚¢é–“ã®å®‰å®šæ€§ã‚¹ã‚³ã‚¢
        stability_matrix: å…¨ãƒšã‚¢é–“ã®å®‰å®šæ€§è¡Œåˆ—
    """
    if len(rankings) <= 1:
        return {
            "stability_score": 1.0,
            "pairwise_stability": [],
            "stability_matrix": None
        }

    # å…¨ã¦ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«å«ã¾ã‚Œã‚‹ã‚µãƒ¼ãƒ“ã‚¹åã‚’åé›†
    all_services = set()
    for ranking in rankings:
        all_services.update(ranking)

    # å„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ã‚µãƒ¼ãƒ“ã‚¹â†’é †ä½ã®ãƒãƒƒãƒ—ã«å¤‰æ›
    rank_maps = []
    for ranking in rankings:
        rank_map = {service: idx for idx, service in enumerate(ranking)}
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«å«ã¾ã‚Œãªã„ã‚µãƒ¼ãƒ“ã‚¹ã«ã¯å¤§ãã„é †ä½ã‚’ä»®å®š
        for service in all_services:
            if service not in rank_map:
                rank_map[service] = len(ranking)
        rank_maps.append(rank_map)

    # ãƒšã‚¢é–“ã®å®‰å®šæ€§ã‚’è¨ˆç®—
    n = len(rankings)
    stability_matrix = np.ones((n, n))
    pairwise_stability = []

    for i in range(n):
        for j in range(i+1, n):
            # å…±é€šã®ã‚µãƒ¼ãƒ“ã‚¹åã®ã¿ã‚’æŠ½å‡º
            common_services = set(rank_maps[i].keys()) & set(rank_maps[j].keys())
            if len(common_services) < 2:
                tau = 0.0  # å…±é€šã‚µãƒ¼ãƒ“ã‚¹ãŒ1å€‹ä»¥ä¸‹ã®å ´åˆç›¸é–¢ã‚’è¨ˆç®—ã§ããªã„
            else:
                ranks_i = [rank_maps[i][s] for s in common_services]
                ranks_j = [rank_maps[j][s] for s in common_services]
                tau, _ = stats.kendalltau(ranks_i, ranks_j)
                if np.isnan(tau):  # NANã®å ´åˆ0ã¨ã—ã¦æ‰±ã†
                    tau = 0.0

            stability_matrix[i, j] = tau
            stability_matrix[j, i] = tau
            pairwise_stability.append(tau)

    # å¹³å‡å®‰å®šæ€§ã‚¹ã‚³ã‚¢
    avg_stability = np.mean(pairwise_stability) if pairwise_stability else 1.0

    return {
        "stability_score": avg_stability,
        "pairwise_stability": pairwise_stability,
        "stability_matrix": stability_matrix
    }

def interpret_stability(score):
    """å®‰å®šæ€§ã‚¹ã‚³ã‚¢ã®è§£é‡ˆ"""
    if score >= 0.8:
        return "éå¸¸ã«å®‰å®š"
    elif score >= 0.6:
        return "å®‰å®š"
    elif score >= 0.4:
        return "ã‚„ã‚„å®‰å®š"
    elif score >= 0.2:
        return "ã‚„ã‚„ä¸å®‰å®š"
    elif score >= 0:
        return "ä¸å®‰å®š"
    else:
        return "éå¸¸ã«ä¸å®‰å®šï¼ˆé€†ç›¸é–¢ï¼‰"

# -----------------------------
# 3. é›†è¨ˆãƒ»ä¿å­˜
# -----------------------------
def compute_rank_metrics(category: str,
                         ranked_runs: list[list[str]],
                         market_share: dict[str, float] = None):
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‹ã‚‰å„æŒ‡æ¨™ã‚’è¨ˆç®—"""
    # ã‚µãƒ¼ãƒ“ã‚¹ã®ä¸€è¦§ã‚’æŠ½å‡º
    services = set()
    for run in ranked_runs:
        services.update(run)

    # å¸‚å ´ã‚·ã‚§ã‚¢ãŒæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯å‡ç­‰é…åˆ†
    if market_share is None:
        market_share = {service: 1.0 / len(services) for service in services}

    # ä¸Šä½ç¢ºç‡ã¨éœ²å‡ºåº¦æŒ‡æ¨™ã‚’è¨ˆç®—
    top_probs = topk_probabilities(ranked_runs, TOP_K)
    expo_idx = exposure_index(ranked_runs)
    rank_dist = rank_distribution(ranked_runs)

    # æŒ‡æ¨™ã‚’è¨ˆç®—
    sp_gap = statistical_parity_gap(top_probs)
    eo_dict, eo_gap = equal_opportunity_ratio(top_probs, market_share)
    ktau = kendall_tau_correlation(ranked_runs, market_share)

    # éœ²å‡ºåº¦ã®ã‚¸ãƒ‹ä¿‚æ•°ã‚’è¨ˆç®—
    gini = gini_coefficient(list(expo_idx.values()))

    # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    df = pd.DataFrame({
        "service": list(services),
        "top_k_prob": [top_probs.get(s, 0.0) for s in services],
        "exposure_idx": [expo_idx.get(s, 0.0) for s in services],
        "market_share": [market_share.get(s, 0.0) for s in services],
        "eo_ratio": [eo_dict.get(s, 0.0) for s in services],
    })

    # ãƒ©ãƒ³ã‚¯åˆ†å¸ƒã‚’è¿½åŠ 
    for rank in range(1, min(TOP_K, len(ranked_runs[0]) if ranked_runs else 1) + 1):
        df[f"rank_{rank}_prob"] = [rank_dist.get(s, [0.0] * TOP_K)[rank - 1] for s in services]

    # çµæœã‚’ã‚½ãƒ¼ãƒˆ
    df = df.sort_values("exposure_idx", ascending=False).reset_index(drop=True)

    # å®‰å®šæ€§ã‚’è¨ˆç®—
    stability_data = calculate_ranking_stability(ranked_runs)
    stability_score = stability_data["stability_score"]
    stability_interp = interpret_stability(stability_score)

    # é›†è¨ˆçŠ¶æ³ã‚’ä½œæˆ
    summary = {
        "category": category,
        "n_runs": len(ranked_runs),
        "n_services": len(services),
        "SP_gap": sp_gap,
        "EO_gap": eo_gap,
        "kendall_tau": ktau,
        "gini_coef": gini,
        "market_share": market_share,
        "exposure_idx": expo_idx,
        "stability_score": stability_score,
        "stability_interpretation": stability_interp
    }

    return df, summary

def plot_rank_distribution(df: pd.DataFrame, category: str, output_dir: str):
    """é †ä½åˆ†å¸ƒã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ãƒ—ãƒ­ãƒƒãƒˆ"""
    # é †ä½åˆ—ã‚’æŠ½å‡º
    rank_cols = [col for col in df.columns if col.startswith("rank_")]
    if not rank_cols:
        return None

    # ãƒ—ãƒ­ãƒƒãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    plot_df = df[["service"] + rank_cols].copy()
    plot_df = plot_df.set_index("service")

    # åˆ—åã‚’ã‚ã‹ã‚Šã‚„ã™ãå¤‰æ›´
    plot_df.columns = [f"{int(col.split('_')[1])}ä½" for col in rank_cols]

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
    plt.figure(figsize=(8, max(5, len(df) * 0.4)))
    sns.heatmap(plot_df, annot=True, cmap="YlGnBu", vmin=0, vmax=1, fmt=".2f")
    plt.title(f"{category}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†å¸ƒ")
    plt.tight_layout()

    # ä¿å­˜
    file_name = f"{category}_rank_heatmap.png"
    save_path = os.path.join(output_dir, file_name)
    plt.savefig(save_path, dpi=100)
    plt.close()

    return file_name

def plot_exposure_vs_market(df: pd.DataFrame, category: str, output_dir: str):
    """éœ²å‡ºåº¦ã¨å¸‚å ´ã‚·ã‚§ã‚¢ã®æ•£å¸ƒå›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    plt.figure(figsize=(8, 6))

    # å¯¾è§’ç·šï¼ˆå®Œå…¨å…¬å¹³ï¼‰
    max_val = max(df["exposure_idx"].max(), df["market_share"].max()) * 1.1
    plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.5, label="Perfect fairness")

    # æ•£å¸ƒå›³
    sc = plt.scatter(df["market_share"], df["exposure_idx"], s=80, alpha=0.7)

    # ã‚µãƒ¼ãƒ“ã‚¹åã®ãƒ©ãƒ™ãƒ«
    for i, row in df.iterrows():
        plt.annotate(row["service"], (row["market_share"], row["exposure_idx"]),
                    xytext=(5, 5), textcoords="offset points")

    plt.xlabel("å¸‚å ´ã‚·ã‚§ã‚¢")
    plt.ylabel("AIéœ²å‡ºåº¦æŒ‡æ¨™")
    plt.title(f"{category}ã®å¸‚å ´ã‚·ã‚§ã‚¢ã¨AIéœ²å‡ºåº¦ã®é–¢ä¿‚")
    plt.grid(alpha=0.3)
    plt.tight_layout()

    # ä¿å­˜
    file_name = f"{category}_exposure_market.png"
    save_path = os.path.join(output_dir, file_name)
    plt.savefig(save_path, dpi=100)
    plt.close()

    return file_name

def plot_stability_matrix(stability_matrix, category, output_dir):
    """å®‰å®šæ€§è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    if stability_matrix is None or len(stability_matrix) <= 1:
        return None

    plt.figure(figsize=(8, 6))
    sns.heatmap(stability_matrix, annot=True, cmap="YlGnBu", vmin=-1, vmax=1,
                fmt=".2f", square=True)
    plt.title(f"{category}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°å®‰å®šæ€§è¡Œåˆ—")
    plt.xlabel("å®Ÿè¡Œå›æ•°")
    plt.ylabel("å®Ÿè¡Œå›æ•°")
    plt.tight_layout()

    # ä¿å­˜
    file_name = f"{category}_stability_matrix.png"
    save_path = os.path.join(output_dir, file_name)
    plt.savefig(save_path, dpi=100)
    plt.close()

    return file_name

# -----------------------------
# 4. S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦åˆ†æ
# -----------------------------
def analyze_s3_rankings(date_str=None, api_type="perplexity", output_dir=None, upload_results=True, verbose=False):
    """
    S3ã‹ã‚‰æŒ‡å®šæ—¥ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦åˆ†æ

    Parameters
    ----------
    date_str : str, optional
        YYYYMMDDå½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—ã€æŒ‡å®šã—ãªã„å ´åˆã¯æœ€æ–°æ—¥ä»˜
    api_type : str, optional
        "perplexity" ã¾ãŸã¯ "openai"
    output_dir : str, optional
        å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€æŒ‡å®šã—ãªã„å ´åˆã¯ "results/perplexity_analysis/{date_str}"
    upload_results : bool, optional
        åˆ†æçµæœã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹
    verbose : bool, optional
        è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›ã‚’è¡Œã†ã‹

    Returns
    -------
    pd.DataFrame
        å…¨ã‚«ãƒ†ã‚´ãƒªã®é›†è¨ˆæŒ‡æ¨™
    """
    # è©³ç´°ãƒ­ã‚°ã®è¨­å®š
    if verbose:
        import logging
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        logger.info(f"è©³ç´°ãƒ­ã‚°ãƒ¢ãƒ¼ãƒ‰ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æã‚’å®Ÿè¡Œä¸­: {api_type}, æ—¥ä»˜: {date_str}")

    # æ—¥ä»˜ãŒæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ç”¨
    if date_str is None:
        date_str = datetime.datetime.now().strftime("%Y%m%d")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
    if output_dir is None:
        output_dir = f"results/perplexity_analysis/{date_str}"

    os.makedirs(output_dir, exist_ok=True)
    if verbose:
        logging.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {output_dir}")

    # S3ã‹ã‚‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    s3_key, content = get_latest_file(date_str, "rankings", api_type)

    if not content:
        print(f"âš ï¸  {date_str}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        if verbose:
            logging.error(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {s3_key}")
        return None

    print(f"ğŸ“¥ S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—: {s3_key}")
    if verbose:
        logging.info(f"S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—: {s3_key}, ã‚µã‚¤ã‚º: {len(content)}ãƒã‚¤ãƒˆ")

    # JSONã‚’ãƒ‘ãƒ¼ã‚¹
    try:
        ranked_json = json.loads(content)
        if verbose:
            logging.info(f"JSONã‚’ãƒ‘ãƒ¼ã‚¹: {len(ranked_json)}å€‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’æ¤œå‡º")
    except json.JSONDecodeError as e:
        print(f"âš ï¸  JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        if verbose:
            logging.error(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None

    print(f"ğŸ” {len(ranked_json)}å€‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†æã—ã¾ã™")

    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®åˆ†æ
    summaries = []
    uploaded_files = []

    for category, data in ranked_json.items():
        print(f"- {category} åˆ†æä¸­...")
        if verbose:
            logging.info(f"ã‚«ãƒ†ã‚´ãƒªã®åˆ†æé–‹å§‹: {category}")

        # ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’ç¢ºèªï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‚’å–å¾—ï¼‰
        runs = []
        if isinstance(data, dict):
            # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã®å‡¦ç†
            for subcategory, subdata in data.items():
                if isinstance(subdata, dict):
                    if "all_rankings" in subdata:
                        # è¤‡æ•°å›å®Ÿè¡Œçµæœã®å ´åˆï¼ˆrecommendedï¼‰
                        runs.extend(subdata["all_rankings"])
                        if verbose:
                            logging.info(f"è¤‡æ•°å›å®Ÿè¡Œãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º: {len(subdata['all_rankings'])}å›åˆ†ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                    elif "ranking" in subdata:
                        # å˜ä¸€å®Ÿè¡Œçµæœã®å ´åˆ
                        runs.append(subdata["ranking"])
                        if verbose:
                            logging.info("å˜ä¸€å®Ÿè¡Œãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º")
                    elif "search_result_companies" in subdata:
                        # æ–°ã—ã„å½¢å¼ï¼ˆsearch_result_companiesã‚’ä½¿ç”¨ï¼‰
                        runs.append(subdata["search_result_companies"])
                        if verbose:
                            logging.info("search_result_companiesãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º")
                    else:
                        print(f"  âš ï¸  ä¸æ˜ãªè¾æ›¸å½¢å¼: {list(subdata.keys())}")
                        if verbose:
                            logging.warning(f"ä¸æ˜ãªè¾æ›¸å½¢å¼: {list(subdata.keys())}")
                        continue
                elif isinstance(subdata, list):
                    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãƒªã‚¹ãƒˆã®å ´åˆ
                    runs.extend(subdata)
                    if verbose:
                        logging.info(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒªã‚¹ãƒˆã‚’æ¤œå‡º: {len(subdata)}é …ç›®")
                else:
                    print(f"  âš ï¸  ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿å½¢å¼: {type(subdata)}")
                    if verbose:
                        logging.warning(f"ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿å½¢å¼: {type(subdata)}")
                    continue

            if not runs:
                print(f"  âš ï¸  æœ‰åŠ¹ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                if verbose:
                    logging.warning("æœ‰åŠ¹ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
        elif isinstance(data, list):
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãƒªã‚¹ãƒˆã®å ´åˆ
            runs = data
            if verbose:
                logging.info(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒªã‚¹ãƒˆã‚’æ¤œå‡º: {len(runs)}é …ç›®")
        else:
            print(f"  âš ï¸  ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿å½¢å¼: {type(data)}")
            if verbose:
                logging.warning(f"ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿å½¢å¼: {type(data)}")
            continue

        # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸå¸‚å ´ã‚·ã‚§ã‚¢ã‚’é¸æŠ
        market_share = get_market_shares().get(category, None)
        if verbose:
            if market_share:
                logging.info(f"å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨: {len(market_share)}ä¼æ¥­")
            else:
                logging.warning(f"'{category}'ã®å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

        # æŒ‡æ¨™è¨ˆç®—
        df_metrics, summary = compute_rank_metrics(category, runs, market_share)
        if verbose:
            logging.info(f"æŒ‡æ¨™è¨ˆç®—å®Œäº†: {len(df_metrics)}ä¼æ¥­ã®æŒ‡æ¨™ã‚’ç”Ÿæˆ")

        # çµæœã‚’ä¿å­˜
        csv_path = os.path.join(output_dir, f"{category}_rank_metrics.csv")
        df_metrics.to_csv(csv_path, index=False)
        uploaded_files.append((csv_path, f"results/perplexity_analysis/{date_str}/{category}_rank_metrics.csv", "text/csv"))
        if verbose:
            logging.info(f"CSVã‚’ä¿å­˜: {csv_path}")

        # å¯è¦–åŒ–
        heatmap_file = plot_rank_distribution(df_metrics, category, output_dir)
        if heatmap_file:
            uploaded_files.append((
                os.path.join(output_dir, heatmap_file),
                f"results/perplexity_analysis/{date_str}/{heatmap_file}",
                "image/png"
            ))
            if verbose:
                logging.info(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ: {heatmap_file}")

        scatter_file = plot_exposure_vs_market(df_metrics, category, output_dir)
        if scatter_file:
            uploaded_files.append((
                os.path.join(output_dir, scatter_file),
                f"results/perplexity_analysis/{date_str}/{scatter_file}",
                "image/png"
            ))
            if verbose:
                logging.info(f"æ•£å¸ƒå›³ã‚’ç”Ÿæˆ: {scatter_file}")

        # å®‰å®šæ€§è¡Œåˆ—ã®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆ2å›ä»¥ä¸Šå®Ÿè¡Œã—ãŸå ´åˆã®ã¿ï¼‰
        if len(runs) > 1:
            stability_data = calculate_ranking_stability(runs)
            stability_matrix = stability_data["stability_matrix"]
            stability_file = plot_stability_matrix(stability_matrix, category, output_dir)
            if stability_file:
                uploaded_files.append((
                    os.path.join(output_dir, stability_file),
                    f"results/perplexity_analysis/{date_str}/{stability_file}",
                    "image/png"
                ))
                if verbose:
                    logging.info(f"å®‰å®šæ€§è¡Œåˆ—ã‚’ç”Ÿæˆ: {stability_file}")

        # é›†è¨ˆã‚’åé›†
        summaries.append(summary)

    # é›†è¨ˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›ã—ã¦ä¿å­˜
    summary_df = pd.DataFrame(summaries)
    summary_path = os.path.join(output_dir, f"{date_str}_{api_type}_rank_summary.csv")
    summary_df.to_csv(summary_path, index=False)
    uploaded_files.append((
        summary_path,
        f"results/perplexity_analysis/{date_str}/{date_str}_{api_type}_rank_summary.csv",
        "text/csv"
    ))
    if verbose:
        logging.info(f"é›†è¨ˆã‚’ä¿å­˜: {summary_path}, {len(summaries)}ã‚«ãƒ†ã‚´ãƒª")

    # S3ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if upload_results and AWS_ACCESS_KEY and AWS_SECRET_KEY:
        print("ğŸ“¤ åˆ†æçµæœã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        if verbose:
            logging.info("S3ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        for local_path, s3_key, content_type in uploaded_files:
            if upload_to_s3(local_path, s3_key, content_type):
                print(f"  âœ… {s3_key}")
                if verbose:
                    logging.info(f"S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {s3_key}")
            else:
                print(f"  âŒ {s3_key}")
                if verbose:
                    logging.error(f"S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {s3_key}")

    print(f"âœ… ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ: {output_dir}")
    if verbose:
        logging.info(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æå®Œäº†: {len(summary_df)}ã‚«ãƒ†ã‚´ãƒª")

    # é›†è¨ˆã‚’è¡¨ç¤º
    print("\n=== ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æã®é›†è¨ˆ ===")
    display_cols = ['category', 'SP_gap', 'EO_gap', 'kendall_tau', 'gini_coef', 'stability_score', 'stability_interpretation']
    display_cols = [col for col in display_cols if col in summary_df.columns]
    print(summary_df[display_cols])

    return summary_df

def get_exposure_market_data(category):
    """éœ²å‡ºåº¦ã¨å¸‚å ´ã‚·ã‚§ã‚¢ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        # S3ã‹ã‚‰æœ€æ–°ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        s3_key, content = get_latest_file(get_today_str(), "rankings", "perplexity")
        if not content:
            print(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        rankings_data = json.loads(content)
        if category not in rankings_data:
            print(f"ã‚«ãƒ†ã‚´ãƒª '{category}' ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        runs = rankings_data[category]
        if not runs:
            print(f"ã‚«ãƒ†ã‚´ãƒª '{category}' ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
            return None

        # å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        market_share = get_market_shares().get(category)
        if not market_share:
            print(f"ã‚«ãƒ†ã‚´ãƒª '{category}' ã®å¸‚å ´ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        df_metrics, _ = compute_rank_metrics(category, runs, market_share)

        # æ—¥ä»˜ã‚’è¿½åŠ 
        df_metrics["date"] = get_today_str()

        return df_metrics

    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None

def get_timeseries_exposure_market_data(category):
    """æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®éœ²å‡ºåº¦ã¨å¸‚å ´ã‚·ã‚§ã‚¢ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’S3ã‹ã‚‰åé›†"""
    from src.utils.s3_utils import list_s3_files
    dfs = []
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®S3ã‚­ãƒ¼ä¸€è¦§ã‚’å–å¾—
    files = list_s3_files("results/rankings/")
    for key in files:
        # æ—¥ä»˜æŠ½å‡º
        date_match = re.search(r"rankings/([0-9]{8})_", key)
        if not date_match:
            continue
        date_str = date_match.group(1)
        s3_key, content = get_latest_file(date_str, "rankings", "perplexity")
        if not content:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {s3_key}")
            continue
        try:
            rankings_data = json.loads(content)
            if category not in rankings_data:
                print(f"ã‚«ãƒ†ã‚´ãƒª '{category}' ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                continue
            runs = rankings_data[category]
            market_share = get_market_shares().get(category)
            if not runs or not market_share:
                print(f"ã‚«ãƒ†ã‚´ãƒª '{category}' ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
                continue
            df_metrics, _ = compute_rank_metrics(category, runs, market_share)
            df_metrics["date"] = date_str
            dfs.append(df_metrics)
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            continue
    if dfs:
        return pd.concat(dfs, ignore_index=True)
    return None

# -----------------------------
# 5. CLI ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# -----------------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='S3ã‹ã‚‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦åˆ†æ')
    parser.add_argument('--date', help='åˆ†æã™ã‚‹æ—¥ä»˜ï¼ˆYYYYMMDDå½¢å¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä»Šæ—¥ï¼‰')
    parser.add_argument('--api', default='perplexity', choices=['perplexity', 'openai'],
                        help='APIã‚¿ã‚¤ãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: perplexityï¼‰')
    parser.add_argument('--output', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: results/perplexity_analysis/YYYYMMDDï¼‰')
    parser.add_argument('--no-upload', action='store_true', help='S3ã¸ã®çµæœã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--verbose', action='store_true', help='è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›ã‚’è¡Œã†')
    parser.add_argument('input_file', nargs='?', help='ãƒ­ãƒ¼ã‚«ãƒ«JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥åˆ†æã™ã‚‹å ´åˆã®ãƒ‘ã‚¹')

    args = parser.parse_args()

    if args.input_file:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®åˆ†æ
        with open(args.input_file, 'r', encoding='utf-8') as f:
            ranked_json = json.load(f)

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        if args.output is None:
            today = datetime.datetime.now().strftime("%Y%m%d")
            output_dir = f"results/perplexity_analysis/{today}"
        else:
            output_dir = args.output

        os.makedirs(output_dir, exist_ok=True)

        # å„ã‚«ãƒ†ã‚´ãƒªã®åˆ†æã¨çµæœä¿å­˜
        summaries = []
        for category, runs in ranked_json.items():
            market_share = get_market_shares().get(category, None)
            df_metrics, summary = compute_rank_metrics(category, runs, market_share)
            df_metrics.to_csv(f"{output_dir}/{category}_rank_metrics.csv", index=False)
            plot_rank_distribution(df_metrics, category, output_dir)
            plot_exposure_vs_market(df_metrics, category, output_dir)
            summaries.append(summary)

        pd.DataFrame(summaries).to_csv(f"{output_dir}/_rank_summary.csv", index=False)
        print(f"âœ… ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ: {output_dir}")
    else:
        # S3ã‹ã‚‰ã®åˆ†æ
        analyze_s3_rankings(
            date_str=args.date,
            api_type=args.api,
            output_dir=args.output,
            upload_results=not args.no_upload,
            verbose=args.verbose
        )