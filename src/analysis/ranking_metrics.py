#!/usr/bin/env python
# coding: utf-8

"""
ãƒ©ãƒ³ã‚­ãƒ³ã‚°æŒ‡æ¨™åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

AIãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‹ã‚‰å„ç¨®ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã€
å¸‚å ´ã‚·ã‚§ã‚¢ã¨ã®æ¯”è¼ƒã‚„çµ±è¨ˆçš„åˆ†æã‚’è¡Œã†ãŸã‚ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
S3ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¯¾å¿œã€‚
"""

import collections
import json
import datetime
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import kendalltau
from dotenv import load_dotenv
import boto3

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# -----------------------------
# 0. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# -----------------------------
TOP_K        = 3                       # ã€Œä¸Šä½ k ä½ã€ã‚’é™½æ€§æ‰±ã„
EXPOSURE_WTS = {1: 3, 2: 2, 3: 1}      # 1ä½=3pt, 2ä½=2pt, 3ä½=1pt

# AWS S3æ¥ç¶šæƒ…å ±
AWS_ACCESS_KEY = os.environ.get("AWS_ACCESS_KEY")
AWS_SECRET_KEY = os.environ.get("AWS_SECRET_KEY")
AWS_REGION = os.environ.get("AWS_REGION", "ap-northeast-1")
S3_BUCKET_NAME = os.environ.get("S3_BUCKET_NAME")

# ã‚«ãƒ†ã‚´ãƒªåˆ¥å¸‚å ´ã‚·ã‚§ã‚¢ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆï¼‰
MARKET_SHARES = {
    "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹": {
        "AWS": 0.32, "Azure": 0.23, "Google Cloud": 0.10,
        "IBM Cloud": 0.04, "Oracle Cloud": 0.03
    },
    "æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³": {
        "Google": 0.85, "Bing": 0.07, "Yahoo! Japan": 0.03, "Baidu": 0.01,
        "DuckDuckGo": 0.01
    }
    # ä»–ã®ã‚«ãƒ†ã‚´ãƒªã¯å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
}

# -----------------------------
# S3æ“ä½œãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------
def get_s3_client():
    """S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
    if not all([AWS_ACCESS_KEY, AWS_SECRET_KEY]):
        raise ValueError("AWSèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    return boto3.client(
        "s3",
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY,
        region_name=AWS_REGION
    )

def list_s3_files(prefix="results/perplexity_rankings/"):
    """S3ãƒã‚±ãƒƒãƒˆå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    s3_client = get_s3_client()

    try:
        response = s3_client.list_objects_v2(
            Bucket=S3_BUCKET_NAME,
            Prefix=prefix
        )

        if 'Contents' in response:
            return [item['Key'] for item in response['Contents']]
        return []
    except Exception as e:
        print(f"S3ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def download_from_s3(s3_key):
    """S3ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å†…å®¹ã‚’è¿”ã™"""
    s3_client = get_s3_client()

    try:
        response = s3_client.get_object(
            Bucket=S3_BUCKET_NAME,
            Key=s3_key
        )

        content = response['Body'].read().decode('utf-8')
        return content
    except Exception as e:
        print(f"S3ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ ({s3_key}): {e}")
        return None

def upload_to_s3(local_path, s3_key, content_type=None):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    s3_client = get_s3_client()

    extra_args = {}
    if content_type:
        extra_args['ContentType'] = content_type

    try:
        with open(local_path, 'rb') as file_data:
            s3_client.upload_fileobj(
                file_data,
                S3_BUCKET_NAME,
                s3_key,
                ExtraArgs=extra_args
            )
        return True
    except Exception as e:
        print(f"S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ ({local_path} -> {s3_key}): {e}")
        return False

def get_latest_ranking_file(date_str=None, prefix="results/perplexity_rankings/"):
    """
    æŒ‡å®šã—ãŸæ—¥ä»˜ã®æœ€æ–°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—

    Parameters
    ----------
    date_str : str, optional
        YYYYMMDDå½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—ã€æœªæŒ‡å®šæ™‚ã¯æœ€æ–°æ—¥ä»˜
    prefix : str, optional
        S3å†…ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹

    Returns
    -------
    tuple
        (s3_key, json_content) ã®ã‚¿ãƒ—ãƒ«ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ (None, None)
    """
    files = list_s3_files(prefix)

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    ranking_files = [f for f in files if f.endswith('_rankings.json') or f.endswith('_rankings_5runs.json')]

    if not ranking_files:
        print(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prefix}")
        return None, None

    # æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if date_str:
        ranking_files = [f for f in ranking_files if date_str in f]
        if not ranking_files:
            print(f"{date_str}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None, None

    # è¤‡æ•°å®Ÿè¡Œç‰ˆã‚’å„ªå…ˆçš„ã«é¸æŠï¼ˆã‚ˆã‚Šä¿¡é ¼æ€§ãŒé«˜ã„ãŸã‚ï¼‰
    multi_run_files = [f for f in ranking_files if '_5runs.json' in f]
    target_file = multi_run_files[0] if multi_run_files else ranking_files[0]

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å–å¾—
    content = download_from_s3(target_file)
    if content:
        return target_file, content

    return None, None

# -----------------------------
# 1. ä¸Šä½å‡ºç¾ç¢ºç‡ãƒ»éœ²å‡ºåº¦
# -----------------------------
def topk_probabilities(runs: list[list[str]], k: int = TOP_K):
    """å„ã‚µãƒ¼ãƒ“ã‚¹ãŒä¸Šä½kä½ä»¥å†…ã«ç¾ã‚Œã‚‹ç¢ºç‡ã‚’è¨ˆç®—"""
    counter = collections.Counter()
    for lst in runs:
        counter.update(lst[:k])
    n_runs = len(runs)
    return {c: counter[c] / n_runs for c in counter}

def exposure_index(runs: list[list[str]], wts=EXPOSURE_WTS):
    """å„ã‚µãƒ¼ãƒ“ã‚¹ã®éœ²å‡ºåº¦æŒ‡æ•°ã‚’è¨ˆç®—ï¼ˆãƒ©ãƒ³ã‚¯åˆ¥é‡ã¿ä»˜ã‘ã‚¹ã‚³ã‚¢ï¼‰"""
    expo = collections.Counter()
    for lst in runs:
        for rank, name in enumerate(lst, 1):
            if rank in wts:
                expo[name] += wts[rank]
    total = sum(expo.values()) or 1
    return {c: expo[c] / total for c in expo}

def rank_distribution(runs: list[list[str]], max_rank: int = 5):
    """å„ã‚µãƒ¼ãƒ“ã‚¹ã®é †ä½åˆ†å¸ƒã‚’è¨ˆç®—"""
    # å…¨ã¦ã®ã‚µãƒ¼ãƒ“ã‚¹åã‚’æŠ½å‡º
    all_services = set()
    for run in runs:
        all_services.update(run[:max_rank])

    # çµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸ã‚’åˆæœŸåŒ–
    dist = {service: [0.0] * max_rank for service in all_services}

    # å„ã‚µãƒ¼ãƒ“ã‚¹ã®é †ä½åˆ†å¸ƒã‚’è¨ˆç®—
    n_runs = len(runs)
    for run in runs:
        for rank, service in enumerate(run[:max_rank]):
            if rank < max_rank:
                dist[service][rank] += 1 / n_runs

    return dist

# -----------------------------
# 2. æŒ‡æ¨™ (SPãƒ»EOãƒ»Correlationãƒ»Gini)
# -----------------------------
def statistical_parity_gap(top_probs: dict[str, float]) -> float:
    """Statistical Parity Gap (æœ€å¤§éœ²å‡ºç¢ºç‡ã¨æœ€å°éœ²å‡ºç¢ºç‡ã®å·®)"""
    if not top_probs:
        return 0.0
    return max(top_probs.values()) - min(top_probs.values())

def equal_opportunity_ratio(top_probs: dict[str, float],
                           market_share: dict[str, float]):
    """ä¼æ¥­ã”ã¨ã® EO æ¯”ç‡ã¨æœ€å¤§ä¹–é›¢å€¤"""
    # å¸‚å ´ã‚·ã‚§ã‚¢ãŒæœªå®šç¾©ã®ã‚µãƒ¼ãƒ“ã‚¹ã«ã¯æœ€å°å€¤ã‚’è¨­å®š
    min_share = min(market_share.values()) / 10 if market_share else 1e-6
    eo = {c: top_probs[c] / market_share.get(c, min_share) for c in top_probs}

    # 1ã‹ã‚‰ã®æœ€å¤§ä¹–é›¢ã‚’è¨ˆç®—
    eo_gap = max(abs(v - 1) for v in eo.values()) if eo else 0

    return eo, eo_gap

def kendall_tau_correlation(ranked_runs: list[list[str]],
                          market_share: dict[str, float]):
    """Kendallã®ã‚¿ã‚¦é †ä½ç›¸é–¢ä¿‚æ•°ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨å¸‚å ´ã‚·ã‚§ã‚¢ã®ç›¸é–¢åº¦ï¼‰"""
    # å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨ˆç®—
    rank_counts = collections.defaultdict(list)
    for run in ranked_runs:
        for rank, service in enumerate(run):
            rank_counts[service].append(rank)

    avg_ranks = {service: np.mean(ranks) for service, ranks in rank_counts.items()}

    # å…±é€šã®ã‚µãƒ¼ãƒ“ã‚¹ã®ã¿ã‚’æŠ½å‡º
    common_services = set(avg_ranks.keys()) & set(market_share.keys())
    if len(common_services) < 2:
        return 0.0  # ç›¸é–¢ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒå¿…è¦

    # é †ä½ã¨å¸‚å ´ã‚·ã‚§ã‚¢ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    x = [avg_ranks[service] for service in common_services]
    y = [-market_share[service] for service in common_services]  # ã‚·ã‚§ã‚¢ã¯å¤§ãã„ã»ã©é †ä½ãŒä¸Šä½ãªã®ã§è² ã«ã™ã‚‹

    # Kendallã®ã‚¿ã‚¦ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
    tau, _ = kendalltau(x, y)
    return tau

def gini_coefficient(values: list[float]):
    """ã‚¸ãƒ‹ä¿‚æ•°ã®è¨ˆç®—ï¼ˆä¸å¹³ç­‰åº¦ã®æŒ‡æ¨™ï¼‰"""
    if not values or all(v == 0 for v in values):
        return 0.0

    # æ˜‡é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_values = sorted(values)
    n = len(sorted_values)

    # ç´¯ç©ã‚·ã‚§ã‚¢ã‚’è¨ˆç®—
    cumsum = np.cumsum(sorted_values)

    # ã‚¸ãƒ‹ä¿‚æ•°ã®è¨ˆç®—
    numerator = 2 * sum(i * val for i, val in enumerate(sorted_values, 1))
    denominator = n * sum(sorted_values)

    return numerator / denominator - (n + 1) / n

def hhi_index(market_shares: dict[str, float]):
    """HHI (Herfindahl-Hirschman Index) ã®è¨ˆç®—ï¼ˆå¸‚å ´é›†ä¸­åº¦ï¼‰"""
    return sum(share ** 2 for share in market_shares.values())

def calculate_ranking_stability(rankings):
    """
    ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã®å®‰å®šæ€§ã‚’è©•ä¾¡

    Parameters
    ----------
    rankings : list[list[str]]
        è¤‡æ•°å›ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ

    Returns
    -------
    dict
        stability_score: å¹³å‡å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆ-1ã€œ1ï¼‰
        pairwise_stability: ãƒšã‚¢ã”ã¨ã®å®‰å®šæ€§ã‚¹ã‚³ã‚¢
        stability_matrix: å…¨ãƒšã‚¢é–“ã®å®‰å®šæ€§è¡Œåˆ—
    """
    if len(rankings) <= 1:
        return {
            "stability_score": 1.0,
            "pairwise_stability": [],
            "stability_matrix": None
        }

    # å…¨ã¦ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«ç™»å ´ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹åã‚’åé›†
    all_services = set()
    for ranking in rankings:
        all_services.update(ranking)

    # å„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ã‚µãƒ¼ãƒ“ã‚¹â†’é †ä½ã®ãƒãƒƒãƒ—ã«å¤‰æ›
    rank_maps = []
    for ranking in rankings:
        rank_map = {service: idx for idx, service in enumerate(ranking)}
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«å«ã¾ã‚Œã¦ã„ãªã„ã‚µãƒ¼ãƒ“ã‚¹ã«ã¯å¤§ããªé †ä½ã‚’å‰²ã‚Šå½“ã¦
        for service in all_services:
            if service not in rank_map:
                rank_map[service] = len(ranking)
        rank_maps.append(rank_map)

    # ã™ã¹ã¦ã®ãƒšã‚¢é–“ã®å®‰å®šæ€§ã‚’è¨ˆç®—
    n = len(rankings)
    stability_matrix = np.ones((n, n))
    pairwise_stability = []

    for i in range(n):
        for j in range(i+1, n):
            # å…±é€šã®ã‚µãƒ¼ãƒ“ã‚¹åã®ã¿ã‚’æŠ½å‡º
            common_services = set(rank_maps[i].keys()) & set(rank_maps[j].keys())
            if len(common_services) < 2:
                tau = 0.0  # å…±é€šã‚µãƒ¼ãƒ“ã‚¹ãŒ1ã¤ä»¥ä¸‹ã®å ´åˆã¯ç›¸é–¢ã‚’è¨ˆç®—ã§ããªã„
            else:
                ranks_i = [rank_maps[i][s] for s in common_services]
                ranks_j = [rank_maps[j][s] for s in common_services]
                tau, _ = kendalltau(ranks_i, ranks_j)
                if np.isnan(tau):  # NANã®å ´åˆã¯0ã¨ã—ã¦æ‰±ã†
                    tau = 0.0

            stability_matrix[i, j] = tau
            stability_matrix[j, i] = tau
            pairwise_stability.append(tau)

    # å¹³å‡å®‰å®šæ€§ã‚¹ã‚³ã‚¢
    avg_stability = np.mean(pairwise_stability) if pairwise_stability else 1.0

    return {
        "stability_score": avg_stability,
        "pairwise_stability": pairwise_stability,
        "stability_matrix": stability_matrix
    }

def interpret_stability(score):
    """å®‰å®šæ€§ã‚¹ã‚³ã‚¢ã®è§£é‡ˆ"""
    if score >= 0.8:
        return "éå¸¸ã«å®‰å®š"
    elif score >= 0.6:
        return "å®‰å®š"
    elif score >= 0.4:
        return "ã‚„ã‚„å®‰å®š"
    elif score >= 0.2:
        return "ã‚„ã‚„ä¸å®‰å®š"
    elif score >= 0:
        return "ä¸å®‰å®š"
    else:
        return "éå¸¸ã«ä¸å®‰å®šï¼ˆé€†ç›¸é–¢ï¼‰"

# -----------------------------
# 3. é›†ç´„ï¼‹ä¿å­˜
# -----------------------------
def compute_rank_metrics(category: str,
                         ranked_runs: list[list[str]],
                         market_share: dict[str, float] = None):
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‹ã‚‰å„ç¨®æŒ‡æ¨™ã‚’è¨ˆç®—"""
    # ã‚µãƒ¼ãƒ“ã‚¹ã®ä¸€è¦§ã‚’æŠ½å‡º
    services = set()
    for run in ranked_runs:
        services.update(run)

    # å¸‚å ´ã‚·ã‚§ã‚¢ãŒæœªæŒ‡å®šã®å ´åˆã¯å‡ç­‰é…åˆ†
    if market_share is None:
        market_share = {service: 1.0 / len(services) for service in services}

    # ä¸Šä½ç¢ºç‡ã¨éœ²å‡ºåº¦æŒ‡æ•°ã‚’è¨ˆç®—
    top_probs = topk_probabilities(ranked_runs, TOP_K)
    expo_idx = exposure_index(ranked_runs)
    rank_dist = rank_distribution(ranked_runs)

    # æŒ‡æ¨™ã‚’è¨ˆç®—
    sp_gap = statistical_parity_gap(top_probs)
    eo_dict, eo_gap = equal_opportunity_ratio(top_probs, market_share)
    ktau = kendall_tau_correlation(ranked_runs, market_share)

    # éœ²å‡ºåº¦ã®ã‚¸ãƒ‹ä¿‚æ•°ã‚’è¨ˆç®—
    gini = gini_coefficient(list(expo_idx.values()))

    # HHIï¼ˆå¸‚å ´é›†ä¸­åº¦ï¼‰ã‚’è¨ˆç®—
    market_hhi = hhi_index(market_share)
    expo_hhi = hhi_index(expo_idx)

    # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    df = pd.DataFrame({
        "service": list(services),
        "top_k_prob": [top_probs.get(s, 0.0) for s in services],
        "exposure_idx": [expo_idx.get(s, 0.0) for s in services],
        "market_share": [market_share.get(s, 0.0) for s in services],
        "eo_ratio": [eo_dict.get(s, 0.0) for s in services],
    })

    # ãƒ©ãƒ³ã‚¯åˆ†å¸ƒã‚’è¿½åŠ 
    for rank in range(1, min(TOP_K, len(ranked_runs[0]) if ranked_runs else 1) + 1):
        df[f"rank_{rank}_prob"] = [rank_dist.get(s, [0.0] * TOP_K)[rank - 1] for s in services]

    # çµæœã‚’ã‚½ãƒ¼ãƒˆ
    df = df.sort_values("exposure_idx", ascending=False).reset_index(drop=True)

    # å®‰å®šæ€§ã‚’è¨ˆç®—
    stability_data = calculate_ranking_stability(ranked_runs)
    stability_score = stability_data["stability_score"]
    stability_interp = interpret_stability(stability_score)

    # çµ±è¨ˆæ¦‚è¦ã‚’ä½œæˆ
    summary = {
        "category": category,
        "n_runs": len(ranked_runs),
        "n_services": len(services),
        "SP_gap": sp_gap,
        "EO_gap": eo_gap,
        "kendall_tau": ktau,
        "gini_coef": gini,
        "market_hhi": market_hhi,
        "exposure_hhi": expo_hhi,
        "hhi_ratio": expo_hhi / market_hhi if market_hhi else float('inf'),
        "stability_score": stability_score,
        "stability_interpretation": stability_interp
    }

    return df, summary

def plot_rank_distribution(df: pd.DataFrame, category: str, output_dir: str):
    """é †ä½åˆ†å¸ƒã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ãƒ—ãƒ­ãƒƒãƒˆ"""
    # é †ä½åˆ—ã‚’æŠ½å‡º
    rank_cols = [col for col in df.columns if col.startswith("rank_")]
    if not rank_cols:
        return None

    # ãƒ—ãƒ­ãƒƒãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    plot_df = df[["service"] + rank_cols].copy()
    plot_df = plot_df.set_index("service")

    # åˆ—åã‚’ã‚ã‹ã‚Šã‚„ã™ãå¤‰æ›´
    plot_df.columns = [f"{int(col.split('_')[1])}ä½" for col in rank_cols]

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
    plt.figure(figsize=(8, max(5, len(df) * 0.4)))
    sns.heatmap(plot_df, annot=True, cmap="YlGnBu", vmin=0, vmax=1, fmt=".2f")
    plt.title(f"{category}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†å¸ƒ")
    plt.tight_layout()

    # ä¿å­˜
    file_name = f"{category}_rank_heatmap.png"
    save_path = os.path.join(output_dir, file_name)
    plt.savefig(save_path, dpi=100)
    plt.close()

    return file_name

def plot_exposure_vs_market(df: pd.DataFrame, category: str, output_dir: str):
    """éœ²å‡ºåº¦ã¨å¸‚å ´ã‚·ã‚§ã‚¢ã®æ•£å¸ƒå›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    plt.figure(figsize=(8, 6))

    # å¯¾è§’ç·šï¼ˆå®Œå…¨å…¬å¹³ãƒ©ã‚¤ãƒ³ï¼‰
    max_val = max(df["exposure_idx"].max(), df["market_share"].max()) * 1.1
    plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.5, label="Perfect fairness")

    # æ•£å¸ƒå›³
    sc = plt.scatter(df["market_share"], df["exposure_idx"], s=80, alpha=0.7)

    # ã‚µãƒ¼ãƒ“ã‚¹åã®ãƒ©ãƒ™ãƒ«
    for i, row in df.iterrows():
        plt.annotate(row["service"], (row["market_share"], row["exposure_idx"]),
                    xytext=(5, 5), textcoords="offset points")

    plt.xlabel("å¸‚å ´ã‚·ã‚§ã‚¢")
    plt.ylabel("AIéœ²å‡ºåº¦æŒ‡æ•°")
    plt.title(f"{category}ã®å¸‚å ´ã‚·ã‚§ã‚¢ã¨AIéœ²å‡ºåº¦ã®é–¢ä¿‚")
    plt.grid(alpha=0.3)
    plt.tight_layout()

    # ä¿å­˜
    file_name = f"{category}_exposure_market.png"
    save_path = os.path.join(output_dir, file_name)
    plt.savefig(save_path, dpi=100)
    plt.close()

    return file_name

def plot_stability_matrix(stability_matrix, category, output_dir):
    """å®‰å®šæ€§è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    if stability_matrix is None or len(stability_matrix) <= 1:
        return None

    plt.figure(figsize=(8, 6))
    sns.heatmap(stability_matrix, annot=True, cmap="YlGnBu", vmin=-1, vmax=1,
                fmt=".2f", square=True)
    plt.title(f"{category}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°å®‰å®šæ€§è¡Œåˆ—")
    plt.xlabel("å®Ÿè¡Œå›æ•°")
    plt.ylabel("å®Ÿè¡Œå›æ•°")
    plt.tight_layout()

    # ä¿å­˜
    file_name = f"{category}_stability_matrix.png"
    save_path = os.path.join(output_dir, file_name)
    plt.savefig(save_path, dpi=100)
    plt.close()

    return file_name

# -----------------------------
# 4. S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦åˆ†æ
# -----------------------------
def analyze_s3_rankings(date_str=None, api_type="perplexity", output_dir=None, upload_results=True):
    """
    S3ã‹ã‚‰æŒ‡å®šæ—¥ä»˜ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦åˆ†æ

    Parameters
    ----------
    date_str : str, optional
        YYYYMMDDå½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—ã€æœªæŒ‡å®šæ™‚ã¯æœ€æ–°æ—¥ä»˜
    api_type : str, optional
        "perplexity" ã¾ãŸã¯ "openai"
    output_dir : str, optional
        å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€æœªæŒ‡å®šæ™‚ã¯ "results/ranking_analysis/{date_str}"
    upload_results : bool, optional
        åˆ†æçµæœã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã©ã†ã‹

    Returns
    -------
    pd.DataFrame
        å…¨ã‚«ãƒ†ã‚´ãƒªã®æ¦‚è¦æŒ‡æ¨™
    """
    # æ—¥ä»˜ãŒæœªæŒ‡å®šã®å ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ç”¨
    if date_str is None:
        date_str = datetime.datetime.now().strftime("%Y%m%d")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
    if output_dir is None:
        output_dir = f"results/ranking_analysis/{date_str}"

    os.makedirs(output_dir, exist_ok=True)

    # S3ã‹ã‚‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    s3_prefix = f"results/{api_type}_rankings/{date_str}/"
    s3_key, json_content = get_latest_ranking_file(date_str, s3_prefix)

    if not json_content:
        print(f"âš ï¸ {date_str}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

    print(f"ğŸ“¥ S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—: {s3_key}")

    # JSONã‚’ãƒ‘ãƒ¼ã‚¹
    try:
        ranked_json = json.loads(json_content)
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None

    print(f"ğŸ” {len(ranked_json)}å€‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†æã—ã¾ã™")

    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«åˆ†æ
    summaries = []
    uploaded_files = []

    for category, data in ranked_json.items():
        print(f"- {category} åˆ†æä¸­...")

        # ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’ç¢ºèªï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‚’å–å¾—ï¼‰
        runs = []
        if isinstance(data, dict) and "all_rankings" in data:
            # è¤‡æ•°å®Ÿè¡Œçµæœã®å ´åˆï¼ˆrecommendedï¼‰
            runs = data["all_rankings"]
        elif isinstance(data, dict) and "ranking" in data:
            # å˜ä¸€å®Ÿè¡Œçµæœã®å ´åˆ
            runs = [data["ranking"]]
        elif isinstance(data, list):
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãƒªã‚¹ãƒˆã®å ´åˆ
            runs = data
        else:
            print(f"  âš ï¸ ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿å½¢å¼: {type(data)}")
            continue

        # ã‚«ãƒ†ã‚´ãƒªã«åˆã£ãŸå¸‚å ´ã‚·ã‚§ã‚¢ã‚’é¸æŠ
        market_share = MARKET_SHARES.get(category, None)

        # æŒ‡æ¨™è¨ˆç®—
        df_metrics, summary = compute_rank_metrics(category, runs, market_share)

        # çµæœã‚’ä¿å­˜
        csv_path = os.path.join(output_dir, f"{category}_rank_metrics.csv")
        df_metrics.to_csv(csv_path, index=False)
        uploaded_files.append((csv_path, f"results/ranking_analysis/{date_str}/{category}_rank_metrics.csv", "text/csv"))

        # å¯è¦–åŒ–
        heatmap_file = plot_rank_distribution(df_metrics, category, output_dir)
        if heatmap_file:
            uploaded_files.append((
                os.path.join(output_dir, heatmap_file),
                f"results/ranking_analysis/{date_str}/{heatmap_file}",
                "image/png"
            ))

        scatter_file = plot_exposure_vs_market(df_metrics, category, output_dir)
        if scatter_file:
            uploaded_files.append((
                os.path.join(output_dir, scatter_file),
                f"results/ranking_analysis/{date_str}/{scatter_file}",
                "image/png"
            ))

        # å®‰å®šæ€§è¡Œåˆ—ã®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆ2å›ä»¥ä¸Šå®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
        if len(runs) > 1:
            stability_data = calculate_ranking_stability(runs)
            stability_matrix = stability_data["stability_matrix"]
            stability_file = plot_stability_matrix(stability_matrix, category, output_dir)
            if stability_file:
                uploaded_files.append((
                    os.path.join(output_dir, stability_file),
                    f"results/ranking_analysis/{date_str}/{stability_file}",
                    "image/png"
                ))

        # æ¦‚è¦ã‚’é›†è¨ˆ
        summaries.append(summary)

    # æ¦‚è¦ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›ã—ã¦ä¿å­˜
    summary_df = pd.DataFrame(summaries)
    summary_path = os.path.join(output_dir, f"{date_str}_{api_type}_rank_summary.csv")
    summary_df.to_csv(summary_path, index=False)
    uploaded_files.append((
        summary_path,
        f"results/ranking_analysis/{date_str}/{date_str}_{api_type}_rank_summary.csv",
        "text/csv"
    ))

    # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if upload_results and AWS_ACCESS_KEY and AWS_SECRET_KEY:
        print("ğŸ“¤ åˆ†æçµæœã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        for local_path, s3_key, content_type in uploaded_files:
            if upload_to_s3(local_path, s3_key, content_type):
                print(f"  âœ“ {s3_key}")
            else:
                print(f"  âœ— {s3_key}")

    print(f"âœ… ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ: {output_dir}")

    # æ¦‚è¦ã‚’è¡¨ç¤º
    print("\n=== ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æã®æ¦‚è¦ ===")
    display_cols = ['category', 'SP_gap', 'EO_gap', 'kendall_tau', 'gini_coef', 'stability_score', 'stability_interpretation']
    display_cols = [col for col in display_cols if col in summary_df.columns]
    print(summary_df[display_cols])

    return summary_df

# -----------------------------
# 5. CLI ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# -----------------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='S3ã‹ã‚‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦åˆ†æ')
    parser.add_argument('--date', help='åˆ†æã™ã‚‹æ—¥ä»˜ï¼ˆYYYYMMDDå½¢å¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä»Šæ—¥ï¼‰')
    parser.add_argument('--api', default='perplexity', choices=['perplexity', 'openai'],
                        help='APIã‚¿ã‚¤ãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: perplexityï¼‰')
    parser.add_argument('--output', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: results/ranking_analysis/YYYYMMDDï¼‰')
    parser.add_argument('--no-upload', action='store_true', help='S3ã¸ã®çµæœã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--json-path', help='ãƒ­ãƒ¼ã‚«ãƒ«JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥åˆ†æã™ã‚‹å ´åˆã®ãƒ‘ã‚¹')

    args = parser.parse_args()

    if args.json_path:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®åˆ†æ
        with open(args.json_path, 'r', encoding='utf-8') as f:
            ranked_json = json.load(f)

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        if args.output is None:
            today = datetime.datetime.now().strftime("%Y%m%d")
            output_dir = f"results/ranking_analysis/{today}"
        else:
            output_dir = args.output

        os.makedirs(output_dir, exist_ok=True)

        # å„ã‚«ãƒ†ã‚´ãƒªã®åˆ†æã¨çµæœä¿å­˜
        summaries = []
        for category, runs in ranked_json.items():
            market_share = MARKET_SHARES.get(category, None)
            df_metrics, summary = compute_rank_metrics(category, runs, market_share)
            df_metrics.to_csv(f"{output_dir}/{category}_rank_metrics.csv", index=False)
            plot_rank_distribution(df_metrics, category, output_dir)
            plot_exposure_vs_market(df_metrics, category, output_dir)
            summaries.append(summary)

        pd.DataFrame(summaries).to_csv(f"{output_dir}/_rank_summary.csv", index=False)
        print(f"âœ… ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ: {output_dir}")
    else:
        # S3ã‹ã‚‰ã®åˆ†æ
        analyze_s3_rankings(
            date_str=args.date,
            api_type=args.api,
            output_dir=args.output,
            upload_results=not args.no_upload
        )