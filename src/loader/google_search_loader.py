#!/usr/bin/env python
# coding: utf-8

"""
Google Search Loader ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Google Custom Search APIã‚’ä½¿ã£ã¦Googleæ¤œç´¢çµæœã‚’å–å¾—ã—ã€Perplexityã®çµæœã¨æ¯”è¼ƒåˆ†æã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import os
import datetime
import requests
import time
import argparse
from dotenv import load_dotenv
from tqdm import tqdm
from ..utils import (
    extract_domain,
    get_results_paths
)
from ..utils.storage_utils import get_results_paths, save_results
from ..utils.storage_config import get_s3_key
from ..categories import get_categories, get_all_categories

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Google Custom Search API ã®è¨­å®š
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.environ.get("GOOGLE_CSE_ID")

# S3 è¨­å®šæƒ…å ±
AWS_ACCESS_KEY = os.environ.get("AWS_ACCESS_KEY")
AWS_SECRET_KEY = os.environ.get("AWS_SECRET_KEY")
AWS_REGION = os.environ.get("AWS_REGION", "ap-northeast-1")
S3_BUCKET_NAME = os.environ.get("S3_BUCKET_NAME")

# ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã®å®šç¾©èª­ã¿è¾¼ã¿
categories = get_categories()

# -------------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -------------------------------------------------------------------

# -------------------------------------------------------------------
# Google Custom Search API é–¢é€£
# -------------------------------------------------------------------
def get_google_search_results(query, num_results=10):
    """
    Google Custom Search APIã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹

    Parameters:
    -----------
    query : str
        æ¤œç´¢ã‚¯ã‚¨ãƒª
    num_results : int, optional
        å–å¾—ã™ã‚‹çµæœã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰

    Returns:
    --------
    dict
        æ¤œç´¢çµæœã®è¾æ›¸
    """
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
            raise ValueError("GOOGLE_API_KEY ã¾ãŸã¯ GOOGLE_CSE_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")

        # Google Custom Search APIã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        endpoint = "https://www.googleapis.com/customsearch/v1"

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        params = {
            "key": GOOGLE_API_KEY,
            "cx": GOOGLE_CSE_ID,
            "q": query,
            "num": num_results,
            "gl": "jp",  # æ—¥æœ¬å‘ã‘æ¤œç´¢
            "hl": "ja",   # æ—¥æœ¬èªçµæœ
            "lr": "lang_ja"  # æ—¥æœ¬èªãƒšãƒ¼ã‚¸é™å®š
        }

        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = requests.get(endpoint, params=params)

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
        if response.status_code == 429:
            print("âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚60ç§’å¾…æ©Ÿã—ã¾ã™...")
            time.sleep(60)  # 60ç§’å¾…æ©Ÿ
            response = requests.get(endpoint, params=params)  # å†è©¦è¡Œ

        response.raise_for_status()
        data = response.json()
        print(f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹: {data}")

        # æ¤œç´¢çµæœã‚’æ•´å½¢
        results = {
            "organic_results": []
        }

        if "items" in data:
            for item in data["items"]:
                results["organic_results"].append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "position": len(results["organic_results"]) + 1
                })
            print(f"âœ… æ¤œç´¢çµæœ: {len(results['organic_results'])}ä»¶")
        else:
            print("âš ï¸ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        return results

    except Exception as e:
        print(f"âŒ Google Custom Search API ã‚¨ãƒ©ãƒ¼: {e}")
        return {"organic_results": []}

def process_search_results(data):
    """Google Custom Search API ã®çµæœã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦æ•´å½¢"""
    if not data or "organic_results" not in data:
        return []
    organic_results = data["organic_results"]
    results = []
    for i, result in enumerate(organic_results):
        title = result.get("title", "")
        link = result.get("link", "")
        snippet = result.get("snippet", "")
        domain = extract_domain(link)
        result_dict = {
            "rank": i + 1,
            "title": title,
            "link": link,
            "domain": domain,
            "snippet": snippet
        }
        results.append(result_dict)
    return results

def process_categories_with_search(categories, max_categories=None):
    """ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«Googleæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€entitieså±æ€§ã®ä¸‹ã«æ ¼ç´"""
    results = {}
    count = 0
    if max_categories:
        filtered_categories = {}
        for toplevel, subcats in categories.items():
            filtered_categories[toplevel] = {}
            for subcat, values in subcats.items():
                if count < max_categories:
                    filtered_categories[toplevel][subcat] = values
                    count += 1
        categories_to_process = filtered_categories
    else:
        categories_to_process = categories

    for category, subcategories in categories_to_process.items():
        results[category] = {}
        for subcategory, services in tqdm(subcategories.items(), desc=f"å‡¦ç†ä¸­: {category}"):
            entities = {}
            for service in services:
                # å…¬å¼/éå…¬å¼åˆ¤å®šç”¨ã®æ¤œç´¢
                query = f"{service}"
                search_data = get_google_search_results(query, num_results=10)
                official_results = process_search_results(search_data) if search_data else []
                time.sleep(2)
                # è©•åˆ¤æƒ…å ±ç”¨ã®æ¤œç´¢
                query_rep = f"{service} è©•åˆ¤ å£ã‚³ãƒŸ"
                search_data_rep = get_google_search_results(query_rep, num_results=10)
                reputation_results = process_search_results(search_data_rep) if search_data_rep else []
                time.sleep(2)
                entities[service] = {
                    "official_results": official_results,
                    "reputation_results": reputation_results,
                    "official_query": query,
                    "reputation_query": query_rep
                }
            # çµæœã‚’æ ¼ç´
            results[category][subcategory] = {
                "timestamp": datetime.datetime.now().isoformat(),
                "category": category,
                "subcategory": subcategory,
                "entities": entities
            }
    return results

# -------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³é–¢æ•°
# -------------------------------------------------------------------
def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # å¼•æ•°å‡¦ç†ï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒã‚ã‚Œã°ä½¿ç”¨ï¼‰
    parser = argparse.ArgumentParser(description='Googleæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€Perplexityã¨ã®æ¯”è¼ƒåˆ†æã‚’è¡Œã†')
    parser.add_argument('--perplexity-date', type=str, help='ä½¿ç”¨ã™ã‚‹Perplexityãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ï¼ˆYYYYMMDDå½¢å¼ï¼‰')
    parser.add_argument('--data-type', choices=['rankings', 'citations'], default='citations',
                        help='æ¯”è¼ƒã™ã‚‹Perplexityãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: citationsï¼‰')
    parser.add_argument('--max', type=int, help='å‡¦ç†ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªæ•°ã®ä¸Šé™')
    parser.add_argument('--verbose', action='store_true', help='è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›ã‚’æœ‰åŠ¹åŒ–')
    args = parser.parse_args()

    # è©³ç´°ãƒ­ã‚°ã®è¨­å®š
    if args.verbose:
        import logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        logging.info("è©³ç´°ãƒ­ã‚°ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã—ãŸ")

    # ã‚«ãƒ†ã‚´ãƒªã¨ã‚µãƒ¼ãƒ“ã‚¹ã®å–å¾—
    categories = get_categories()
    if args.max:
        # ã‚«ãƒ†ã‚´ãƒªæ•°ã®ä¸Šé™ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯åˆ¶é™
        limited_categories = {}
        for i, (cat, subcat) in enumerate(categories.items()):
            if i >= args.max:
                break
            limited_categories[cat] = subcat
        categories = limited_categories
        if args.verbose:
            logging.info(f"ã‚«ãƒ†ã‚´ãƒªã‚’{args.max}å€‹ã«åˆ¶é™ã—ã¾ã—ãŸ")

    # æ—¥ä»˜ã‚’å–å¾—ï¼ˆæŒ‡å®šãŒãªã‘ã‚Œã°ä»Šæ—¥ã®æ—¥ä»˜ï¼‰
    perplexity_date = args.perplexity_date or datetime.datetime.now().strftime("%Y%m%d")
    if args.verbose:
        logging.info(f"Perplexityåˆ†ææ—¥ä»˜: {perplexity_date}, ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {args.data_type}")

    # Googleæ¤œç´¢çµæœã‚’å–å¾—
    result = process_categories_with_search(categories, args.max)

    # çµæœã‚’ä¿å­˜
    today_date = datetime.datetime.now().strftime("%Y%m%d")
    paths = get_results_paths(today_date)
    file_name = "custom_search.json"
    local_path = os.path.join(paths["raw_data"]["google"], file_name)
    s3_key = get_s3_key(file_name, today_date, "raw_data/google")
    save_results(result, local_path, s3_key, verbose=args.verbose)

    if args.verbose:
        logging.info(f"Googleæ¤œç´¢çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {local_path}")

if __name__ == "__main__":
    main()