#!/usr/bin/env python
# coding: utf-8

"""
ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æ - ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

Streamlitã‚’ä½¿ç”¨ã—ã¦ã€ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æã®çµæœãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã™ã€‚
å‹•çš„å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼šäº‹å‰ç”Ÿæˆç”»åƒã§ã¯ãªãã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
"""

import os
import json
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from datetime import datetime
from dotenv import load_dotenv
from src.utils.storage_utils import get_s3_client, S3_BUCKET_NAME, get_latest_file, load_json
from src.utils.plot_utils import draw_reliability_badge
import numpy as np

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæœ€åˆã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
import japanize_matplotlib

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# japanize_matplotlibã§æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è‡ªå‹•è¨­å®š
# japanize_matplotlibãŒæ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€è‡ªå‹•çš„ã«æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¨­å®šã•ã‚Œã‚‹
# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®è¨­å®š
try:
    font_path = '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc'
    if os.path.exists(font_path):
        prop = fm.FontProperties(fname=font_path)
        plt.rcParams['font.family'] = prop.get_name()
        plt.rcParams['font.sans-serif'] = [prop.get_name()]
    else:
        # japanize_matplotlibã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨
        plt.rcParams['font.family'] = 'IPAexGothic'
except:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯japanize_matplotlibã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨
    plt.rcParams['font.family'] = 'IPAexGothic'

plt.rcParams['axes.unicode_minus'] = False

# ã‚°ãƒ©ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
def set_plot_style():
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams['figure.figsize'] = (6, 3)  # ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å°ã•ãè¨­å®š
    plt.rcParams['font.size'] = 9
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['axes.titlesize'] = 12
    plt.rcParams['xtick.labelsize'] = 8
    plt.rcParams['ytick.labelsize'] = 8
    plt.rcParams['legend.fontsize'] = 8
    plt.rcParams['figure.titlesize'] = 14
    plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã‚’æ­£ã—ãè¡¨ç¤º

    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã®ç¢ºèª
    if 'font.family' not in plt.rcParams or not plt.rcParams['font.family']:
        plt.rcParams['font.family'] = 'IPAexGothic'

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®šã‚’é©ç”¨
set_plot_style()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å‹•çš„å¯è¦–åŒ–é–¢æ•°ç¾¤
def plot_ranking_similarity(similarity_data, title):
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ã®å‹•çš„å¯è¦–åŒ–"""
    metrics = ['rbo_score', 'kendall_tau', 'overlap_ratio']
    values = [similarity_data.get(metric, 0) for metric in metrics]
    labels = ['RBO\n(ä¸Šä½é‡è¦–é‡è¤‡åº¦)', 'Kendall Tau\n(é †ä½ç›¸é–¢)', 'Overlap Ratio\n(å…±é€šè¦ç´ ç‡)']

    fig, ax = plt.subplots(figsize=(10, 7))
    bars = ax.bar(labels, values, alpha=0.7, color=['blue', 'orange', 'green'])
    ax.set_title(f"{title} - Google vs Perplexityé¡ä¼¼åº¦")
    ax.set_ylabel("é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢")
    ax.set_ylim(0, 1)

    # Xè»¸ãƒ©ãƒ™ãƒ«ã®å›è»¢ã¨ä½ç½®èª¿æ•´
    plt.xticks(rotation=0, ha='center')

    for bar, value in zip(bars, values):
        if value is not None:
            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

    # ã‚°ãƒªãƒƒãƒ‰è¿½åŠ 
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    return fig

def plot_bias_indices_bar(bias_data, title, reliability_label=None):
    """æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã®å‹•çš„å¯è¦–åŒ–"""
    entities = list(bias_data.keys())
    values = [bias_data[e] for e in entities]

    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(entities, values, color=["red" if v > 0 else "green" for v in values])
    ax.axhline(0, color='k', linestyle='--', alpha=0.5)
    ax.set_ylabel("Normalized Bias Index (BI)")
    ax.set_title(title)
    plt.xticks(rotation=30, ha="right")
    plt.tight_layout()

    # ä¿¡é ¼æ€§ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
    if reliability_label:
        draw_reliability_badge(ax, reliability_label)

    return fig

def plot_official_domain_comparison(official_data, title):
    """å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒã®å‹•çš„å¯è¦–åŒ–"""
    google_ratio = official_data.get("google_official_ratio", 0)
    citations_ratio = official_data.get("citations_official_ratio", 0)

    fig, ax = plt.subplots(figsize=(8, 6))
    labels = ['Googleæ¤œç´¢', 'Perplexity Citations']
    values = [google_ratio, citations_ratio]
    colors = ['blue', 'orange']

    bars = ax.bar(labels, values, color=colors, alpha=0.7)
    ax.set_title(f"{title} - å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡æ¯”è¼ƒ")
    ax.set_ylabel("å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡")
    ax.set_ylim(0, 1)

    for bar, value in zip(bars, values):
        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    return fig

def plot_sentiment_comparison(sentiment_data, title):
    """æ„Ÿæƒ…åˆ†ææ¯”è¼ƒã®å‹•çš„å¯è¦–åŒ–"""
    google_dist = sentiment_data.get("google_sentiment_distribution", {})
    citations_dist = sentiment_data.get("citations_sentiment_distribution", {})

    fig, ax = plt.subplots(figsize=(10, 6))

    categories = ['positive', 'negative', 'neutral', 'unknown']
    x = np.arange(len(categories))
    width = 0.35

    google_values = [google_dist.get(cat, 0) for cat in categories]
    citations_values = [citations_dist.get(cat, 0) for cat in categories]

    ax.bar(x - width/2, google_values, width, label='Googleæ¤œç´¢', alpha=0.7, color='blue')
    ax.bar(x + width/2, citations_values, width, label='Perplexity Citations', alpha=0.7, color='orange')

    ax.set_title(f"{title} - æ„Ÿæƒ…åˆ†å¸ƒæ¯”è¼ƒ")
    ax.set_ylabel("æ¯”ç‡")
    ax.set_xticks(x)
    ax.set_xticklabels(['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ä¸­ç«‹', 'ä¸æ˜'])
    ax.legend()
    ax.set_ylim(0, 1)

    plt.tight_layout()
    return fig

def get_reliability_label(execution_count):
    """å®Ÿè¡Œå›æ•°ã«åŸºã¥ã„ã¦ä¿¡é ¼æ€§ãƒ©ãƒ™ãƒ«ã‚’å–å¾—"""
    if execution_count >= 15:
        return "é«˜ä¿¡é ¼æ€§"
    elif execution_count >= 10:
        return "ä¸­ä¿¡é ¼æ€§"
    elif execution_count >= 5:
        return "æ¨™æº–"
    elif execution_count >= 2:
        return "å‚è€ƒ"
    else:
        return "å‚è€ƒï¼ˆå®Ÿè¡Œå›æ•°ä¸è¶³ï¼‰"

def load_analysis_data(date_str):
    """æŒ‡å®šæ—¥ä»˜ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        local_path = f"corporate_bias_datasets/integrated/{date_str}/bias_analysis_results.json"

        if os.path.exists(local_path):
            with open(local_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            st.error(f"åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {local_path}")
            return None
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def get_data_files():
    """S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSON/CSVï¼‰ã®ã¿ã‚’å–å¾—"""
    try:
        s3_client = get_s3_client()
        prefix = 'results/'
        response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)

        if 'Contents' not in response:
            st.warning("S3ãƒã‚±ãƒƒãƒˆã«ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        files = []
        for content in response['Contents']:
            file_path = content['Key']
            file_name = os.path.basename(file_path)

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡
            if not (file_name.endswith('.json') or file_name.endswith('.csv')):
                continue

            # æ—¥ä»˜ã‚’æŠ½å‡º (YYYYMMDDå½¢å¼)
            date_match = next((part for part in file_name.split('_') if len(part) == 8 and part.isdigit()), None)

            if date_match:
                try:
                    date_obj = datetime.strptime(date_match, "%Y%m%d")
                    date_str = date_obj.strftime("%Y-%m-%d")
                except:
                    date_str = "ä¸æ˜"
            else:
                date_str = "ä¸æ˜"

            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’åˆ¤æ–­
            if "perplexity_sentiment" in file_name:
                data_type = "Perplexity æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "perplexity_results" in file_name and "/sentiment/" in file_path:
                data_type = "Perplexity æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "perplexity_rankings" in file_name:
                data_type = "Perplexity ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
            elif "perplexity_results" in file_name and "/rankings/" in file_path:
                data_type = "Perplexity ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
            elif "perplexity_citations" in file_name:
                data_type = "Perplexity å¼•ç”¨ãƒªãƒ³ã‚¯"
            elif "perplexity_results" in file_name and "/citations/" in file_path:
                data_type = "Perplexity å¼•ç”¨ãƒªãƒ³ã‚¯"
            elif "openai_sentiment" in file_name:
                data_type = "OpenAI æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "openai_results" in file_name:
                data_type = "OpenAI æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            else:
                data_type = "ãã®ä»–"

            # è¤‡æ•°å›å®Ÿè¡Œã‹ã©ã†ã‹
            import re
            m = re.search(r'_(\d+)runs\.json', file_name)
            if m:
                runs_count = int(m.group(1))
                is_multi_run = True
                runs_suffix = f"ï¼ˆ{runs_count}å›å®Ÿè¡Œï¼‰"
            elif "_runs.json" in file_name:
                is_multi_run = False
                runs_suffix = "ï¼ˆå˜ä¸€å®Ÿè¡Œï¼‰"
            else:
                is_multi_run = False
                runs_suffix = "ï¼ˆå˜ä¸€å®Ÿè¡Œï¼‰"

            # è¡¨ç¤ºåã‚’ç”Ÿæˆ
            display_name = f"{date_str}: {data_type}{runs_suffix}"

            files.append({
                "path": f"s3://{S3_BUCKET_NAME}/{file_path}",
                "name": file_name,
                "date": date_str,
                "date_obj": date_obj if date_match else datetime.now(),
                "type": data_type,
                "is_multi_run": is_multi_run,
                "display_name": display_name,
                "date_raw": date_match
            })

        # æ—¥ä»˜ã®æ–°ã—ã„é †ï¼‹åŒæ—¥ä»˜ãªã‚‰å®Ÿè¡Œå›æ•°å¤šã„é †ã«ã‚½ãƒ¼ãƒˆ
        def extract_runs(file):
            m = re.search(r'_(\d+)runs\.json', file["name"])
            return int(m.group(1)) if m else 1
        files.sort(key=lambda x: (x["date_obj"], extract_runs(x)), reverse=True)
        return files

    except Exception as e:
        st.error(f"S3ã‹ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_image_files():
    """S3ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPNGï¼‰ã®ã¿ã‚’å–å¾—"""
    try:
        s3_client = get_s3_client()
        prefix = 'results/perplexity_analysis/'
        response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)

        if 'Contents' not in response:
            st.warning("S3ãƒã‚±ãƒƒãƒˆã«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        files = []
        for content in response['Contents']:
            file_path = content['Key']
            file_name = os.path.basename(file_path)

            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡
            if not file_name.endswith('.png'):
                continue

            # æ—¥ä»˜ã‚’æŠ½å‡º (YYYYMMDDå½¢å¼)
            date_match = next((part for part in file_path.split('/') if len(part) == 8 and part.isdigit()), None)

            if date_match:
                try:
                    date_obj = datetime.strptime(date_match, "%Y%m%d")
                    date_str = date_obj.strftime("%Y-%m-%d")
                except:
                    date_str = "ä¸æ˜"
            else:
                date_str = "ä¸æ˜"

            # ç”»åƒã‚¿ã‚¤ãƒ—ã‚’åˆ¤æ–­
            if "_exposure_market.png" in file_name:
                image_type = "éœ²å‡ºåº¦ãƒ»å¸‚å ´ã‚·ã‚§ã‚¢"
            elif "_rank_heatmap.png" in file_name:
                image_type = "ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†å¸ƒ"
            elif "_stability_matrix.png" in file_name:
                image_type = "å®‰å®šæ€§è¡Œåˆ—"
            else:
                image_type = "ãã®ä»–"

            # è¡¨ç¤ºåã‚’ç”Ÿæˆ
            display_name = f"{date_str}: {image_type} - {file_name}"

            files.append({
                "path": f"s3://{S3_BUCKET_NAME}/{file_path}",
                "name": file_name,
                "date": date_str,
                "date_obj": date_obj if date_match else datetime.now(),
                "type": image_type,
                "display_name": display_name,
                "date_raw": date_match
            })

        # æ—¥ä»˜ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        files.sort(key=lambda x: x["date_obj"], reverse=True)
        return files

    except Exception as e:
        st.error(f"S3ã‹ã‚‰ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_visualization_files(date_str):
    """æŒ‡å®šæ—¥ä»˜ã®å¯è¦–åŒ–ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    # integratedã®æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨åŒã˜æ—¥ä»˜ã®analysis_visuals/é…ä¸‹ã‚’å‚ç…§
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»S3å…±ã«corporate_bias_datasets/analysis_visuals/YYYYMMDD/ã«çµ±ä¸€

    visuals_dir = os.path.join("corporate_bias_datasets/analysis_visuals", date_str)
    return visuals_dir

# CSSã§ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’è¿½åŠ 
st.markdown("""
<style>
    body {
        font-family: 'Hiragino Sans', 'Meiryo', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
st.markdown("AIæ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã«ãŠã‘ã‚‹ä¼æ¥­å„ªé‡ãƒã‚¤ã‚¢ã‚¹ã®å¯è¦–åŒ–")

# --- åˆ†æã‚µãƒãƒªãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
st.header("åˆ†æã‚µãƒãƒªãƒ¼ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼‰")

def get_latest_integrated_dir():
    """corporate_bias_datasets/integrated/é…ä¸‹ã®æœ€æ–°æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿”ã™"""
    base_dir = "corporate_bias_datasets/integrated"
    dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.isdigit()]
    if not dirs:
        return None
    latest = sorted(dirs, reverse=True)[0]
    return os.path.join(base_dir, latest)

def get_bias_summary():
    """æœ€æ–°ã®bias_analysis_results.jsonã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä¸»è¦ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚’æŠ½å‡ºã—DataFrameã§è¿”ã™"""
    latest_dir = get_latest_integrated_dir()
    if not latest_dir:
        return None, "ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    bias_path = os.path.join(latest_dir, "bias_analysis_results.json")
    if not os.path.exists(bias_path):
        return None, f"{bias_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    with open(bias_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    results = []
    sentiment = data.get("sentiment_bias_analysis", {})
    for category, subcats in sentiment.items():
        for subcat, subdata in subcats.items():
            entities = subdata.get("entities", {})
            for key, ent in entities.items():
                metrics = ent.get("basic_metrics", {})
                interp = ent.get("interpretation", {})
                stability = ent.get("stability_metrics", {})
                stat = ent.get("statistical_significance", {})
                results.append({
                    "ã‚«ãƒ†ã‚´ãƒª": category,
                    "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª": subcat,
                    "åˆ†æå˜ä½": key,
                    "ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™": metrics.get("normalized_bias_index"),
                    "ã‚¹ã‚³ã‚¢ã®å¹³å‡å·®": metrics.get("raw_delta"),
                    "å®‰å®šæ€§": stability.get("stability_score"),
                    "ãƒã‚¤ã‚¢ã‚¹æ–¹å‘": interp.get("bias_direction"),
                    "ãƒã‚¤ã‚¢ã‚¹å¼·åº¦": interp.get("bias_strength"),
                    "æœ‰æ„æ€§": stat.get("available"),
                    "æ¨å¥¨": interp.get("recommendation")
                })
    df = pd.DataFrame(results)
    return df, None

df, err = get_bias_summary()
if err:
    st.warning(err)
else:
    # ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    st.subheader("ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé™é †ï¼‰")
    top = df.sort_values("ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™", ascending=False).head(20)
    st.dataframe(top)
    # ãƒã‚¤ã‚¢ã‚¹æ–¹å‘ãƒ»å¼·åº¦ã®åˆ†å¸ƒ
    st.subheader("ãƒã‚¤ã‚¢ã‚¹æ–¹å‘ãƒ»å¼·åº¦ã®åˆ†å¸ƒ")
    bias_dir_counts = df["ãƒã‚¤ã‚¢ã‚¹æ–¹å‘"].value_counts()
    bias_strength_counts = df["ãƒã‚¤ã‚¢ã‚¹å¼·åº¦"].value_counts()
    col1, col2 = st.columns(2)
    with col1:
        st.bar_chart(bias_dir_counts)
    with col2:
        st.bar_chart(bias_strength_counts)
    # æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    st.subheader("æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ï¼ˆp<0.05ç›¸å½“ï¼‰")
    sig = df[df["æœ‰æ„æ€§"] == True]
    if not sig.empty:
        st.dataframe(sig)
    else:
        st.info("æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ã¯æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“")

# --- ãƒ‡ãƒ¼ã‚¿åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
st.header("è©³ç´°åˆ†æï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå˜ä½ï¼‰")

def get_latest_dataset_path():
    latest_dir = get_latest_integrated_dir()
    if not latest_dir:
        return None
    dataset_path = os.path.join(latest_dir, "corporate_bias_dataset.json")
    if not os.path.exists(dataset_path):
        return None
    return dataset_path

dataset_path = get_latest_dataset_path()
if not dataset_path:
    st.warning("corporate_bias_dataset.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
else:
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    categories = list(dataset.keys())
    selected_category = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", categories)
    subcategories = list(dataset[selected_category].keys())
    selected_subcategory = st.selectbox("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", subcategories)
    subcat_data = dataset[selected_category][selected_subcategory]
    st.subheader(f"{selected_category} - {selected_subcategory} ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿")
    st.json(subcat_data)

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---
st.sidebar.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿é¸æŠ")

# åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚’å–å¾—
def get_available_dates():
    """åˆ©ç”¨å¯èƒ½ãªåˆ†ææ—¥ä»˜ã‚’å–å¾—"""
    integrated_dir = "corporate_bias_datasets/integrated"
    if not os.path.exists(integrated_dir):
        return []

    dates = []
    for item in os.listdir(integrated_dir):
        item_path = os.path.join(integrated_dir, item)
        if os.path.isdir(item_path):
            bias_path = os.path.join(item_path, "bias_analysis_results.json")
            if os.path.exists(bias_path):
                dates.append(item)

    return sorted(dates, reverse=True)

available_dates = get_available_dates()
st.write(f"åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜: {available_dates}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

if not available_dates:
    st.sidebar.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    st.stop()

# æ—¥ä»˜é¸æŠ
selected_date = st.sidebar.selectbox(
    "åˆ†ææ—¥ä»˜ã‚’é¸æŠ",
    available_dates,
    index=0
)

# åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
analysis_data = load_analysis_data(selected_date)
st.write(f"åˆ†æãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿çµæœ: {analysis_data is not None}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

if not analysis_data:
    st.sidebar.error(f"åˆ†æãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {selected_date}")
    st.stop()

# å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—é¸æŠ
viz_type = st.sidebar.selectbox(
    "å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
    ["æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹åˆ†æ", "Citations-Googleæ¯”è¼ƒ", "çµ±åˆåˆ†æ"]
)

# ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªé¸æŠï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
if viz_type == "æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹åˆ†æ":
    sentiment_data = analysis_data.get("sentiment_bias_analysis", {})
    if sentiment_data:
        categories = list(sentiment_data.keys())
        selected_category = st.sidebar.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", categories)

        subcategories = list(sentiment_data[selected_category].keys())
        selected_subcategory = st.sidebar.selectbox("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", subcategories)

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¸æŠ
        entities_data = sentiment_data[selected_category][selected_subcategory].get("entities", {})
        entities = list(entities_data.keys())
        selected_entities = st.sidebar.multiselect(
            "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
            entities,
            default=entities[:5] if len(entities) > 5 else entities
        )

elif viz_type == "Citations-Googleæ¯”è¼ƒ":
    citations_data = analysis_data.get("citations_google_comparison", {})
    if citations_data:
        categories = list(citations_data.keys())
        if "error" in categories:
            categories.remove("error")

        if categories:
            selected_category = st.sidebar.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", categories)
            subcategories = list(citations_data[selected_category].keys())
            selected_subcategory = st.sidebar.selectbox("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", subcategories)

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.header(f"ğŸ“ˆ å‹•çš„å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - {selected_date}")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿çŠ¶æ³ã®è¡¨ç¤º
st.sidebar.markdown("---")
st.sidebar.markdown("**ğŸ“‹ ãƒ‡ãƒ¼ã‚¿çŠ¶æ³**")

# ãƒ‡ãƒ¼ã‚¿å¯ç”¨æ€§ã®ç¢ºèª
data_availability = analysis_data.get("data_availability_summary", {})
if data_availability:
    available_count = sum(1 for item in data_availability.values() if isinstance(item, dict) and item.get("available", False))
    total_count = len(data_availability)
    st.sidebar.metric("åˆ©ç”¨å¯èƒ½æŒ‡æ¨™", f"{available_count}/{total_count}")

# åˆ†æåˆ¶é™äº‹é …ã®è¡¨ç¤º
limitations = analysis_data.get("analysis_limitations", {})
if limitations and isinstance(limitations, dict):
    warnings = []
    if "execution_count_warning" in limitations:
        warnings.append("âš ï¸ å®Ÿè¡Œå›æ•°ä¸è¶³")
    if "reliability_note" in limitations:
        warnings.append("ğŸ“Š å‚è€ƒãƒ¬ãƒ™ãƒ«ã®ä¿¡é ¼æ€§")

    if warnings:
        st.sidebar.markdown("**âš ï¸ æ³¨æ„äº‹é …**")
        for warning in warnings:
            st.sidebar.markdown(warning)

# --- ãƒ¡ã‚¤ãƒ³å¯è¦–åŒ–ç”»é¢ ---
if viz_type == "æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹åˆ†æ":
    st.subheader(f"ğŸ¯ æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹åˆ†æ - {selected_category} / {selected_subcategory}")

    if not selected_entities:
        st.warning("ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é¸æŠã—ã¦ãã ã•ã„")
    else:
        # é¸æŠã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚’æŠ½å‡º
        bias_indices = {}
        execution_counts = {}

        for entity in selected_entities:
            if entity in entities_data:
                entity_data = entities_data[entity]
                if "basic_metrics" in entity_data:
                    bias_indices[entity] = entity_data["basic_metrics"].get("normalized_bias_index", 0)
                    execution_counts[entity] = entity_data["basic_metrics"].get("execution_count", 0)

        if bias_indices:
            # ä¿¡é ¼æ€§ãƒ©ãƒ™ãƒ«ã‚’è¨ˆç®—ï¼ˆæœ€å°å®Ÿè¡Œå›æ•°ã‚’ä½¿ç”¨ï¼‰
            min_exec_count = min(execution_counts.values()) if execution_counts else 0
            reliability_label = get_reliability_label(min_exec_count)

            # ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
            fig = plot_bias_indices_bar(
                bias_indices,
                f"{selected_category} - {selected_subcategory}",
                reliability_label
            )
            st.pyplot(fig)

            # è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**ğŸ“Š ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™è©³ç´°**")
                for entity, bias_index in bias_indices.items():
                    exec_count = execution_counts.get(entity, 0)
                    st.markdown(f"- **{entity}**: {bias_index:.3f} (å®Ÿè¡Œå›æ•°: {exec_count}å›)")

            with col2:
                st.markdown("**ğŸ“‹ ä¿¡é ¼æ€§æƒ…å ±**")
                st.markdown(f"- **ä¿¡é ¼æ€§ãƒ¬ãƒ™ãƒ«**: {reliability_label}")
                st.markdown(f"- **æœ€å°å®Ÿè¡Œå›æ•°**: {min_exec_count}å›")
                if min_exec_count < 5:
                    st.warning("âš ï¸ çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®šã«ã¯æœ€ä½5å›ã®å®Ÿè¡ŒãŒå¿…è¦ã§ã™")
        else:
            st.info("é¸æŠã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

elif viz_type == "Citations-Googleæ¯”è¼ƒ":
    st.subheader(f"ğŸ”„ Citations-Googleæ¯”è¼ƒåˆ†æ - {selected_category} / {selected_subcategory}")

    comparison_data = citations_data[selected_category][selected_subcategory]

    # ã‚¿ãƒ–ã§å¯è¦–åŒ–ã‚’åˆ†ã‘ã‚‹
    tab1, tab2, tab3, tab4 = st.tabs(["ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦", "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒ", "æ„Ÿæƒ…åˆ†ææ¯”è¼ƒ", "ãƒ‡ãƒ¼ã‚¿å“è³ª"])

    with tab1:
        if "ranking_similarity" in comparison_data:
            fig = plot_ranking_similarity(
                comparison_data["ranking_similarity"],
                f"{selected_category} - {selected_subcategory}"
            )
            st.pyplot(fig)

            # æŒ‡æ¨™ã®èª¬æ˜
            st.markdown("""
            **ğŸ“ˆ æŒ‡æ¨™ã®èª¬æ˜:**
            - **RBO (Rank Biased Overlap)**: ä¸Šä½ã®çµæœã‚’é‡è¦–ã—ãŸé‡è¤‡åº¦ï¼ˆ0-1ï¼‰
            - **Kendall Tau**: é †ä½ã®ç›¸é–¢ä¿‚æ•°ï¼ˆ-1ã€œ1ï¼‰
            - **Overlap Ratio**: å…±é€šè¦ç´ ã®å‰²åˆï¼ˆ0-1ï¼‰
            """)
        else:
            st.info("ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    with tab2:
        if "official_domain_analysis" in comparison_data:
            fig = plot_official_domain_comparison(
                comparison_data["official_domain_analysis"],
                f"{selected_category} - {selected_subcategory}"
            )
            st.pyplot(fig)

            # åˆ†æçµæœã®èª¬æ˜
            official_data = comparison_data["official_domain_analysis"]
            st.markdown(f"""
            **ğŸ“Š åˆ†æçµæœ:**
            - Googleæ¤œç´¢å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡: {official_data.get('google_official_ratio', 0):.3f}
            - Perplexityå…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡: {official_data.get('citations_official_ratio', 0):.3f}
            - ãƒã‚¤ã‚¢ã‚¹æ–¹å‘: {official_data.get('bias_direction', 'unknown')}
            """)
        else:
            st.info("å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    with tab3:
        if "sentiment_comparison" in comparison_data:
            fig = plot_sentiment_comparison(
                comparison_data["sentiment_comparison"],
                f"{selected_category} - {selected_subcategory}"
            )
            st.pyplot(fig)

            # ç›¸é–¢æƒ…å ±
            sentiment_data = comparison_data["sentiment_comparison"]
            st.markdown(f"""
            **ğŸ“ˆ æ„Ÿæƒ…åˆ†æç›¸é–¢:**
            - ç›¸é–¢ä¿‚æ•°: {sentiment_data.get('sentiment_correlation', 0):.3f}
            - ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¤ã‚¢ã‚¹å·®åˆ†: {sentiment_data.get('positive_bias_delta', 0):.3f}
            """)
        else:
            st.info("æ„Ÿæƒ…åˆ†ææ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    with tab4:
        if "data_quality" in comparison_data:
            quality_data = comparison_data["data_quality"]
            st.markdown("**ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªæƒ…å ±:**")
            st.json(quality_data)
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿å“è³ªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")

elif viz_type == "çµ±åˆåˆ†æ":
    st.subheader("ğŸ“Š çµ±åˆåˆ†æçµæœ")
    cross_data = analysis_data.get("cross_analysis_insights", {})

    if cross_data:
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**ğŸ“ˆ ä¸»è¦æŒ‡æ¨™**")
            st.metric("æ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢", f"{cross_data.get('sentiment_ranking_correlation', 0):.3f}")
            st.metric("Google-Citationsæ•´åˆæ€§", cross_data.get('google_citations_alignment', 'unknown'))
            st.metric("å…¨ä½“çš„ãƒã‚¤ã‚¢ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³", cross_data.get('overall_bias_pattern', 'unknown'))

        with col2:
            st.markdown("**ğŸ“‹ åˆ†æã‚«ãƒãƒ¬ãƒƒã‚¸**")
            coverage = cross_data.get('analysis_coverage', {})
            for key, value in coverage.items():
                status = "âœ…" if value else "âŒ"
                st.markdown(f"- {key}: {status}")

        # è©³ç´°ãƒ‡ãƒ¼ã‚¿
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿"):
            st.json(cross_data)
    else:
        st.info("çµ±åˆåˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

# --- å¾“æ¥ã®ç”»åƒè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå‚è€ƒç”¨ï¼‰ ---
with st.expander("å¾“æ¥ã®äº‹å‰ç”Ÿæˆç”»åƒï¼ˆå‚è€ƒï¼‰"):
    st.header("äº‹å‰ç”Ÿæˆç”»åƒï¼ˆå‚è€ƒç”¨ï¼‰")

    def get_latest_visuals_dir():
        # integratedã®æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨åŒã˜æ—¥ä»˜ã®analysis_visuals/é…ä¸‹ã‚’å‚ç…§
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»S3å…±ã«corporate_bias_datasets/analysis_visuals/YYYYMMDD/ã«çµ±ä¸€
        latest_dir = get_latest_integrated_dir()
        if not latest_dir:
            return None
        date_str = os.path.basename(latest_dir)
        visuals_dir = os.path.join("corporate_bias_datasets/analysis_visuals", date_str)
        if not os.path.exists(visuals_dir):
            return None
        return visuals_dir

    visuals_dir = get_latest_visuals_dir()
    if not visuals_dir:
        st.warning("ç”»åƒæŒ‡æ¨™ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    else:
        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨ã«ç”»åƒã‚’è¡¨ç¤º
        for subdir in os.listdir(visuals_dir):
            subdir_path = os.path.join(visuals_dir, subdir)
            if os.path.isdir(subdir_path):
                st.subheader(f"{subdir} ã®ç”»åƒæŒ‡æ¨™")
                images = [f for f in os.listdir(subdir_path) if f.lower().endswith(".png")]
                for img in images:
                    img_path = os.path.join(subdir_path, img)
                    st.image(img_path, caption=img)