#!/usr/bin/env python
# coding: utf-8

"""
ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æ - ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

Streamlitã‚’ä½¿ç”¨ã—ã¦ã€ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æã®çµæœãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã™ã€‚
å‹•çš„å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼šäº‹å‰ç”Ÿæˆç”»åƒã§ã¯ãªãã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
"""

import os
import json
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from datetime import datetime
from dotenv import load_dotenv
from src.utils.storage_utils import get_s3_client, S3_BUCKET_NAME, get_latest_file, load_json
from src.utils.plot_utils import draw_reliability_badge
import numpy as np

# åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’å„ªå…ˆçš„ã«å–å¾—
import japanize_matplotlib

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ã®ãƒ‘ã‚¹ï¼ˆW3ã‚’ä¾‹ã«ï¼‰
font_path = '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc'
prop = fm.FontProperties(fname=font_path)
plt.rcParams['font.family'] = prop.get_name()
plt.rcParams['font.sans-serif'] = [prop.get_name()]
plt.rcParams['axes.unicode_minus'] = False

# ã‚°ãƒ©ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
def set_plot_style():
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams['figure.figsize'] = (6, 3)  # ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å°ã•ãè¨­å®š
    plt.rcParams['font.size'] = 9
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['axes.titlesize'] = 12
    plt.rcParams['xtick.labelsize'] = 8
    plt.rcParams['ytick.labelsize'] = 8
    plt.rcParams['legend.fontsize'] = 8
    plt.rcParams['figure.titlesize'] = 14
    plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã‚’æ­£ã—ãè¡¨ç¤º

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®šã‚’é©ç”¨
set_plot_style()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å‹•çš„å¯è¦–åŒ–é–¢æ•°ç¾¤
def plot_ranking_similarity(similarity_data, title):
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ã®å‹•çš„å¯è¦–åŒ–"""
    metrics = ['rbo_score', 'kendall_tau', 'overlap_ratio']
    values = [similarity_data.get(metric, 0) for metric in metrics]
    labels = ['RBO\n(ä¸Šä½é‡è¦–é‡è¤‡åº¦)', 'Kendall Tau\n(é †ä½ç›¸é–¢)', 'Overlap Ratio\n(å…±é€šè¦ç´ ç‡)']

    fig, ax = plt.subplots(figsize=(10, 7))
    bars = ax.bar(labels, values, alpha=0.7, color=['blue', 'orange', 'green'])
    ax.set_title(f"{title} - Google vs Perplexityé¡ä¼¼åº¦")
    ax.set_ylabel("é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢")
    ax.set_ylim(0, 1)

    # Xè»¸ãƒ©ãƒ™ãƒ«ã®å›è»¢ã¨ä½ç½®èª¿æ•´
    plt.xticks(rotation=0, ha='center')

    for bar, value in zip(bars, values):
        if value is not None:
            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

    # ã‚°ãƒªãƒƒãƒ‰è¿½åŠ 
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    return fig

def plot_sentiment_bias(bias_data, title):
    """æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹ã®å‹•çš„å¯è¦–åŒ–"""
    entities = list(bias_data.keys())
    values = [bias_data[e] for e in entities]

    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(entities, values, color=["red" if v > 0 else "green" for v in values])
    ax.axhline(0, color='k', linestyle='--', alpha=0.5)
    ax.set_ylabel("Normalized Bias Index (BI)")
    ax.set_title(title)
    plt.xticks(rotation=30, ha="right")
    plt.tight_layout()
    return fig

def plot_official_domain_comparison(official_data, title):
    """å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒã®å‹•çš„å¯è¦–åŒ–"""
    google_ratio = official_data.get("google_official_ratio", 0)
    citations_ratio = official_data.get("citations_official_ratio", 0)

    fig, ax = plt.subplots(figsize=(8, 6))
    labels = ['Googleæ¤œç´¢', 'Perplexity Citations']
    values = [google_ratio, citations_ratio]
    colors = ['blue', 'orange']

    bars = ax.bar(labels, values, color=colors, alpha=0.7)
    ax.set_title(f"{title} - å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡æ¯”è¼ƒ")
    ax.set_ylabel("å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡")
    ax.set_ylim(0, 1)

    for bar, value in zip(bars, values):
        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    return fig

def plot_sentiment_comparison(sentiment_data, title):
    """æ„Ÿæƒ…åˆ†ææ¯”è¼ƒã®å‹•çš„å¯è¦–åŒ–"""
    google_dist = sentiment_data.get("google_sentiment_distribution", {})
    citations_dist = sentiment_data.get("citations_sentiment_distribution", {})

    fig, ax = plt.subplots(figsize=(10, 6))

    categories = ['positive', 'negative', 'neutral', 'unknown']
    x = np.arange(len(categories))
    width = 0.35

    google_values = [google_dist.get(cat, 0) for cat in categories]
    citations_values = [citations_dist.get(cat, 0) for cat in categories]

    ax.bar(x - width/2, google_values, width, label='Googleæ¤œç´¢', alpha=0.7, color='blue')
    ax.bar(x + width/2, citations_values, width, label='Perplexity Citations', alpha=0.7, color='orange')

    ax.set_title(f"{title} - æ„Ÿæƒ…åˆ†å¸ƒæ¯”è¼ƒ")
    ax.set_ylabel("æ¯”ç‡")
    ax.set_xticks(x)
    ax.set_xticklabels(['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ä¸­ç«‹', 'ä¸æ˜'])
    ax.legend()
    ax.set_ylim(0, 1)

    plt.tight_layout()
    return fig

def get_data_files():
    """S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSON/CSVï¼‰ã®ã¿ã‚’å–å¾—"""
    try:
        s3_client = get_s3_client()
        prefix = 'results/'
        response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)

        if 'Contents' not in response:
            st.warning("S3ãƒã‚±ãƒƒãƒˆã«ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        files = []
        for content in response['Contents']:
            file_path = content['Key']
            file_name = os.path.basename(file_path)

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡
            if not (file_name.endswith('.json') or file_name.endswith('.csv')):
                continue

            # æ—¥ä»˜ã‚’æŠ½å‡º (YYYYMMDDå½¢å¼)
            date_match = next((part for part in file_name.split('_') if len(part) == 8 and part.isdigit()), None)

            if date_match:
                try:
                    date_obj = datetime.strptime(date_match, "%Y%m%d")
                    date_str = date_obj.strftime("%Y-%m-%d")
                except:
                    date_str = "ä¸æ˜"
            else:
                date_str = "ä¸æ˜"

            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’åˆ¤æ–­
            if "perplexity_sentiment" in file_name:
                data_type = "Perplexity æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "perplexity_results" in file_name and "/sentiment/" in file_path:
                data_type = "Perplexity æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "perplexity_rankings" in file_name:
                data_type = "Perplexity ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
            elif "perplexity_results" in file_name and "/rankings/" in file_path:
                data_type = "Perplexity ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
            elif "perplexity_citations" in file_name:
                data_type = "Perplexity å¼•ç”¨ãƒªãƒ³ã‚¯"
            elif "perplexity_results" in file_name and "/citations/" in file_path:
                data_type = "Perplexity å¼•ç”¨ãƒªãƒ³ã‚¯"
            elif "openai_sentiment" in file_name:
                data_type = "OpenAI æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "openai_results" in file_name:
                data_type = "OpenAI æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            else:
                data_type = "ãã®ä»–"

            # è¤‡æ•°å›å®Ÿè¡Œã‹ã©ã†ã‹
            import re
            m = re.search(r'_(\d+)runs\.json', file_name)
            if m:
                runs_count = int(m.group(1))
                is_multi_run = True
                runs_suffix = f"ï¼ˆ{runs_count}å›å®Ÿè¡Œï¼‰"
            elif "_runs.json" in file_name:
                is_multi_run = False
                runs_suffix = "ï¼ˆå˜ä¸€å®Ÿè¡Œï¼‰"
            else:
                is_multi_run = False
                runs_suffix = "ï¼ˆå˜ä¸€å®Ÿè¡Œï¼‰"

            # è¡¨ç¤ºåã‚’ç”Ÿæˆ
            display_name = f"{date_str}: {data_type}{runs_suffix}"

            files.append({
                "path": f"s3://{S3_BUCKET_NAME}/{file_path}",
                "name": file_name,
                "date": date_str,
                "date_obj": date_obj if date_match else datetime.now(),
                "type": data_type,
                "is_multi_run": is_multi_run,
                "display_name": display_name,
                "date_raw": date_match
            })

        # æ—¥ä»˜ã®æ–°ã—ã„é †ï¼‹åŒæ—¥ä»˜ãªã‚‰å®Ÿè¡Œå›æ•°å¤šã„é †ã«ã‚½ãƒ¼ãƒˆ
        def extract_runs(file):
            m = re.search(r'_(\d+)runs\.json', file["name"])
            return int(m.group(1)) if m else 1
        files.sort(key=lambda x: (x["date_obj"], extract_runs(x)), reverse=True)
        return files

    except Exception as e:
        st.error(f"S3ã‹ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_image_files():
    """S3ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPNGï¼‰ã®ã¿ã‚’å–å¾—"""
    try:
        s3_client = get_s3_client()
        prefix = 'results/perplexity_analysis/'
        response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)

        if 'Contents' not in response:
            st.warning("S3ãƒã‚±ãƒƒãƒˆã«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        files = []
        for content in response['Contents']:
            file_path = content['Key']
            file_name = os.path.basename(file_path)

            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡
            if not file_name.endswith('.png'):
                continue

            # æ—¥ä»˜ã‚’æŠ½å‡º (YYYYMMDDå½¢å¼)
            date_match = next((part for part in file_path.split('/') if len(part) == 8 and part.isdigit()), None)

            if date_match:
                try:
                    date_obj = datetime.strptime(date_match, "%Y%m%d")
                    date_str = date_obj.strftime("%Y-%m-%d")
                except:
                    date_str = "ä¸æ˜"
            else:
                date_str = "ä¸æ˜"

            # ç”»åƒã‚¿ã‚¤ãƒ—ã‚’åˆ¤æ–­
            if "_exposure_market.png" in file_name:
                image_type = "éœ²å‡ºåº¦ãƒ»å¸‚å ´ã‚·ã‚§ã‚¢"
            elif "_rank_heatmap.png" in file_name:
                image_type = "ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†å¸ƒ"
            elif "_stability_matrix.png" in file_name:
                image_type = "å®‰å®šæ€§è¡Œåˆ—"
            else:
                image_type = "ãã®ä»–"

            # è¡¨ç¤ºåã‚’ç”Ÿæˆ
            display_name = f"{date_str}: {image_type} - {file_name}"

            files.append({
                "path": f"s3://{S3_BUCKET_NAME}/{file_path}",
                "name": file_name,
                "date": date_str,
                "date_obj": date_obj if date_match else datetime.now(),
                "type": image_type,
                "display_name": display_name,
                "date_raw": date_match
            })

        # æ—¥ä»˜ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        files.sort(key=lambda x: x["date_obj"], reverse=True)
        return files

    except Exception as e:
        st.error(f"S3ã‹ã‚‰ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_visualization_files(date_str):
    """æŒ‡å®šæ—¥ä»˜ã®å¯è¦–åŒ–ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    # integratedã®æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨åŒã˜æ—¥ä»˜ã®analysis_visuals/é…ä¸‹ã‚’å‚ç…§
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»S3å…±ã«corporate_bias_datasets/analysis_visuals/YYYYMMDD/ã«çµ±ä¸€

    visuals_dir = os.path.join("corporate_bias_datasets/analysis_visuals", date_str)
    return visuals_dir

# CSSã§ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’è¿½åŠ 
st.markdown("""
<style>
    body {
        font-family: 'Hiragino Sans', 'Meiryo', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
st.markdown("AIæ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã«ãŠã‘ã‚‹ä¼æ¥­å„ªé‡ãƒã‚¤ã‚¢ã‚¹ã®å¯è¦–åŒ–")

# --- åˆ†æã‚µãƒãƒªãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
st.header("åˆ†æã‚µãƒãƒªãƒ¼ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼‰")

def get_latest_integrated_dir():
    """corporate_bias_datasets/integrated/é…ä¸‹ã®æœ€æ–°æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿”ã™"""
    base_dir = "corporate_bias_datasets/integrated"
    dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.isdigit()]
    if not dirs:
        return None
    latest = sorted(dirs, reverse=True)[0]
    return os.path.join(base_dir, latest)

def get_bias_summary():
    """æœ€æ–°ã®bias_analysis_results.jsonã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä¸»è¦ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚’æŠ½å‡ºã—DataFrameã§è¿”ã™"""
    latest_dir = get_latest_integrated_dir()
    if not latest_dir:
        return None, "ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    bias_path = os.path.join(latest_dir, "bias_analysis_results.json")
    if not os.path.exists(bias_path):
        return None, f"{bias_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    with open(bias_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    results = []
    sentiment = data.get("sentiment_bias_analysis", {})
    for category, subcats in sentiment.items():
        for subcat, subdata in subcats.items():
            entities = subdata.get("entities", {})
            for key, ent in entities.items():
                metrics = ent.get("basic_metrics", {})
                interp = ent.get("interpretation", {})
                stability = ent.get("stability_metrics", {})
                stat = ent.get("statistical_significance", {})
                results.append({
                    "ã‚«ãƒ†ã‚´ãƒª": category,
                    "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª": subcat,
                    "åˆ†æå˜ä½": key,
                    "ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™": metrics.get("normalized_bias_index"),
                    "ã‚¹ã‚³ã‚¢ã®å¹³å‡å·®": metrics.get("raw_delta"),
                    "å®‰å®šæ€§": stability.get("stability_score"),
                    "ãƒã‚¤ã‚¢ã‚¹æ–¹å‘": interp.get("bias_direction"),
                    "ãƒã‚¤ã‚¢ã‚¹å¼·åº¦": interp.get("bias_strength"),
                    "æœ‰æ„æ€§": stat.get("available"),
                    "æ¨å¥¨": interp.get("recommendation")
                })
    df = pd.DataFrame(results)
    return df, None

df, err = get_bias_summary()
if err:
    st.warning(err)
else:
    # ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    st.subheader("ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé™é †ï¼‰")
    top = df.sort_values("ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™", ascending=False).head(20)
    st.dataframe(top)
    # ãƒã‚¤ã‚¢ã‚¹æ–¹å‘ãƒ»å¼·åº¦ã®åˆ†å¸ƒ
    st.subheader("ãƒã‚¤ã‚¢ã‚¹æ–¹å‘ãƒ»å¼·åº¦ã®åˆ†å¸ƒ")
    bias_dir_counts = df["ãƒã‚¤ã‚¢ã‚¹æ–¹å‘"].value_counts()
    bias_strength_counts = df["ãƒã‚¤ã‚¢ã‚¹å¼·åº¦"].value_counts()
    col1, col2 = st.columns(2)
    with col1:
        st.bar_chart(bias_dir_counts)
    with col2:
        st.bar_chart(bias_strength_counts)
    # æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    st.subheader("æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ï¼ˆp<0.05ç›¸å½“ï¼‰")
    sig = df[df["æœ‰æ„æ€§"] == True]
    if not sig.empty:
        st.dataframe(sig)
    else:
        st.info("æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ã¯æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“")

# --- ãƒ‡ãƒ¼ã‚¿åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
st.header("è©³ç´°åˆ†æï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå˜ä½ï¼‰")

def get_latest_dataset_path():
    latest_dir = get_latest_integrated_dir()
    if not latest_dir:
        return None
    dataset_path = os.path.join(latest_dir, "corporate_bias_dataset.json")
    if not os.path.exists(dataset_path):
        return None
    return dataset_path

dataset_path = get_latest_dataset_path()
if not dataset_path:
    st.warning("corporate_bias_dataset.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
else:
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    categories = list(dataset.keys())
    selected_category = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", categories)
    subcategories = list(dataset[selected_category].keys())
    selected_subcategory = st.selectbox("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", subcategories)
    subcat_data = dataset[selected_category][selected_subcategory]
    st.subheader(f"{selected_category} - {selected_subcategory} ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿")
    st.json(subcat_data)

# --- å‹•çš„å¯è¦–åŒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
st.header("å‹•çš„å¯è¦–åŒ–ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆï¼‰")

def get_latest_bias_analysis():
    """æœ€æ–°ã®bias_analysis_results.jsonã‚’å–å¾—"""
    latest_dir = get_latest_integrated_dir()
    if not latest_dir:
        return None
    bias_path = os.path.join(latest_dir, "bias_analysis_results.json")
    if not os.path.exists(bias_path):
        return None
    with open(bias_path, "r", encoding="utf-8") as f:
        return json.load(f)

bias_data = get_latest_bias_analysis()
if not bias_data:
    st.warning("bias_analysis_results.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
else:
    # å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—é¸æŠ
    viz_type = st.selectbox(
        "å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
        ["Citations-Googleæ¯”è¼ƒ", "æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹åˆ†æ", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æ", "çµ±åˆåˆ†æ"]
    )

    if viz_type == "Citations-Googleæ¯”è¼ƒ":
        st.subheader("Citations-Googleæ¯”è¼ƒåˆ†æ")
        citations_data = bias_data.get("citations_google_comparison", {})

        if not citations_data:
            st.info("Citations-Googleæ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            # ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªé¸æŠ
            categories = list(citations_data.keys())
            if "error" in categories:
                categories.remove("error")

            if categories:
                selected_category = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", categories)
                subcategories = list(citations_data[selected_category].keys())
                selected_subcategory = st.selectbox("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", subcategories)

                comparison_data = citations_data[selected_category][selected_subcategory]

                # ã‚¿ãƒ–ã§å¯è¦–åŒ–ã‚’åˆ†ã‘ã‚‹
                tab1, tab2, tab3, tab4 = st.tabs(["ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦", "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒ", "æ„Ÿæƒ…åˆ†ææ¯”è¼ƒ", "ãƒ‡ãƒ¼ã‚¿å“è³ª"])

                with tab1:
                    if "ranking_similarity" in comparison_data:
                        fig = plot_ranking_similarity(
                            comparison_data["ranking_similarity"],
                            f"{selected_category} - {selected_subcategory}"
                        )
                        st.pyplot(fig)

                        # æŒ‡æ¨™ã®èª¬æ˜
                        st.markdown("""
                        **æŒ‡æ¨™ã®èª¬æ˜:**
                        - **RBO (Rank Biased Overlap)**: ä¸Šä½ã®çµæœã‚’é‡è¦–ã—ãŸé‡è¤‡åº¦ï¼ˆ0-1ï¼‰
                        - **Kendall Tau**: é †ä½ã®ç›¸é–¢ä¿‚æ•°ï¼ˆ-1ã€œ1ï¼‰
                        - **Overlap Ratio**: å…±é€šè¦ç´ ã®å‰²åˆï¼ˆ0-1ï¼‰
                        """)
                    else:
                        st.info("ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                with tab2:
                    if "official_domain_analysis" in comparison_data:
                        fig = plot_official_domain_comparison(
                            comparison_data["official_domain_analysis"],
                            f"{selected_category} - {selected_subcategory}"
                        )
                        st.pyplot(fig)

                        # åˆ†æçµæœã®èª¬æ˜
                        official_data = comparison_data["official_domain_analysis"]
                        st.markdown(f"""
                        **åˆ†æçµæœ:**
                        - Googleæ¤œç´¢å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡: {official_data.get('google_official_ratio', 0):.3f}
                        - Perplexityå…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡: {official_data.get('citations_official_ratio', 0):.3f}
                        - ãƒã‚¤ã‚¢ã‚¹æ–¹å‘: {official_data.get('bias_direction', 'unknown')}
                        """)
                    else:
                        st.info("å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                with tab3:
                    if "sentiment_comparison" in comparison_data:
                        fig = plot_sentiment_comparison(
                            comparison_data["sentiment_comparison"],
                            f"{selected_category} - {selected_subcategory}"
                        )
                        st.pyplot(fig)

                        # ç›¸é–¢æƒ…å ±
                        sentiment_data = comparison_data["sentiment_comparison"]
                        st.markdown(f"""
                        **æ„Ÿæƒ…åˆ†æç›¸é–¢:**
                        - ç›¸é–¢ä¿‚æ•°: {sentiment_data.get('sentiment_correlation', 0):.3f}
                        - ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¤ã‚¢ã‚¹å·®åˆ†: {sentiment_data.get('positive_bias_delta', 0):.3f}
                        """)
                    else:
                        st.info("æ„Ÿæƒ…åˆ†ææ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                with tab4:
                    if "data_quality" in comparison_data:
                        quality_data = comparison_data["data_quality"]
                        st.markdown("**ãƒ‡ãƒ¼ã‚¿å“è³ªæƒ…å ±:**")
                        st.json(quality_data)
                    else:
                        st.info("ãƒ‡ãƒ¼ã‚¿å“è³ªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                st.info("æ¯”è¼ƒå¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚Šã¾ã›ã‚“")

    elif viz_type == "æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹åˆ†æ":
        st.subheader("æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹åˆ†æ")
        sentiment_data = bias_data.get("sentiment_bias_analysis", {})

        if not sentiment_data:
            st.info("æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            # ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªé¸æŠ
            categories = list(sentiment_data.keys())
            selected_category = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", categories)
            subcategories = list(sentiment_data[selected_category].keys())
            selected_subcategory = st.selectbox("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", subcategories)

            entities_data = sentiment_data[selected_category][selected_subcategory].get("entities", {})

            # ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚’æŠ½å‡º
            bias_indices = {}
            for entity, data in entities_data.items():
                if "basic_metrics" in data:
                    bias_indices[entity] = data["basic_metrics"].get("normalized_bias_index", 0)

            if bias_indices:
                fig = plot_sentiment_bias(
                    bias_indices,
                    f"{selected_category} - {selected_subcategory}"
                )
                st.pyplot(fig)
            else:
                st.info("ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    elif viz_type == "ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æ":
        st.subheader("ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æ")
        ranking_data = bias_data.get("ranking_bias_analysis", {})

        if not ranking_data:
            st.info("ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            st.info("ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æã®å¯è¦–åŒ–æ©Ÿèƒ½ã¯é–‹ç™ºä¸­ã§ã™")

    elif viz_type == "çµ±åˆåˆ†æ":
        st.subheader("çµ±åˆåˆ†æ")
        cross_data = bias_data.get("cross_analysis_insights", {})

        if not cross_data:
            st.info("çµ±åˆåˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            st.markdown("**çµ±åˆåˆ†æçµæœ:**")
            st.json(cross_data)

# --- å¾“æ¥ã®ç”»åƒè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå‚è€ƒç”¨ï¼‰ ---
with st.expander("å¾“æ¥ã®äº‹å‰ç”Ÿæˆç”»åƒï¼ˆå‚è€ƒï¼‰"):
    st.header("äº‹å‰ç”Ÿæˆç”»åƒï¼ˆå‚è€ƒç”¨ï¼‰")

    def get_latest_visuals_dir():
        # integratedã®æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨åŒã˜æ—¥ä»˜ã®analysis_visuals/é…ä¸‹ã‚’å‚ç…§
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»S3å…±ã«corporate_bias_datasets/analysis_visuals/YYYYMMDD/ã«çµ±ä¸€
        latest_dir = get_latest_integrated_dir()
        if not latest_dir:
            return None
        date_str = os.path.basename(latest_dir)
        visuals_dir = os.path.join("corporate_bias_datasets/analysis_visuals", date_str)
        if not os.path.exists(visuals_dir):
            return None
        return visuals_dir

    visuals_dir = get_latest_visuals_dir()
    if not visuals_dir:
        st.warning("ç”»åƒæŒ‡æ¨™ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    else:
        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨ã«ç”»åƒã‚’è¡¨ç¤º
        for subdir in os.listdir(visuals_dir):
            subdir_path = os.path.join(visuals_dir, subdir)
            if os.path.isdir(subdir_path):
                st.subheader(f"{subdir} ã®ç”»åƒæŒ‡æ¨™")
                images = [f for f in os.listdir(subdir_path) if f.lower().endswith(".png")]
                for img in images:
                    img_path = os.path.join(subdir_path, img)
                    st.image(img_path, caption=img)