#!/usr/bin/env python
# coding: utf-8

"""
ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æ - ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

Streamlitã‚’ä½¿ç”¨ã—ã¦ã€ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æã®çµæœãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã™ã€‚
å‹•çš„å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼šäº‹å‰ç”Ÿæˆç”»åƒã§ã¯ãªãã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from src.utils.plot_utils import draw_reliability_badge
import numpy as np
from src.analysis.hybrid_data_loader import HybridDataLoader
import japanize_matplotlib
from src.utils.plot_utils import plot_severity_radar, plot_pvalue_heatmap, plot_stability_score_distribution
import os
from src.utils.storage_config import get_base_paths
import plotly.graph_objects as go
import time
from src.utils.plot_utils import (
    plot_ranking_similarity_for_ranking_analysis, plot_bias_indices_bar_for_ranking_analysis,
    plot_sentiment_ranking_correlation_scatter
)

# èªè¨¼æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.components.auth_ui import render_auth_page, show_dashboard_header
from src.utils.auth_utils import validate_auth_config, is_authenticated

# Google Analytics 4çµ±åˆ
from src.utils.analytics_utils import render_ga4_tracking, is_ga4_enabled, get_ga4_status_info

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Google Analytics 4 ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°åˆæœŸåŒ–
render_ga4_tracking()

# èªè¨¼ãƒã‚§ãƒƒã‚¯
def check_authentication():
    """èªè¨¼çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€æœªèªè¨¼ã®å ´åˆã¯èªè¨¼ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤º"""
    # OAuthèªè¨¼ãƒ•ãƒ©ã‚°ã®ç¢ºèª
    oauth_flag = os.getenv('OAUTH_FLAG', 'true').lower()
    if oauth_flag in ['false', '0', 'no']:
        st.info("èªè¨¼æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        return True

    # èªè¨¼è¨­å®šã®æ¤œè¨¼
    if not validate_auth_config():
        st.error("èªè¨¼è¨­å®šãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚")
        return False

    # èªè¨¼çŠ¶æ…‹ã®ãƒã‚§ãƒƒã‚¯
    if not is_authenticated():
        render_auth_page()
        st.stop()
        return False

    return True

# èªè¨¼ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
if check_authentication():
    # èªè¨¼æˆåŠŸæ™‚ã®ã¿ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤º
    if 'user_info' in st.session_state:
        show_dashboard_header(st.session_state.user_info)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
@st.cache_data(ttl=3600)  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_cached_dashboard_data(_loader, selected_date):
    """
    ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§å–å¾—

    Parameters:
    -----------
    _loader : HybridDataLoader
        ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‹ã‚‰é™¤å¤–ï¼‰
    selected_date : str
        é¸æŠã•ã‚ŒãŸæ—¥ä»˜

    Returns:
    --------
    dict
        ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
    """
    return _loader.get_integrated_dashboard_data(selected_date)

# éåŒæœŸãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
def get_dashboard_data_async(_loader, selected_date):
    """
    ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’éåŒæœŸã§å–å¾—ï¼ˆèª­ã¿è¾¼ã¿çŠ¶æ³ã‚’è¡¨ç¤ºï¼‰

    Parameters:
    -----------
    _loader : HybridDataLoader
        ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    selected_date : str
        é¸æŠã•ã‚ŒãŸæ—¥ä»˜

    Returns:
    --------
    dict
        ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
    """
    # èª­ã¿è¾¼ã¿é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    start_time = datetime.now()

    # èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’è¡¨ç¤º
    with st.spinner(f"ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {selected_date}..."):
        try:
            data = get_cached_dashboard_data(_loader, selected_date)
            end_time = datetime.now()
            load_time = (end_time - start_time).total_seconds()

            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’åˆ¤å®šï¼ˆS3/Localï¼‰
            data_source = data.get("metadata", {}).get("storage_mode", "Local") if data else "Local"

            # èª­ã¿è¾¼ã¿æƒ…å ±ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            if "load_status" not in st.session_state:
                st.session_state.load_status = []

            if load_time < 0.1:  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã•ã‚ŒãŸå ´åˆ
                st.session_state.load_status.append(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿: {selected_date} ({data_source})")
            else:
                st.session_state.load_status.append(f"ğŸ“¥ æ–°è¦èª­ã¿è¾¼ã¿: {selected_date} ({data_source}) ({load_time:.2f}ç§’)")

            if not data:
                st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {selected_date}")

            return data
        except Exception as e:
            end_time = datetime.now()
            load_time = (end_time - start_time).total_seconds()
            st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {selected_date} (èª­ã¿è¾¼ã¿æ™‚é–“: {load_time:.2f}ç§’)")
            st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            return None

def render_load_status(expanded=False, key_prefix="", simplified=False):
    """
    èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’è¡¨ç¤ºã™ã‚‹å…±é€šé–¢æ•°

    Parameters:
    -----------
    expanded : bool
        ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã®åˆæœŸå±•é–‹çŠ¶æ…‹
    key_prefix : str
        ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚­ãƒ¼ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    simplified : bool
        ç°¡ç´ åŒ–è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆå˜æ—¥åˆ†æç”¨ï¼‰
    """
    if hasattr(st.session_state, 'load_status') and st.session_state.load_status:
        if simplified:
            # ç°¡ç´ åŒ–è¡¨ç¤ºï¼šæœ€æ–°ã®èª­ã¿è¾¼ã¿æƒ…å ±ã®ã¿ã‚’ç·‘è‰²ã§ç›´æ¥è¡¨ç¤ºï¼ˆã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ãªã—ï¼‰
            latest_item = st.session_state.load_status[-1]
            st.sidebar.success(latest_item)
        else:
            # é€šå¸¸è¡¨ç¤ºï¼šã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ä»˜ãã§çµ±è¨ˆæƒ…å ±ã¨ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
            with st.sidebar.expander("ğŸ“Š èª­ã¿è¾¼ã¿çŠ¶æ³", expanded=expanded):
                # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
                total_loads = len(st.session_state.load_status)
                new_loads = sum(1 for item in st.session_state.load_status if "æ–°è¦èª­ã¿è¾¼ã¿" in item)
                cache_loads = sum(1 for item in st.session_state.load_status if "ã‚­ãƒ£ãƒƒã‚·ãƒ¥" in item)
                st.caption(f"ğŸ“ˆ ç·èª­ã¿è¾¼ã¿: {total_loads}ä»¶ (æ–°è¦: {new_loads}ä»¶, ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cache_loads}ä»¶)")

                # èª­ã¿è¾¼ã¿ãƒªã‚¹ãƒˆã‚’è¡¨ç¤ºï¼ˆå…¨ä»¶è¡¨ç¤ºï¼‰
                for item in st.session_state.load_status:
                    if "æ–°è¦èª­ã¿è¾¼ã¿" in item:
                        st.info(item)
                    else:
                        st.success(item)



# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
# load_dotenv() # å‰Šé™¤

# ã‚°ãƒ©ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
def set_plot_style():
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams['figure.figsize'] = (6, 3)  # ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å°ã•ãè¨­å®š
    plt.rcParams['font.size'] = 9
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['axes.titlesize'] = 12
    plt.rcParams['xtick.labelsize'] = 8
    plt.rcParams['ytick.labelsize'] = 8
    plt.rcParams['legend.fontsize'] = 8
    plt.rcParams['figure.titlesize'] = 14
    plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã‚’æ­£ã—ãè¡¨ç¤º
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¤‡æ•°æŒ‡å®š
    plt.rcParams['font.family'] = ['IPAexGothic', 'Noto Sans CJK JP', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAPGothic', 'VL PGothic', 'DejaVu Sans']

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®šã‚’é©ç”¨
set_plot_style()

# çµ±ä¸€ã•ã‚ŒãŸCSSè¨­å®š
# ï¼ˆmain-dashboard-areaã‚„stDataFrameç­‰ã®ã‚«ã‚¹ã‚¿ãƒ CSSã¯å‰Šé™¤ï¼‰

# å‹•çš„å¯è¦–åŒ–é–¢æ•°ç¾¤
def plot_ranking_similarity(similarity_data, title):
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ã®å‹•çš„å¯è¦–åŒ–"""
    metrics = ['rbo_score', 'kendall_tau', 'overlap_ratio']
    values = [similarity_data.get(metric, 0) for metric in metrics]
    labels = ['RBO\n(ä¸Šä½é‡è¦–é‡è¤‡åº¦)', 'Kendall Tau\n(é †ä½ç›¸é–¢)', 'Overlap Ratio\n(å…±é€šè¦ç´ ç‡)']

    fig, ax = plt.subplots(figsize=(8, 6))  # ã‚µã‚¤ã‚ºã‚’èª¿æ•´
    bars = ax.bar(labels, values, alpha=0.7, color=['blue', 'orange', 'green'])
    ax.set_title(f"{title} - Google vs Perplexityé¡ä¼¼åº¦")
    ax.set_ylabel("é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢")

    # Yè»¸ç¯„å›²ã‚’å‹•çš„ã«èª¿æ•´ï¼ˆKendall Tauã¯-1ã‹ã‚‰1ã®ç¯„å›²ï¼‰
    min_val = min(values) if values else 0
    max_val = max(values) if values else 1
    y_margin = 0.1  # ä¸Šä¸‹ã®ãƒãƒ¼ã‚¸ãƒ³

    # ãƒã‚¤ãƒŠã‚¹å€¤ãŒã‚ã‚‹å ´åˆã¯Yè»¸ç¯„å›²ã‚’èª¿æ•´
    if min_val < 0:
        ax.set_ylim(min_val - y_margin, max_val + y_margin)
        # ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³ã‚’è¿½åŠ 
        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3, linewidth=0.5)
    else:
        ax.set_ylim(0, max_val + y_margin)

    # Xè»¸ãƒ©ãƒ™ãƒ«ã®å›è»¢ã¨ä½ç½®èª¿æ•´
    plt.xticks(rotation=0, ha='center')

    # å€¤ãƒ©ãƒ™ãƒ«è¿½åŠ ï¼ˆãƒã‚¤ãƒŠã‚¹å€¤ã‚‚è€ƒæ…®ï¼‰
    for bar, value in zip(bars, values):
        if value is not None:
            # ãƒã‚¤ãƒŠã‚¹å€¤ã®å ´åˆã¯ãƒãƒ¼ã®ä¸‹ã«ã€ãƒ—ãƒ©ã‚¹å€¤ã®å ´åˆã¯ãƒãƒ¼ã®ä¸Šã«ãƒ©ãƒ™ãƒ«ã‚’é…ç½®
            if value < 0:
                ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() - 0.05,
                        f'{value:.3f}', ha='center', va='top', fontweight='bold')
            else:
                ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                        f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

    # ã‚°ãƒªãƒƒãƒ‰è¿½åŠ 
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    return fig

def plot_bias_indices_bar(bias_data, title, reliability_label=None):
    """æ„Ÿæƒ…ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã®å‹•çš„å¯è¦–åŒ–"""
    entities = list(bias_data.keys())
    values = [bias_data[e] for e in entities]

    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°ã«å¿œã˜ã¦ã‚µã‚¤ã‚ºã‚’èª¿æ•´
    if len(entities) > 10:
        fig, ax = plt.subplots(figsize=(12, 6))  # å¤šã„å ´åˆã¯æ¨ªã«åºƒã’ã‚‹
    else:
        fig, ax = plt.subplots(figsize=(8, 6))   # å°‘ãªã„å ´åˆã¯æ¨™æº–ã‚µã‚¤ã‚º

    bars = ax.bar(entities, values, color=["red" if v > 0 else "green" for v in values])
    ax.axhline(0, color='k', linestyle='--', alpha=0.5)
    ax.set_ylabel("Normalized Bias Index (BI)")
    ax.set_title(title)

    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã‚’å³å´ã«ã‚·ãƒ•ãƒˆï¼ˆä½ç½®ã‚’0.2ã ã‘å³ã«ãšã‚‰ã™ï¼‰
    x_positions = range(len(entities))
    shifted_positions = [x + 0.2 for x in x_positions]
    ax.set_xticks(shifted_positions)
    ax.set_xticklabels(entities, rotation=30, ha="right", fontsize=14)

    plt.subplots_adjust(bottom=0.25)
    plt.tight_layout()

    # ä¿¡é ¼æ€§ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
    if reliability_label:
        draw_reliability_badge(ax, reliability_label)

    return fig

def plot_official_domain_comparison(official_data, title):
    """å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒã®å‹•çš„å¯è¦–åŒ–"""
    google_ratio = official_data.get("google_official_ratio", 0)
    citations_ratio = official_data.get("citations_official_ratio", 0)

    fig, ax = plt.subplots(figsize=(6, 5))  # ã‚µã‚¤ã‚ºã‚’èª¿æ•´
    labels = ['Googleæ¤œç´¢', 'Perplexity Citations']
    values = [google_ratio, citations_ratio]
    colors = ['blue', 'orange']

    bars = ax.bar(labels, values, color=colors, alpha=0.7)
    ax.set_title(f"{title} - å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡æ¯”è¼ƒ")
    ax.set_ylabel("å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡")
    ax.set_ylim(0, 1)

    for bar, value in zip(bars, values):
        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    return fig

def plot_sentiment_comparison(sentiment_data, title):
    """æ„Ÿæƒ…åˆ†ææ¯”è¼ƒã®å‹•çš„å¯è¦–åŒ–"""
    google_dist = sentiment_data.get("google_sentiment_distribution", {})
    citations_dist = sentiment_data.get("citations_sentiment_distribution", {})

    fig, ax = plt.subplots(figsize=(8, 5))  # ã‚µã‚¤ã‚ºã‚’èª¿æ•´

    categories = ['positive', 'negative', 'neutral', 'unknown']
    x = np.arange(len(categories))
    width = 0.35

    google_values = [google_dist.get(cat, 0) for cat in categories]
    citations_values = [citations_dist.get(cat, 0) for cat in categories]

    ax.bar(x - width/2, google_values, width, label='Googleæ¤œç´¢', alpha=0.7, color='blue')
    ax.bar(x + width/2, citations_values, width, label='Perplexity Citations', alpha=0.7, color='orange')

    ax.set_title(f"{title} - æ„Ÿæƒ…åˆ†å¸ƒæ¯”è¼ƒ")
    ax.set_ylabel("æ¯”ç‡")
    ax.set_xticks(x)
    ax.set_xticklabels(['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ä¸­ç«‹', 'ä¸æ˜'])
    ax.legend()
    ax.set_ylim(0, 1)

    plt.tight_layout()
    return fig

def _display_ranking_interpretation(metrics_data, result_type):
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ã®è§£èª¬ã‚’è¡¨ç¤º"""
    if not metrics_data or "error" in metrics_data:
        st.info(f"{result_type}ã®ã‚°ãƒ©ãƒ•è§£èª¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    st.markdown("**ğŸ“Š ã‚°ãƒ©ãƒ•è§£èª¬**")

    # å€‹åˆ¥æŒ‡æ¨™è§£é‡ˆ
    kendall_interpretation = metrics_data.get("kendall_tau_interpretation", "")
    rbo_interpretation = metrics_data.get("rbo_interpretation", "")
    overall_similarity = metrics_data.get("overall_similarity_level", "")

    if kendall_interpretation:
        st.markdown(f"**Kendall Tauè§£é‡ˆ**: {kendall_interpretation}")
    if rbo_interpretation:
        st.markdown(f"**RBOè§£é‡ˆ**: {rbo_interpretation}")

    # çµ±åˆè§£é‡ˆ
    if overall_similarity:
        similarity_text = {
            "high": "é«˜ã„é¡ä¼¼åº¦",
            "medium": "ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦",
            "low": "ä½ã„é¡ä¼¼åº¦"
        }.get(overall_similarity, overall_similarity)
        st.markdown(f"**çµ±åˆè§£é‡ˆ**: {result_type}ã§{similarity_text}ã‚’ç¤ºã—ã¦ã„ã¾ã™")

    # å…±é€šã‚µã‚¤ãƒˆæƒ…å ±
    common_count = metrics_data.get("common_domains_count", 0)
    overlap_ratio = metrics_data.get("overlap_ratio", 0)
    if common_count > 0:
        st.markdown(f"**å…±é€šã‚µã‚¤ãƒˆ**: {common_count}å€‹ï¼ˆé‡è¤‡ç‡: {overlap_ratio:.1%}ï¼‰")

def get_reliability_label(execution_count):
    """å®Ÿè¡Œå›æ•°ã«åŸºã¥ã„ã¦ä¿¡é ¼æ€§ãƒ©ãƒ™ãƒ«ã‚’å–å¾—"""
    if execution_count >= 15:
        return "é«˜ä¿¡é ¼æ€§"
    elif execution_count >= 10:
        return "ä¸­ä¿¡é ¼æ€§"
    elif execution_count >= 5:
        return "æ¨™æº–"
    elif execution_count >= 2:
        return "å‚è€ƒ"
    else:
        return "å‚è€ƒï¼ˆå®Ÿè¡Œå›æ•°ä¸è¶³ï¼‰"

def plot_effect_significance_scatter(effect_data, title, reliability_label=None):
    """åŠ¹æœé‡ vs på€¤æ•£å¸ƒå›³ã®å‹•çš„å¯è¦–åŒ–"""
    entities = list(effect_data.keys())
    cliffs = [effect_data[e]["cliffs_delta"] for e in entities]
    pvals = [effect_data[e]["p_value"] for e in entities]
    fig, ax = plt.subplots(figsize=(8, 6))
    scatter = ax.scatter(cliffs, [-np.log10(p) if p > 0 else 0 for p in pvals], c=["red" if c > 0 else "green" for c in cliffs], s=80)
    for i, e in enumerate(entities):
        ax.annotate(e, (cliffs[i], -np.log10(pvals[i]) if pvals[i] > 0 else 0), fontsize=10)
    ax.set_xlabel("Cliff's Delta")
    ax.set_ylabel("-log10(på€¤)")
    ax.set_title(title)
    if reliability_label:
        draw_reliability_badge(ax, reliability_label)
    plt.tight_layout()
    return fig

# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§storage-modeã‚’å—ã‘å–ã‚‹
if not hasattr(st, 'session_state') or 'storage_mode' not in st.session_state:
    st.session_state['storage_mode'] = 'auto' # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ‡ãƒ¼ã‚¿å–å¾—å…ƒã‚’é¸æŠï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆï¼‰
def get_storage_mode():
    cli_mode = st.session_state.get('storage_mode', 'auto')
    if 'storage_mode_sidebar' not in st.session_state:
        st.session_state['storage_mode_sidebar'] = cli_mode
    mode = st.sidebar.selectbox(
        'ãƒ‡ãƒ¼ã‚¿å–å¾—å…ƒã‚’é¸æŠ',
        ['auto', 'local', 's3'],
        index=['auto', 'local', 's3'].index(cli_mode),
        key='storage_mode_sidebar',
        help='auto: ãƒ­ãƒ¼ã‚«ãƒ«å„ªå…ˆã€ãªã‘ã‚Œã°S3 / local: ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿ / s3: S3ã®ã¿'
    )
    return mode

storage_mode = get_storage_mode()

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
st.markdown("AIæ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã«ãŠã‘ã‚‹ä¼æ¥­å„ªé‡ãƒã‚¤ã‚¢ã‚¹ã®å¯è¦–åŒ–")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---
st.sidebar.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿é¸æŠ")

# Google Analytics 4 ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
ga4_status = get_ga4_status_info()
if ga4_status['enabled']:
    st.sidebar.success(f"ğŸ” GA4: {ga4_status['status']}")
else:
    # ç®¡ç†è€…å‘ã‘ã«è¨­å®šãªã—çŠ¶æ…‹ã‚’è¡¨ç¤ºï¼ˆä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯å½±éŸ¿ã—ãªã„ï¼‰
    with st.sidebar.expander("ğŸ” Analytics", expanded=False):
        st.caption(f"GA4: {ga4_status['message']}")

# å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—é¸æŠï¼ˆå˜æ—¥åˆ†æãƒ»æ™‚ç³»åˆ—åˆ†æï¼‰
viz_type = st.sidebar.selectbox(
    "å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
    ["å˜æ—¥åˆ†æ", "æ™‚ç³»åˆ—åˆ†æ"],
    key="analysis_type_selector"
)

# --- ãƒ‡ãƒ¼ã‚¿å–å¾—å…ƒã€Œautoã€æ™‚ã®local/S3ä¸¡æ–¹å€™è£œãƒªã‚¹ãƒˆè¡¨ç¤º ---
if storage_mode == "auto":
    loader_local = HybridDataLoader("local")
    loader_s3 = HybridDataLoader("s3")
    dates_local = set(loader_local.list_available_dates(mode="local"))
    dates_s3 = set(loader_s3.list_available_dates(mode="s3"))
    all_dates = sorted(list(dates_local | dates_s3), reverse=True)
    date_source_options = []
    for d in all_dates:
        if d in dates_local:
            date_source_options.append(f"local: {d}")
        if d in dates_s3:
            date_source_options.append(f"S3: {d}")
    if not date_source_options:
        st.sidebar.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.stop()
    if viz_type == "å˜æ—¥åˆ†æ":
        selected_date_source = st.sidebar.selectbox(
            "åˆ†ææ—¥ä»˜ã¨å–å¾—å…ƒã‚’é¸æŠ",
            date_source_options,
            index=0,
            key="date_source_selector"
        )
        # é¸æŠã«å¿œã˜ã¦loaderã¨dateã‚’æ±ºå®š
        if selected_date_source.startswith("local: "):
            loader = loader_local
            selected_date = selected_date_source.replace("local: ", "")
        else:
            loader = loader_s3
            selected_date = selected_date_source.replace("S3: ", "")
        selected_dates = [selected_date]
    # --- æ™‚ç³»åˆ—åˆ†ææ™‚ã¯ã“ã®UIã‚’è¡¨ç¤ºã—ãªã„ ---
else:
    loader = HybridDataLoader(storage_mode)
    available_dates = loader.list_available_dates(mode=storage_mode)
    if not available_dates:
        st.sidebar.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.stop()
    if viz_type == "å˜æ—¥åˆ†æ":
        selected_date = st.sidebar.selectbox(
            "åˆ†ææ—¥ä»˜ã‚’é¸æŠ",
            available_dates,
            index=0,
            key="date_selector"
        )
        selected_dates = [selected_date]
if viz_type == "æ™‚ç³»åˆ—åˆ†æ":
    loader_local = HybridDataLoader("local")
    loader_s3 = HybridDataLoader("s3")
    dates_local = set(loader_local.list_available_dates(mode="local"))
    dates_s3 = set(loader_s3.list_available_dates(mode="s3"))
    all_dates = sorted(list(dates_local | dates_s3))

    best_data_by_date = {}

    # é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, date in enumerate(all_dates):
        # é€²æ—ã‚’æ›´æ–°
        progress = (i + 1) / len(all_dates)
        progress_bar.progress(progress)
        status_text.text(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... {i+1}/{len(all_dates)}: {date}")

        # éåŒæœŸã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        data_local = get_dashboard_data_async(loader_local, date) if date in dates_local else None
        data_s3 = get_dashboard_data_async(loader_s3, date) if date in dates_s3 else None
        def get_meta(d):
            if d and "analysis_results" in d and "metadata" in d["analysis_results"]:
                meta = d["analysis_results"]["metadata"]
                return meta.get("execution_count", 0), meta.get("analysis_date", "")
            return 0, ""
        exec_local, date_local = get_meta(data_local)
        exec_s3, date_s3 = get_meta(data_s3)
        # ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãŒNoneã®å ´åˆã¯é™¤å¤–
        if data_local is None and data_s3 is None:
            continue

        if exec_local > exec_s3:
            best_data_by_date[date] = (data_local, "local")
        elif exec_s3 > exec_local:
            best_data_by_date[date] = (data_s3, "s3")
        else:
            if date_local >= date_s3:
                best_data_by_date[date] = (data_local, "local")
            else:
                best_data_by_date[date] = (data_s3, "s3")

    # é€²æ—ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
    progress_bar.empty()
    status_text.empty()

    # èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’è¡¨ç¤ºï¼ˆçµ±åˆç‰ˆï¼‰
    render_load_status(expanded=False, key_prefix="")

    available_dates = sorted(best_data_by_date.keys())
    # è¡¨ç¤ºæœŸé–“é¸æŠ
    period_options = {
        "1ãƒ¶æœˆ": 4,
        "3ãƒ¶æœˆ": 12,
        "åŠå¹´": 24,
        "1å¹´": 52,
        "å…¨æœŸé–“": None
    }
    selected_period = st.sidebar.selectbox(
        "è¡¨ç¤ºæœŸé–“ã‚’é¸æŠ",
        list(period_options.keys()),
        index=2,
        key="ts_period_selector"
    )

    period_n = period_options[selected_period]
    sorted_dates = sorted(available_dates, reverse=True)

    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if period_n is not None:
        selected_dates = sorted(sorted_dates[:period_n], reverse=False)
    else:
        selected_dates = sorted(available_dates)
    if not selected_dates:
        st.info("åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()


    latest_date = max(selected_dates)
    dashboard_data, source = best_data_by_date[latest_date]
    if dashboard_data is None:
        if source == 'local':
            file_path = os.path.join(get_base_paths(latest_date)['integrated'], 'bias_analysis_results.json')
        else:
            file_path = f"s3://{get_base_paths(latest_date)['integrated']}/bias_analysis_results.json"
        st.error(
            f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {latest_date}\n"
            f"å–å¾—å…ƒ: {source}\n"
            f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {file_path}\n"
            "è©²å½“æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ã‹ã€èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¦ã„ã¾ã™ã€‚"
        )
        st.stop()
    if "analysis_results" not in dashboard_data:
        st.error(
            f"analysis_resultsãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {latest_date}\n"
            f"å–å¾—å…ƒ: {source}\n"
            f"ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹: {str(dashboard_data)[:500]}..."
        )
        st.stop()
    analysis_data = dashboard_data["analysis_results"]
    sentiment_data = analysis_data.get("sentiment_bias_analysis", {})
    all_categories = [c for c in sentiment_data.keys() if c not in ("å…¨ä½“", "all", "ALL", "All")]
    all_categories.sort()
    if not all_categories:
        st.info("ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()
    selected_category = st.sidebar.selectbox(
        "ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
        all_categories,
        key="ts_category_selector",
        index=0
    )
    all_subcategories = list(sentiment_data[selected_category].keys())
    all_subcategories.sort()
    if not all_subcategories:
        st.info("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()
    selected_subcategory = st.sidebar.selectbox(
        "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
        all_subcategories,
        key="ts_subcategory_selector",
        index=0
    )
    entities_data = sentiment_data[selected_category][selected_subcategory].get("entities", {})
    entities = list(entities_data.keys())
    if not entities:
        st.info("ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()
    selected_entities = st.sidebar.multiselect(
        "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
        entities,
        default=entities,  # å…¨ä»¶ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¡¨ç¤º
        key="ts_entities_selector"
    )
    if not selected_entities:
        st.info("ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é¸æŠã—ã¦ãã ã•ã„")
        st.stop()



    # --- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿åé›† ---
    bi_timeseries = {entity: [] for entity in selected_entities}
    sentiment_timeseries = {entity: [] for entity in selected_entities}
    ranking_timeseries = {entity: [] for entity in selected_entities}

    # æ–°è¦è¿½åŠ ï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ã¨å…¬å¼/éå…¬å¼æ¯”ç‡ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
    rbo_timeseries = []
    kendall_tau_timeseries = []
    overlap_ratio_timeseries = []
    google_official_ratio_timeseries = []
    citations_official_ratio_timeseries = []
    official_bias_delta_timeseries = []

    # æ–°è¦è¿½åŠ ï¼šãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
    google_positive_ratio_timeseries = []
    google_negative_ratio_timeseries = []
    citations_positive_ratio_timeseries = []
    citations_negative_ratio_timeseries = []
    positive_bias_delta_timeseries = []

    date_labels = []

    for date in selected_dates:
        dashboard_data, source = best_data_by_date[date]
        if dashboard_data is None:
            continue
        analysis_data = dashboard_data["analysis_results"]
        sentiment_data = analysis_data.get("sentiment_bias_analysis", {})
        ranking_data = analysis_data.get("ranking_bias_analysis", {})
        citations_google_data = analysis_data.get("citations_google_comparison", {})

        # æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿
        subcat_data = sentiment_data.get(selected_category, {}).get(selected_subcategory, {})
        entities_data = subcat_data.get("entities", {})
        # Noneã‚„dictä»¥å¤–ã‚’é™¤å¤–
        entities_data = {k: v for k, v in entities_data.items() if isinstance(v, dict)}

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æãƒ‡ãƒ¼ã‚¿
        ranking_subcat_data = ranking_data.get(selected_category, {}).get(selected_subcategory, {})
        ranking_entities_data = ranking_subcat_data.get("entities", {})

        # æ–°è¦è¿½åŠ ï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ‡ãƒ¼ã‚¿
        similarity_data = citations_google_data.get(selected_category, {}).get(selected_subcategory, {}).get("ranking_similarity", {})
        rbo_timeseries.append(similarity_data.get("rbo_score"))
        kendall_tau_timeseries.append(similarity_data.get("kendall_tau"))
        overlap_ratio_timeseries.append(similarity_data.get("overlap_ratio"))

        # æ–°è¦è¿½åŠ ï¼šå…¬å¼/éå…¬å¼æ¯”ç‡ãƒ‡ãƒ¼ã‚¿
        official_data = citations_google_data.get(selected_category, {}).get(selected_subcategory, {}).get("official_domain_analysis", {})
        google_official_ratio_timeseries.append(official_data.get("google_official_ratio"))
        citations_official_ratio_timeseries.append(official_data.get("citations_official_ratio"))
        official_bias_delta_timeseries.append(official_data.get("official_bias_delta"))

        # æ–°è¦è¿½åŠ ï¼šãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡ãƒ‡ãƒ¼ã‚¿
        sentiment_comparison_data = citations_google_data.get(selected_category, {}).get(selected_subcategory, {}).get("sentiment_comparison", {})
        google_sentiment_dist = sentiment_comparison_data.get("google_sentiment_distribution", {})
        citations_sentiment_dist = sentiment_comparison_data.get("citations_sentiment_distribution", {})

        google_positive_ratio_timeseries.append(google_sentiment_dist.get("positive"))
        google_negative_ratio_timeseries.append(google_sentiment_dist.get("negative"))
        citations_positive_ratio_timeseries.append(citations_sentiment_dist.get("positive"))
        citations_negative_ratio_timeseries.append(citations_sentiment_dist.get("negative"))
        positive_bias_delta_timeseries.append(sentiment_comparison_data.get("positive_bias_delta"))

        date_labels.append(date)

        for entity in selected_entities:
            # BIå€¤
            bi = None
            if entity in entities_data:
                bi = entities_data[entity].get("basic_metrics", {}).get("normalized_bias_index")
            bi_timeseries[entity].append(bi)

            # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å·®åˆ†ï¼ˆraw_deltaï¼‰
            sentiment_avg = None
            if entity in entities_data:
                sentiment_avg = entities_data[entity].get("basic_metrics", {}).get("raw_delta")
            sentiment_timeseries[entity].append(sentiment_avg)

            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¹³å‡ï¼ˆæ­£ã—ã„ãƒ‘ã‚¹ã§å–å¾—ï¼‰
            ranking_avg = None
            if entity in ranking_entities_data:
                ranking_avg = ranking_entities_data[entity].get("avg_rank")
            ranking_timeseries[entity].append(ranking_avg)

    # ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸã‹ãƒã‚§ãƒƒã‚¯
    if not date_labels:
        st.error("é¸æŠã•ã‚ŒãŸæœŸé–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®æœŸé–“ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # --- æ™‚ç³»åˆ—åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ---
    st.subheader(f"æ™‚ç³»åˆ—åˆ†æï½œ{selected_category}ï½œ{selected_subcategory}")

    # ã‚¿ãƒ–ã§å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ
    ts_tabs = st.tabs(["BIå€¤æ™‚ç³»åˆ—æ¨ç§»", "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢æ™‚ç³»åˆ—æ¨ç§»", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ™‚ç³»åˆ—æ¨ç§»", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦æ™‚ç³»åˆ—æ¨ç§»", "å…¬å¼/éå…¬å¼æ¯”ç‡æ™‚ç³»åˆ—æ¨ç§»", "ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡æ™‚ç³»åˆ—æ¨ç§»"])
    import matplotlib.pyplot as plt

    # BIå€¤æ™‚ç³»åˆ—æ¨ç§»ã‚¿ãƒ–
    with ts_tabs[0]:
        st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®Normalized Bias Indexï¼ˆãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ï¼‰ã®æ™‚ç³»åˆ—æ¨ç§»ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚å€¤ãŒå¤§ãã„ã»ã©ãƒã‚¤ã‚¢ã‚¹ãŒå¼·ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚", icon="â„¹ï¸")

        # ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        valid_bi_data = {k: v for k, v in bi_timeseries.items() if any(x is not None for x in v)}

        if not valid_bi_data:
            st.warning("BIå€¤ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚normalized_bias_indexãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            fig, ax = plt.subplots(figsize=(10, 6))
            for entity, values in valid_bi_data.items():
                # Noneå€¤ã‚’é™¤å¤–ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
                valid_values = [(i, v) for i, v in enumerate(values) if v is not None]
                if valid_values:
                    x_indices, y_values = zip(*valid_values)
                    x_dates = [date_labels[i] for i in x_indices]
                    ax.plot(x_dates, y_values, marker="o", label=entity, linewidth=2, markersize=6)

            ax.set_xlabel("æ—¥ä»˜")
            ax.set_ylabel("BIå€¤ï¼ˆnormalized_bias_indexï¼‰")
            ax.set_title(f"BIå€¤ã®æ™‚ç³»åˆ—æ¨ç§»ï¼ˆ{selected_category} - {selected_subcategory}ï¼‰")
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            # 0.0ã®åŸºæº–ç·šã‚’è¿½åŠ 
            ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)
            plt.tight_layout()
            st.pyplot(fig, use_container_width=True)
            plt.close(fig)

            # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
            with st.expander("BIå€¤çµ±è¨ˆæƒ…å ±", expanded=False):
                for entity, values in valid_bi_data.items():
                    valid_values = [v for v in values if v is not None]
                    if valid_values:
                        avg_val = sum(valid_values) / len(valid_values)
                        min_val = min(valid_values)
                        max_val = max(valid_values)
                        bias_direction = "æ­£ã®ãƒã‚¤ã‚¢ã‚¹" if avg_val > 0 else "è² ã®ãƒã‚¤ã‚¢ã‚¹" if avg_val < 0 else "ãƒã‚¤ã‚¢ã‚¹ãªã—"
                        st.write(f"**{entity}**: å¹³å‡BI={avg_val:.3f}, æœ€å°={min_val:.3f}, æœ€å¤§={max_val:.3f} ({bias_direction})")

        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢æ™‚ç³»åˆ—æ¨ç§»ã‚¿ãƒ–
    with ts_tabs[1]:
        st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å·®åˆ†ï¼ˆraw_deltaï¼‰ã®æ™‚ç³»åˆ—æ¨ç§»ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚å€¤ãŒé«˜ã„ã»ã©å¥½æ„çš„ãªãƒã‚¤ã‚¢ã‚¹ã‚’ç¤ºã—ã¾ã™ã€‚", icon="â„¹ï¸")

        # ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        valid_sentiment_data = {k: v for k, v in sentiment_timeseries.items() if any(x is not None for x in v)}

        if not valid_sentiment_data:
            st.warning("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚raw_deltaãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            fig, ax = plt.subplots(figsize=(10, 6))
            for entity, values in valid_sentiment_data.items():
                # Noneå€¤ã‚’é™¤å¤–ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
                valid_values = [(i, v) for i, v in enumerate(values) if v is not None]
                if valid_values:
                    x_indices, y_values = zip(*valid_values)
                    x_dates = [date_labels[i] for i in x_indices]
                    ax.plot(x_dates, y_values, marker="s", label=entity, linewidth=2, markersize=6)

            ax.set_xlabel("æ—¥ä»˜")
            ax.set_ylabel("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å·®åˆ†ï¼ˆraw_deltaï¼‰")
            ax.set_title(f"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å·®åˆ†ã®æ™‚ç³»åˆ—æ¨ç§»ï¼ˆ{selected_category} - {selected_subcategory}ï¼‰")
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            # 0.0ã®åŸºæº–ç·šã‚’è¿½åŠ 
            ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)
            plt.tight_layout()
            st.pyplot(fig, use_container_width=True)
            plt.close(fig)

            # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
            with st.expander("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢çµ±è¨ˆæƒ…å ±", expanded=False):
                for entity, values in valid_sentiment_data.items():
                    valid_values = [v for v in values if v is not None]
                    if valid_values:
                        avg_val = sum(valid_values) / len(valid_values)
                        min_val = min(valid_values)
                        max_val = max(valid_values)
                        bias_direction = "å¥½æ„çš„ãƒã‚¤ã‚¢ã‚¹" if avg_val > 0 else "å¦å®šçš„ãƒã‚¤ã‚¢ã‚¹" if avg_val < 0 else "ãƒã‚¤ã‚¢ã‚¹ãªã—"
                        st.write(f"**{entity}**: å¹³å‡å·®åˆ†={avg_val:.2f}, æœ€å°={min_val:.2f}, æœ€å¤§={max_val:.2f} ({bias_direction})")

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ™‚ç³»åˆ—æ¨ç§»ã‚¿ãƒ–
    with ts_tabs[2]:
        st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æ™‚ç³»åˆ—æ¨ç§»ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚å€¤ãŒå°ã•ã„ã»ã©ä¸Šä½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ç¤ºã—ã¾ã™ã€‚", icon="â„¹ï¸")

        # ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        valid_ranking_data = {k: v for k, v in ranking_timeseries.items() if any(x is not None for x in v)}

        if not valid_ranking_data:
            st.warning("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚avg_rankãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            fig, ax = plt.subplots(figsize=(10, 6))
            for entity, values in valid_ranking_data.items():
                # Noneå€¤ã‚’é™¤å¤–ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
                valid_values = [(i, v) for i, v in enumerate(values) if v is not None]
                if valid_values:
                    x_indices, y_values = zip(*valid_values)
                    x_dates = [date_labels[i] for i in x_indices]
                    ax.plot(x_dates, y_values, marker="^", label=entity, linewidth=2, markersize=6)

            ax.set_xlabel("æ—¥ä»˜")
            ax.set_ylabel("å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆavg_rankï¼‰")
            ax.set_title(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æ™‚ç³»åˆ—æ¨ç§»ï¼ˆ{selected_category} - {selected_subcategory}ï¼‰")
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            plt.tight_layout()
            st.pyplot(fig, use_container_width=True)
            plt.close(fig)

            # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
            with st.expander("ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµ±è¨ˆæƒ…å ±", expanded=False):
                for entity, values in valid_ranking_data.items():
                    valid_values = [v for v in values if v is not None]
                    if valid_values:
                        avg_val = sum(valid_values) / len(valid_values)
                        min_val = min(valid_values)
                        max_val = max(valid_values)
                        st.write(f"**{entity}**: å¹³å‡ãƒ©ãƒ³ã‚¯={avg_val:.1f}, æœ€é«˜ä½={min_val:.0f}, æœ€ä½ä½={max_val:.0f}")

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦æ™‚ç³»åˆ—æ¨ç§»ã‚¿ãƒ–
    with ts_tabs[3]:
        st.info("Googleæ¤œç´¢ã¨Perplexityã®æ¤œç´¢çµæœã®é¡ä¼¼åº¦æŒ‡æ¨™ã®æ™‚ç³»åˆ—æ¨ç§»ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚\n\n"
               "ãƒ»RBOï¼šä¸Šä½ã®æ¤œç´¢çµæœãŒã©ã‚Œã ã‘ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©ä¸Šä½ã®çµæœãŒåŒã˜ï¼‰\n"
               "ãƒ»Kendall Tauï¼šé †ä½ã®ä¸¦ã³ãŒã©ã‚Œã ã‘ä¼¼ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©é †ä½ã®ä¸¦ã³ãŒåŒã˜ï¼‰\n"
               "ãƒ»Overlap Ratioï¼šå…¨ä½“ã®æ¤œç´¢çµæœãŒã©ã‚Œã ã‘é‡è¤‡ã—ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©åŒã˜URLãŒå¤šã„ï¼‰", icon="â„¹ï¸")

        # ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        valid_rbo = [v for v in rbo_timeseries if v is not None]
        valid_kendall = [v for v in kendall_tau_timeseries if v is not None]
        valid_overlap = [v for v in overlap_ratio_timeseries if v is not None]

        if not valid_rbo and not valid_kendall and not valid_overlap:
            st.warning("ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ranking_similarityãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            fig, ax = plt.subplots(figsize=(10, 6))

            # RBOã‚¹ã‚³ã‚¢
            if valid_rbo:
                valid_indices = [i for i, v in enumerate(rbo_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_rbo, marker="o", label="RBO", linewidth=2, markersize=6, color="blue")

            # Kendall Tau
            if valid_kendall:
                valid_indices = [i for i, v in enumerate(kendall_tau_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_kendall, marker="s", label="Kendall Tau", linewidth=2, markersize=6, color="orange")

            # Overlap Ratio
            if valid_overlap:
                valid_indices = [i for i, v in enumerate(overlap_ratio_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_overlap, marker="^", label="Overlap Ratio", linewidth=2, markersize=6, color="green")

            ax.set_xlabel("æ—¥ä»˜")
            ax.set_ylabel("é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢")
            ax.set_title(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ã®æ™‚ç³»åˆ—æ¨ç§»ï¼ˆ{selected_category} - {selected_subcategory}ï¼‰")
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, 1)
            plt.tight_layout()
            st.pyplot(fig, use_container_width=True)
            plt.close(fig)

            # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
            with st.expander("ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦çµ±è¨ˆæƒ…å ±", expanded=False):
                if valid_rbo:
                    avg_rbo = sum(valid_rbo) / len(valid_rbo)
                    min_rbo = min(valid_rbo)
                    max_rbo = max(valid_rbo)
                    st.write(f"**RBO**: å¹³å‡={avg_rbo:.3f}, æœ€å°={min_rbo:.3f}, æœ€å¤§={max_rbo:.3f}")

                if valid_kendall:
                    avg_kendall = sum(valid_kendall) / len(valid_kendall)
                    min_kendall = min(valid_kendall)
                    max_kendall = max(valid_kendall)
                    st.write(f"**Kendall Tau**: å¹³å‡={avg_kendall:.3f}, æœ€å°={min_kendall:.3f}, æœ€å¤§={max_kendall:.3f}")

                if valid_overlap:
                    avg_overlap = sum(valid_overlap) / len(valid_overlap)
                    min_overlap = min(valid_overlap)
                    max_overlap = max(valid_overlap)
                    st.write(f"**Overlap Ratio**: å¹³å‡={avg_overlap:.3f}, æœ€å°={min_overlap:.3f}, æœ€å¤§={max_overlap:.3f}")

    # å…¬å¼/éå…¬å¼æ¯”ç‡æ™‚ç³»åˆ—æ¨ç§»ã‚¿ãƒ–
    with ts_tabs[4]:
        st.info("Googleæ¤œç´¢ã¨Perplexityã®å…¬å¼ã‚µã‚¤ãƒˆæ¯”ç‡ã®æ™‚ç³»åˆ—æ¨ç§»ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚\n\n"
               "ãƒ»Googleå…¬å¼æ¯”ç‡ï¼šGoogleæ¤œç´¢çµæœã«ãŠã‘ã‚‹å…¬å¼ã‚µã‚¤ãƒˆã®å‰²åˆ\n"
               "ãƒ»Citationså…¬å¼æ¯”ç‡ï¼šPerplexityæ¤œç´¢çµæœã«ãŠã‘ã‚‹å…¬å¼ã‚µã‚¤ãƒˆã®å‰²åˆ\n"
               "ãƒ»ãƒã‚¤ã‚¢ã‚¹å·®åˆ†ï¼šGoogleæ¯”ç‡ - Citationsæ¯”ç‡ï¼ˆæ­£ã®å€¤ã¯GoogleãŒå…¬å¼ã‚µã‚¤ãƒˆã‚’å¤šãè¡¨ç¤ºï¼‰", icon="â„¹ï¸")

        # ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        valid_google_ratio = [v for v in google_official_ratio_timeseries if v is not None]
        valid_citations_ratio = [v for v in citations_official_ratio_timeseries if v is not None]
        valid_bias_delta = [v for v in official_bias_delta_timeseries if v is not None]

        if not valid_google_ratio and not valid_citations_ratio and not valid_bias_delta:
            st.warning("å…¬å¼/éå…¬å¼æ¯”ç‡ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚official_domain_analysisãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            fig, ax = plt.subplots(figsize=(10, 6))

            # Googleå…¬å¼æ¯”ç‡
            if valid_google_ratio:
                valid_indices = [i for i, v in enumerate(google_official_ratio_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_google_ratio, marker="o", label="Googleå…¬å¼æ¯”ç‡", linewidth=2, markersize=6, color="blue")

            # Citationså…¬å¼æ¯”ç‡
            if valid_citations_ratio:
                valid_indices = [i for i, v in enumerate(citations_official_ratio_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_citations_ratio, marker="s", label="Citationså…¬å¼æ¯”ç‡", linewidth=2, markersize=6, color="orange")

            # ãƒã‚¤ã‚¢ã‚¹å·®åˆ†
            if valid_bias_delta:
                valid_indices = [i for i, v in enumerate(official_bias_delta_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_bias_delta, marker="^", label="ãƒã‚¤ã‚¢ã‚¹å·®åˆ†", linewidth=2, markersize=6, color="red")

            ax.set_xlabel("æ—¥ä»˜")
            ax.set_ylabel("æ¯”ç‡ãƒ»å·®åˆ†")
            ax.set_title(f"å…¬å¼/éå…¬å¼æ¯”ç‡ã®æ™‚ç³»åˆ—æ¨ç§»ï¼ˆ{selected_category} - {selected_subcategory}ï¼‰")
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            ax.set_ylim(-0.5, 1.0)
            # 0.0ã®åŸºæº–ç·šã‚’è¿½åŠ 
            ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)
            plt.tight_layout()
            st.pyplot(fig, use_container_width=True)
            plt.close(fig)

            # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
            with st.expander("å…¬å¼/éå…¬å¼æ¯”ç‡çµ±è¨ˆæƒ…å ±", expanded=False):
                if valid_google_ratio:
                    avg_google = sum(valid_google_ratio) / len(valid_google_ratio)
                    min_google = min(valid_google_ratio)
                    max_google = max(valid_google_ratio)
                    st.write(f"**Googleå…¬å¼æ¯”ç‡**: å¹³å‡={avg_google:.3f}, æœ€å°={min_google:.3f}, æœ€å¤§={max_google:.3f}")

                if valid_citations_ratio:
                    avg_citations = sum(valid_citations_ratio) / len(valid_citations_ratio)
                    min_citations = min(valid_citations_ratio)
                    max_citations = max(valid_citations_ratio)
                    st.write(f"**Citationså…¬å¼æ¯”ç‡**: å¹³å‡={avg_citations:.3f}, æœ€å°={min_citations:.3f}, æœ€å¤§={max_citations:.3f}")

                if valid_bias_delta:
                    avg_bias = sum(valid_bias_delta) / len(valid_bias_delta)
                    min_bias = min(valid_bias_delta)
                    max_bias = max(valid_bias_delta)
                    bias_trend = "Googleå„ªä½" if avg_bias > 0 else "Citationså„ªä½" if avg_bias < 0 else "å‡è¡¡"
                    st.write(f"**ãƒã‚¤ã‚¢ã‚¹å·®åˆ†**: å¹³å‡={avg_bias:.3f}, æœ€å°={min_bias:.3f}, æœ€å¤§={max_bias:.3f} ({bias_trend})")

    # ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡æ™‚ç³»åˆ—æ¨ç§»ã‚¿ãƒ–
    with ts_tabs[5]:
        st.info("Googleæ¤œç´¢ã¨Perplexityã®æ¤œç´¢çµæœã®æ„Ÿæƒ…åˆ†ææ¯”ç‡ã®æ™‚ç³»åˆ—æ¨ç§»ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚\n\n"
               "ãƒ»Googleãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡ï¼šGoogleæ¤œç´¢çµæœã®ãƒã‚¸ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã®å‰²åˆ\n"
               "ãƒ»Citationsãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡ï¼šPerplexityæ¤œç´¢çµæœã®ãƒã‚¸ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã®å‰²åˆ\n"
               "ãƒ»ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¤ã‚¢ã‚¹å·®åˆ†ï¼šä¸¡è€…ã®ãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡ã®å·®ï¼ˆæ­£ã®å€¤ã¯Googleå„ªä½ï¼‰", icon="â„¹ï¸")

        # ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        valid_google_positive = [v for v in google_positive_ratio_timeseries if v is not None]
        valid_google_negative = [v for v in google_negative_ratio_timeseries if v is not None]
        valid_citations_positive = [v for v in citations_positive_ratio_timeseries if v is not None]
        valid_citations_negative = [v for v in citations_negative_ratio_timeseries if v is not None]
        valid_positive_bias_delta = [v for v in positive_bias_delta_timeseries if v is not None]

        if not valid_google_positive and not valid_citations_positive and not valid_positive_bias_delta:
            st.warning("ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚sentiment_comparisonãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            fig, ax = plt.subplots(figsize=(10, 6))

            # Googleãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡
            if valid_google_positive:
                valid_indices = [i for i, v in enumerate(google_positive_ratio_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_google_positive, marker="o", label="Googleãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡", linewidth=2, markersize=6, color="blue")

            # Citationsãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡
            if valid_citations_positive:
                valid_indices = [i for i, v in enumerate(citations_positive_ratio_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_citations_positive, marker="s", label="Citationsãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡", linewidth=2, markersize=6, color="orange")

            # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¤ã‚¢ã‚¹å·®åˆ†
            if valid_positive_bias_delta:
                valid_indices = [i for i, v in enumerate(positive_bias_delta_timeseries) if v is not None]
                valid_dates = [date_labels[i] for i in valid_indices]
                ax.plot(valid_dates, valid_positive_bias_delta, marker="^", label="ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¤ã‚¢ã‚¹å·®åˆ†", linewidth=2, markersize=6, color="red")

            ax.set_xlabel("æ—¥ä»˜")
            ax.set_ylabel("æ¯”ç‡ãƒ»å·®åˆ†")
            ax.set_title(f"ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡ã®æ™‚ç³»åˆ—æ¨ç§»ï¼ˆ{selected_category} - {selected_subcategory}ï¼‰")
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            ax.set_ylim(-1.0, 1.0)
            # 0.0ã®åŸºæº–ç·šã‚’è¿½åŠ 
            ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)
            plt.tight_layout()
            st.pyplot(fig, use_container_width=True)
            plt.close(fig)

            # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
            with st.expander("ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡çµ±è¨ˆæƒ…å ±", expanded=False):
                if valid_google_positive:
                    avg_google_pos = sum(valid_google_positive) / len(valid_google_positive)
                    min_google_pos = min(valid_google_positive)
                    max_google_pos = max(valid_google_positive)
                    st.write(f"**Googleãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡**: å¹³å‡={avg_google_pos:.3f}, æœ€å°={min_google_pos:.3f}, æœ€å¤§={max_google_pos:.3f}")

                if valid_citations_positive:
                    avg_citations_pos = sum(valid_citations_positive) / len(valid_citations_positive)
                    min_citations_pos = min(valid_citations_positive)
                    max_citations_pos = max(valid_citations_positive)
                    st.write(f"**Citationsãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡**: å¹³å‡={avg_citations_pos:.3f}, æœ€å°={min_citations_pos:.3f}, æœ€å¤§={max_citations_pos:.3f}")

                if valid_positive_bias_delta:
                    avg_positive_bias = sum(valid_positive_bias_delta) / len(valid_positive_bias_delta)
                    min_positive_bias = min(valid_positive_bias_delta)
                    max_positive_bias = max(valid_positive_bias_delta)
                    bias_trend = "Googleå„ªä½" if avg_positive_bias > 0 else "Citationså„ªä½" if avg_positive_bias < 0 else "å‡è¡¡"
                    st.write(f"**ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¤ã‚¢ã‚¹å·®åˆ†**: å¹³å‡={avg_positive_bias:.3f}, æœ€å°={min_positive_bias:.3f}, æœ€å¤§={max_positive_bias:.3f} ({bias_trend})")

elif viz_type == "å˜æ—¥åˆ†æ":
    # éåŒæœŸã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆèª­ã¿è¾¼ã¿çŠ¶æ³ã‚’è¡¨ç¤ºï¼‰
    dashboard_data = get_dashboard_data_async(loader, selected_date)
    analysis_data = dashboard_data["analysis_results"] if dashboard_data else None

    if not analysis_data:
        st.error(f"åˆ†æãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {selected_date}")
        st.stop()

    # å˜æ—¥åˆ†æã§ã‚‚èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’è¡¨ç¤ºï¼ˆç°¡ç´ åŒ–è¡¨ç¤ºï¼‰
    render_load_status(expanded=True, key_prefix="single_", simplified=True)

    # --- è©³ç´°å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—é¸æŠï¼ˆãŠã™ã™ã‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æçµæœã‚’çµ±åˆï¼‰ ---
    viz_type_options = ["æ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†æ", "ãŠã™ã™ã‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æçµæœ", "Perplexity-Googleæ¯”è¼ƒ", "çµ±åˆåˆ†æ"]
    viz_type_detail = st.sidebar.selectbox(
        "è©³ç´°å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
        viz_type_options,
        key="viz_type_selector",
        index=0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ€åˆã®é …ç›®ã‚’é¸æŠ
    )

    # --- ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆçµ±åˆç‰ˆï¼‰ ---
    st.markdown('<div class="main-dashboard-area">', unsafe_allow_html=True)

    # --- çµ±ä¸€ã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆä½œæˆ ---
    sentiment_data = analysis_data.get("sentiment_bias_analysis", {})
    all_categories = [c for c in sentiment_data.keys() if c not in ("å…¨ä½“", "all", "ALL", "All")]
    all_categories.sort()

    # çµ±ä¸€ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªé¸æŠï¼ˆå…¨åˆ†æã‚¿ã‚¤ãƒ—ã§å…±é€šï¼‰
    selected_category = st.sidebar.selectbox(
        "ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
        all_categories,
        key="category_selector",
        index=0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ€åˆã®é …ç›®ã‚’é¸æŠ
    )

    # --- çµ±ä¸€ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆä½œæˆ ---
    all_subcategories = list(sentiment_data[selected_category].keys())
    all_subcategories.sort()

    # çµ±ä¸€ã•ã‚ŒãŸã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªé¸æŠï¼ˆå…¨åˆ†æã‚¿ã‚¤ãƒ—ã§å…±é€šï¼‰
    selected_subcategory = st.sidebar.selectbox(
        "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
        all_subcategories,
        key="subcategory_selector",
        index=0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ€åˆã®é …ç›®ã‚’é¸æŠ
    )

    # --- è©³ç´°å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—åˆ†å² ---
    if viz_type_detail == "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†æ":
        # é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªãŒæ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†æã§åˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        if selected_category not in sentiment_data:
            st.warning(f"é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒª '{selected_category}' ã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†æã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            st.stop()

        entities_data = sentiment_data[selected_category][selected_subcategory].get("entities", {})
        entities = list(entities_data.keys())
        selected_entities = st.sidebar.multiselect(
            "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
            entities,
            key="entities_selector",
            default=entities  # å…¨ä»¶ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¡¨ç¤º
        )
        # --- è¡¨å½¢å¼è¡¨ç¤ºï¼ˆå¸¸ã«ä¸Šéƒ¨ã«è¡¨ç¤ºï¼‰ ---
        sentiment_flat = dashboard_data.get("perplexity_sentiment_flat", [])
        filtered = [row for row in sentiment_flat if row["ã‚«ãƒ†ã‚´ãƒª"] == selected_category and row["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª"] == selected_subcategory and (not selected_entities or row["ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£"] in selected_entities)]
        # è¡¨ç”¨ãƒ‡ãƒ¼ã‚¿æ•´å½¢
        table_rows = []
        for row in filtered:
            entity = row.get("ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£")
            unmasked_values = row.get("unmasked_values")
            masked_avg = None
            # masked_valuesã‚‚å¹³å‡å€¤ã‚’è¨ˆç®—
            if isinstance(row.get("masked_values"), list) and row["masked_values"]:
                # æ•´æ•°ã®ã¿ã§å¹³å‡å€¤ã‚’è¨ˆç®—
                masked_int_vals = [int(v) for v in row["masked_values"] if isinstance(v, (int, float))]
                if masked_int_vals:
                    masked_avg = sum(masked_int_vals) / len(masked_int_vals)
            # unmasked_values: æ•´æ•°ã®ã¿ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¡¨ç¤º
            score_list_str = ""
            score_avg = None
            score_std = None
            if isinstance(unmasked_values, list) and unmasked_values:
                # æ•´æ•°ã®ã¿ã§è¡¨ç¤º
                int_vals = [int(v) for v in unmasked_values if isinstance(v, (int, float))]
                score_list_str = ", ".join([str(v) for v in int_vals])
                if int_vals:
                    score_avg = sum(int_vals) / len(int_vals)
                    if len(int_vals) > 1:
                        mean = score_avg
                        score_std = (sum((x - mean) ** 2 for x in int_vals) / (len(int_vals) - 1)) ** 0.5
            diff = None
            # å·®åˆ†ã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ - æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼ˆãƒã‚¹ã‚¯ã‚ã‚Šï¼‰å¹³å‡
            if isinstance(score_avg, (int, float)) and isinstance(masked_avg, (int, float)):
                diff = score_avg - masked_avg
            table_rows.append({
                "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£": entity,
                "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒã‚¤ã‚¢ã‚¹": diff,
                "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡": score_avg,
                "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ï¼ˆãƒã‚¹ã‚¯ã‚ã‚Šï¼‰": masked_avg,
                "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ä¸€è¦§": score_list_str,
                "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢æ¨™æº–åå·®": score_std
            })
        st.subheader(f"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢è¡¨ï½œ{selected_category}ï½œ{selected_subcategory}")
        source_data = dashboard_data.get("source_data", {})
        perplexity_sentiment = source_data.get("perplexity_sentiment", {})
        subcat_data = perplexity_sentiment.get(selected_category, {}).get(selected_subcategory, {})
        masked_prompt = subcat_data.get("masked_prompt")
        if masked_prompt:
            with st.expander("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", expanded=False):
                st.markdown("**ãƒã‚¹ã‚¯ã‚ã‚Šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:**")
                st.markdown(masked_prompt)

                # ãƒã‚¹ã‚¯ãªã—ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚è¡¨ç¤º
                if selected_entities:
                    st.markdown("---")
                    st.markdown("**ãƒã‚¹ã‚¯ãªã—ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:**")
                    from src.prompts.sentiment_prompts import get_unmasked_prompt
                    # æœ€åˆã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿ã§ãƒã‚¹ã‚¯ãªã—ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º
                    first_entity = selected_entities[0]
                    unmasked_prompt = get_unmasked_prompt(selected_subcategory, first_entity)
                    # st.markdown(f"**{first_entity}ç”¨:**")
                    st.markdown(unmasked_prompt)
        if table_rows:
            df_sentiment = pd.DataFrame(table_rows)
            st.dataframe(df_sentiment)
        else:
            st.info("perplexity_sentimentå±æ€§ã‚’æŒã¤æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                # --- JSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ˜ã‚ŠãŸãŸã¿ã§è¡¨ç¤º ---
        source_data = dashboard_data.get("source_data", {})
        perplexity_sentiment = source_data.get("perplexity_sentiment", {})
        subcat_data = perplexity_sentiment.get(selected_category, {}).get(selected_subcategory, {})
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰", expanded=True):
            st.json(subcat_data, expanded=False)
        # --- ä¸»è¦æŒ‡æ¨™ã‚µãƒãƒªãƒ¼è¡¨ã®ç”Ÿæˆãƒ»è¡¨ç¤ºï¼ˆè¿½åŠ ï¼‰ ---
        summary_rows = []
        interpretation_dict = {}
        for entity in selected_entities:
            if entity in entities_data:
                entity_data = entities_data[entity]
                # çµ±è¨ˆçš„æœ‰æ„æ€§
                stat = entity_data.get("statistical_significance", {})
                p_value = stat.get("sign_test_p_value")
                significance_level = stat.get("significance_level")
                # åŠ¹æœé‡
                effect_size = entity_data.get("effect_size", {})
                cliffs_delta = effect_size.get("cliffs_delta")
                effect_magnitude = effect_size.get("effect_magnitude")
                # ä¿¡é ¼åŒºé–“
                ci = entity_data.get("confidence_interval", {})
                ci_lower = ci.get("ci_lower")
                ci_upper = ci.get("ci_upper")
                # å®‰å®šæ€§
                stability = entity_data.get("stability_metrics", {})
                stability_score = stability.get("stability_score")
                reliability = stability.get("reliability")
                # ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ»ãƒ©ãƒ³ã‚¯
                bias_index = entity_data.get("basic_metrics", {}).get("normalized_bias_index")
                bias_rank = entity_data.get("bias_rank")
                # interpretation
                interpretation = entity_data.get("interpretation", {})
                # ã‚µãƒãƒªãƒ¼è¡Œä½œæˆ
                summary_rows.append({
                    "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£": entity,
                    "çµ±è¨ˆçš„æœ‰æ„æ€§": significance_level or "-",
                    "på€¤": p_value if p_value is not None else "-",
                    "åŠ¹æœé‡": effect_magnitude or "-",
                    "ä¿¡é ¼åŒºé–“": f"{ci_lower}ï½{ci_upper}" if ci_lower is not None and ci_upper is not None else "-",
                    "å®‰å®šæ€§": reliability or "-",
                    "æ­£è¦åŒ–æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒã‚¤ã‚¢ã‚¹": bias_index if bias_index is not None else "-",
                    "ãƒ©ãƒ³ã‚¯": bias_rank if bias_rank is not None else "-",
                })
                # interpretationã¾ã¨ã‚
                interp_lines = []
                if isinstance(interpretation, dict):
                    for k, v in interpretation.items():
                        interp_lines.append(f"- **{k}**: {v}")
                elif isinstance(interpretation, str):
                    interp_lines.append(interpretation)
                interpretation_dict[entity] = "\n".join(interp_lines)
        if summary_rows:
            st.subheader("ä¸»è¦æŒ‡æ¨™ã‚µãƒãƒªãƒ¼è¡¨")
            df_summary = pd.DataFrame(summary_rows)
            st.dataframe(df_summary, use_container_width=True)
            for idx, row in df_summary.iterrows():
                entity = row["ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£"]
                interp = interpretation_dict.get(entity)
                if interp:
                    with st.expander(f"{entity} ã®è©³ç´°è§£é‡ˆ"):
                        st.markdown(interp)
        # --- ã‚°ãƒ©ãƒ•ç¨®åˆ¥ã‚¿ãƒ– ---
        tabs = st.tabs(["BIå€¤æ£’ã‚°ãƒ©ãƒ•", "é‡ç¯¤åº¦ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ", "på€¤ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", "åŠ¹æœé‡ vs på€¤æ•£å¸ƒå›³"])
        bias_indices = {}
        execution_counts = {}
        severity_dict = {}
        pvalue_dict = {}
        effect_data = {}
        for entity in selected_entities:
            if entity in entities_data:
                entity_data = entities_data[entity]
                # BIå€¤
                if "basic_metrics" in entity_data:
                    bias_indices[entity] = entity_data["basic_metrics"].get("normalized_bias_index", 0)
                    execution_counts[entity] = entity_data["basic_metrics"].get("execution_count", 0)
                # é‡ç¯¤åº¦
                if "severity_score" in entity_data:
                    # severity = entity_data.get("severity_score", {}).get("severity_score", 0)
                    severity = (entity_data.get("severity_score") or {}).get("severity_score", 0)
                    if isinstance(severity, dict):
                        score = severity.get("severity_score")
                    else:
                        score = severity
                    if score is not None:
                        severity_dict[entity] = score
                # på€¤
                stat = entity_data.get("statistical_significance", {})
                if "sign_test_p_value" in stat:
                    pvalue_dict[entity] = stat["sign_test_p_value"]
                # åŠ¹æœé‡
                effect_size = entity_data.get("effect_size", {})
                cliffs_delta = effect_size.get("cliffs_delta") if "cliffs_delta" in effect_size else None
                p_value = stat.get("sign_test_p_value") if "sign_test_p_value" in stat else None
                if cliffs_delta is not None and p_value is not None:
                    effect_data[entity] = {"cliffs_delta": cliffs_delta, "p_value": p_value}
        min_exec_count = min(execution_counts.values()) if execution_counts else 0
        reliability_label = get_reliability_label(min_exec_count)
        title = f"{selected_category} - {selected_subcategory}"
        with tabs[0]:
            st.info("å„ä¼æ¥­ã®ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤ºã—ã¾ã™ã€‚ã“ã®æŒ‡æ¨™ã¯ã€Œä¼æ¥­åã‚’çŸ¥ã£ãŸæ™‚ã®è©•ä¾¡ã€ã¨ã€Œä¼æ¥­åã‚’éš ã—ãŸæ™‚ã®è©•ä¾¡ã€ã®å·®ã‚’è¡¨ã—ã¾ã™ã€‚\n\nâ€¢ æ­£ã®å€¤ï¼ˆèµ¤ï¼‰ï¼šä¼æ¥­åã‚’çŸ¥ã‚‹ã¨è©•ä¾¡ãŒä¸ŠãŒã‚‹ï¼ˆå¥½æ„çš„ãªãƒã‚¤ã‚¢ã‚¹ï¼‰\nâ€¢ è² ã®å€¤ï¼ˆç·‘ï¼‰ï¼šä¼æ¥­åã‚’çŸ¥ã‚‹ã¨è©•ä¾¡ãŒä¸‹ãŒã‚‹ï¼ˆå¦å®šçš„ãªãƒã‚¤ã‚¢ã‚¹ï¼‰\nâ€¢ å€¤ã®çµ¶å¯¾å€¤ãŒå¤§ãã„ã»ã©ã€ä¼æ¥­åã«ã‚ˆã‚‹å½±éŸ¿ãŒå¼·ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™", icon="â„¹ï¸")
            if bias_indices:
                fig = plot_bias_indices_bar(bias_indices, title, reliability_label)
                st.pyplot(fig, use_container_width=True)
            else:
                st.info("BIå€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        with tabs[1]:
            st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒã‚¤ã‚¢ã‚¹é‡ç¯¤åº¦ï¼ˆå½±éŸ¿åº¦ï¼‰ã‚’ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã§å¯è¦–åŒ–ã—ã¾ã™ã€‚å€¤ãŒé«˜ã„ã»ã©å½±éŸ¿ãŒå¤§ãã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚", icon="â„¹ï¸")
            if severity_dict:
                fig = plot_severity_radar(severity_dict, output_path=None, title=title, reliability_label=reliability_label)
                st.pyplot(fig, use_container_width=False)
            else:
                st.info("é‡ç¯¤åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        with tabs[2]:
            st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆpå€¤ï¼‰ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§è¡¨ç¤ºã—ã¾ã™ã€‚è‰²ãŒæ¿ƒã„ã»ã©æœ‰æ„æ€§(çµæœãŒå¶ç„¶ã§ãªã„å¯èƒ½æ€§)ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚", icon="â„¹ï¸")
            if pvalue_dict:
                fig = plot_pvalue_heatmap(pvalue_dict, output_path=None, title=title, reliability_label=reliability_label)
                st.pyplot(fig, use_container_width=True)
            else:
                st.info("på€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        with tabs[3]:
            st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŠ¹æœé‡ï¼ˆCliff's Deltaï¼‰ã¨på€¤ã®é–¢ä¿‚ã‚’æ•£å¸ƒå›³ã§è¡¨ç¤ºã—ã¾ã™ã€‚", icon="â„¹ï¸")
            if effect_data:
                fig = plot_effect_significance_scatter(effect_data, title, reliability_label)
                st.pyplot(fig, use_container_width=True)
            else:
                st.info("åŠ¹æœé‡ãƒ»på€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    elif viz_type_detail == "Perplexity-Googleæ¯”è¼ƒ":
        # Perplexity-Googleæ¯”è¼ƒã®ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
        citations_data = analysis_data.get("citations_google_comparison", {})
        if citations_data:
            # é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªãŒPerplexity-Googleæ¯”è¼ƒã§åˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if selected_category not in citations_data:
                st.warning(f"é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒª '{selected_category}' ã¯Perplexity-Googleæ¯”è¼ƒã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                st.stop()

            if selected_category == "å…¨ä½“":
                # å…¨ä½“è¡¨ç¤ºã®å ´åˆã€ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã¯ã€Œå…¨ä½“ã€ã®ã¿
                selected_subcategory = "å…¨ä½“"
                # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
                all_similarity_data = {}
                for cat in citations_data.keys():
                    if cat != "error":
                        for subcat in citations_data[cat].keys():
                            subcat_data = citations_data[cat][subcat]
                            if "ranking_similarity" in subcat_data:
                                similarity = subcat_data["ranking_similarity"]
                                for metric in ['rbo_score', 'kendall_tau', 'overlap_ratio']:
                                    if metric not in all_similarity_data:
                                        all_similarity_data[metric] = []
                                    if similarity.get(metric) is not None:
                                        all_similarity_data[metric].append(similarity[metric])

                # å¹³å‡å€¤ã‚’è¨ˆç®—
                avg_similarity_data = {}
                for metric, values in all_similarity_data.items():
                    if values:
                        avg_similarity_data[metric] = sum(values) / len(values)
                    else:
                        avg_similarity_data[metric] = 0

                similarity_data = avg_similarity_data
            else:
                # ç‰¹å®šã‚«ãƒ†ã‚´ãƒªé¸æŠã®å ´åˆ
                subcat_data = citations_data[selected_category][selected_subcategory]
                similarity_data = subcat_data.get("ranking_similarity", {})

            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¸æŠæ©Ÿèƒ½ã‚’è¿½åŠ 
            # Googleæ¤œç´¢ã¨Perplexity Citationsã®ä¸¡æ–¹ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—
            google_entities = []
            citations_entities = []

            if selected_category != "å…¨ä½“":
                # Googleæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—
                source_data = dashboard_data.get("source_data", {})
                google_search_data = source_data.get("google_data", {})
                if selected_category in google_search_data and selected_subcategory in google_search_data[selected_category]:
                    google_subcat_data = google_search_data[selected_category][selected_subcategory]
                    if "entities" in google_subcat_data:
                        google_entities = list(google_subcat_data["entities"].keys())

                # Perplexity Citationsãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—
                perplexity_citations_data = source_data.get("perplexity_citations", {})
                if selected_category in perplexity_citations_data and selected_subcategory in perplexity_citations_data[selected_category]:
                    citations_subcat_data = perplexity_citations_data[selected_category][selected_subcategory]
                    if "entities" in citations_subcat_data:
                        citations_entities = list(citations_subcat_data["entities"].keys())

            # å…¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
            all_entities = list(set(google_entities + citations_entities))
            all_entities.sort()

            if all_entities:
                selected_entities = st.sidebar.multiselect(
                    "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
                    all_entities,
                    default=all_entities,  # å…¨ä»¶ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¡¨ç¤º
                    key="sentiment_entities"
                )
            else:
                selected_entities = []

            # Perplexity-Googleæ¯”è¼ƒã®è¡¨ç¤º
            st.subheader(f"ğŸ”— Perplexity-Googleæ¯”è¼ƒ - {selected_category} / {selected_subcategory}")

                        # === 1. Perplexity Citationsãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±è¡¨ç¤º ===
            source_data = dashboard_data.get("source_data", {})

            # Perplexity Citationsãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¡¨ç¤º
            perplexity_citations_data = source_data.get("perplexity_citations", {})
            if selected_category in perplexity_citations_data and selected_subcategory in perplexity_citations_data[selected_category]:
                citations_subcat_data = perplexity_citations_data[selected_category][selected_subcategory]
                with st.expander("ğŸ¤– Perplexity-Googleæ¤œç´¢ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±", expanded=False):
                    if "entities" in citations_subcat_data:
                        # æœ€åˆã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±ã‚’å–å¾—
                        first_entity_data = next(iter(citations_subcat_data["entities"].values()), {})
                        official_prompt = first_entity_data.get("official_prompt", "æœªè¨­å®š")
                        reputation_prompt = first_entity_data.get("reputation_prompt", "æœªè¨­å®š")

                        st.markdown("**ğŸ“ ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**:")
                        st.markdown(f"- **å…¬å¼æƒ…å ±**: `{official_prompt}`")
                        st.markdown(f"- **è©•åˆ¤æƒ…å ±**: `{reputation_prompt}`")

            # === 2. Googleæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º ===
            st.markdown("**Googleæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«**:**â€»APIã®ä»•æ§˜ã§å¼•ç”¨URLï¼ˆçµæœæ•°ï¼‰ã¯å„10ä»¶å›ºå®š**")
            if selected_category in google_search_data and selected_subcategory in google_search_data[selected_category]:
                google_subcat_data = google_search_data[selected_category][selected_subcategory]
                if "entities" in google_subcat_data:
                    google_entities_data = google_subcat_data["entities"]

                    # é¸æŠã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿ã‚’è¡¨ç¤º
                    filtered_google_entities = {k: v for k, v in google_entities_data.items()
                                             if not selected_entities or k in selected_entities}

                    if filtered_google_entities:
                        google_table_rows = []
                        for entity_name, entity_data in filtered_google_entities.items():
                            # æ„Ÿæƒ…åˆ†æçµæœã®çµ±è¨ˆ
                            positive_count = 0
                            negative_count = 0
                            neutral_count = 0
                            unknown_count = 0

                            for result in entity_data.get("reputation_results", []):
                                sentiment = result.get("sentiment", "unknown")
                                if sentiment == "positive":
                                    positive_count += 1
                                elif sentiment == "negative":
                                    negative_count += 1
                                elif sentiment == "neutral":
                                    neutral_count += 1
                                else:
                                    unknown_count += 1

                            # å…¬å¼/éå…¬å¼ã®çµ±è¨ˆï¼ˆå…¬å¼çµæœã®ã¿ï¼‰
                            official_official_count = 0
                            official_unofficial_count = 0

                            # official_resultsã®å…¬å¼/éå…¬å¼ã‚«ã‚¦ãƒ³ãƒˆ
                            for result in entity_data.get("official_results", []):
                                if result.get("is_official") == "official":
                                    official_official_count += 1
                                else:
                                    official_unofficial_count += 1

                            # ä¸»è¦ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆä¸Šä½3ã¤ï¼‰
                            all_domains = []
                            for result in entity_data.get("official_results", []) + entity_data.get("reputation_results", []):
                                domain = result.get("domain")
                                if domain:
                                    all_domains.append(domain)

                            top_domains = list(set(all_domains))[:3]
                            top_domains_str = ", ".join(top_domains) if top_domains else "ãªã—"

                            google_table_rows.append({
                                "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£": entity_name,
                                "å…¬å¼": official_official_count,
                                "éå…¬å¼": official_unofficial_count,
                                "ãƒã‚¸ãƒ†ã‚£ãƒ–": positive_count,
                                "ãƒã‚¬ãƒ†ã‚£ãƒ–": negative_count,
                                "ä¸­ç«‹": neutral_count,
                                "æ„Ÿæƒ…ä¸æ˜": unknown_count,
                                "ä¸»è¦ãƒ‰ãƒ¡ã‚¤ãƒ³": top_domains_str
                            })

                        if google_table_rows:
                            df_google = pd.DataFrame(google_table_rows)
                            st.dataframe(df_google, use_container_width=True)
                        else:
                            st.info("è¡¨ç¤ºå¯èƒ½ãªGoogleæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    else:
                        st.info("Googleæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    st.info("Googleæ¤œç´¢ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                st.info("Googleæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # === 3. Perplexity Citationsãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º ===
            st.markdown("**Perplexity Citationsãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«**: **â€»APIã®ä»•æ§˜ã§å¼•ç”¨URLï¼ˆçµæœæ•°ï¼‰ã¯å„5ä»¶å›ºå®š**")
            if selected_category in perplexity_citations_data and selected_subcategory in perplexity_citations_data[selected_category]:
                citations_subcat_data = perplexity_citations_data[selected_category][selected_subcategory]
                if "entities" in citations_subcat_data:
                    citations_entities_data = citations_subcat_data["entities"]

                    # é¸æŠã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿ã‚’è¡¨ç¤º
                    filtered_citations_entities = {k: v for k, v in citations_entities_data.items()
                                                 if not selected_entities or k in selected_entities}

                    if filtered_citations_entities:
                        citations_table_rows = []
                        for entity_name, entity_data in filtered_citations_entities.items():
                            # æ„Ÿæƒ…åˆ†æçµæœã®çµ±è¨ˆ
                            positive_count = 0
                            negative_count = 0
                            neutral_count = 0
                            unknown_count = 0

                            for result in entity_data.get("reputation_results", []):
                                sentiment = result.get("sentiment", "unknown")
                                if sentiment == "positive":
                                    positive_count += 1
                                elif sentiment == "negative":
                                    negative_count += 1
                                elif sentiment == "neutral":
                                    neutral_count += 1
                                else:
                                    unknown_count += 1

                            # å…¬å¼/éå…¬å¼ã®çµ±è¨ˆï¼ˆå…¬å¼çµæœã®ã¿ï¼‰
                            official_official_count = 0
                            official_unofficial_count = 0

                            # official_resultsã®å…¬å¼/éå…¬å¼ã‚«ã‚¦ãƒ³ãƒˆ
                            for result in entity_data.get("official_results", []):
                                if result.get("is_official") == "official":
                                    official_official_count += 1
                                else:
                                    official_unofficial_count += 1

                            # ä¸»è¦ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆä¸Šä½3ã¤ï¼‰
                            all_domains = []
                            for result in entity_data.get("official_results", []) + entity_data.get("reputation_results", []):
                                domain = result.get("domain")
                                if domain:
                                    all_domains.append(domain)

                            top_domains = list(set(all_domains))[:3]
                            top_domains_str = ", ".join(top_domains) if top_domains else "ãªã—"

                            citations_table_rows.append({
                                "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£": entity_name,
                                "å…¬å¼": official_official_count,
                                "éå…¬å¼": official_unofficial_count,
                                "ãƒã‚¸ãƒ†ã‚£ãƒ–": positive_count,
                                "ãƒã‚¬ãƒ†ã‚£ãƒ–": negative_count,
                                "ä¸­ç«‹": neutral_count,
                                "æ„Ÿæƒ…ä¸æ˜": unknown_count,
                                "ä¸»è¦ãƒ‰ãƒ¡ã‚¤ãƒ³": top_domains_str
                            })

                        if citations_table_rows:
                            df_citations = pd.DataFrame(citations_table_rows)
                            st.dataframe(df_citations, use_container_width=True)
                        else:
                            st.info("è¡¨ç¤ºå¯èƒ½ãªPerplexity Citationsãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    else:
                        st.info("Perplexity Citationsãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    st.info("Perplexity Citationsã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                st.info("Perplexity Citationsãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                        # === 3.5. è©³ç´°ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º ===
            with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰", expanded=True):
                col1, col2 = st.columns(2)

                with col1:
                    st.markdown("**Googleæ¤œç´¢è©³ç´°ãƒ‡ãƒ¼ã‚¿**")
                    if selected_category in google_search_data and selected_subcategory in google_search_data[selected_category]:
                        google_subcat_data = google_search_data[selected_category][selected_subcategory]
                        if "entities" in google_subcat_data:
                            st.json(google_subcat_data["entities"], expanded=False)
                        else:
                            st.info("Googleæ¤œç´¢ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    else:
                        st.info("Googleæ¤œç´¢è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                with col2:
                    st.markdown("**Perplexity Citationsè©³ç´°ãƒ‡ãƒ¼ã‚¿**")
                    if selected_category in perplexity_citations_data and selected_subcategory in perplexity_citations_data[selected_category]:
                        citations_subcat_data = perplexity_citations_data[selected_category][selected_subcategory]
                        if "entities" in citations_subcat_data:
                            st.json(citations_subcat_data["entities"], expanded=False)
                        else:
                            st.info("Perplexity Citationsã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    else:
                        st.info("Perplexity Citationsè©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # === 4. æ¯”è¼ƒåˆ†æçµæœè¡¨ç¤ºï¼ˆã‚¿ãƒ–å½¢å¼ï¼‰ ===
            if similarity_data:
                title = f"{selected_category} - {selected_subcategory}"

                # ã‚¿ãƒ–ä½œæˆ
                tab1, tab2, tab3, tab4 = st.tabs([
                    "ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦åˆ†æï¼ˆå…¬å¼çµæœï¼‰",
                    "ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦åˆ†æï¼ˆè©•åˆ¤çµæœï¼‰",
                    "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒ",
                    "æ„Ÿæƒ…åˆ†ææ¯”è¼ƒ"
                ])

                with tab1:
                    st.markdown("**ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦åˆ†æï¼ˆå…¬å¼çµæœï¼‰**")
                    st.info("Googleæ¤œç´¢ã¨Perplexityã®å…¬å¼æ¤œç´¢çµæœã®é¡ä¼¼åº¦ã‚’3ã¤ã®æŒ‡æ¨™ã§æ¯”è¼ƒã—ã¾ã™ï¼š\n\n"
                           "ãƒ»RBOï¼šä¸Šä½ã®æ¤œç´¢çµæœãŒã©ã‚Œã ã‘ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©ä¸Šä½ã®çµæœãŒåŒã˜ï¼‰\n"
                           "ãƒ»Kendall Tauï¼šé †ä½ã®ä¸¦ã³ãŒã©ã‚Œã ã‘ä¼¼ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©é †ä½ã®ä¸¦ã³ãŒåŒã˜ï¼‰\n"
                           "ãƒ»Overlap Ratioï¼šå…¨ä½“ã®æ¤œç´¢çµæœãŒã©ã‚Œã ã‘é‡è¤‡ã—ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©åŒã˜URLãŒå¤šã„ï¼‰", icon="â„¹ï¸")

                    # å…¬å¼çµæœã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
                    official_metrics = similarity_data.get("official_results_metrics", {})
                    if official_metrics and "error" not in official_metrics:
                        fig = plot_ranking_similarity(official_metrics, f"{title} - å…¬å¼çµæœ")
                        st.pyplot(fig, use_container_width=True)

                        # å…¬å¼çµæœã®è§£èª¬ã‚’è¡¨ç¤º
                        _display_ranking_interpretation(official_metrics, "å…¬å¼çµæœ")
                    else:
                        st.warning("å…¬å¼çµæœã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")



                with tab2:
                    st.markdown("**ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦åˆ†æï¼ˆè©•åˆ¤çµæœï¼‰**")
                    st.info("Googleæ¤œç´¢ã¨Perplexityã®è©•åˆ¤æ¤œç´¢çµæœã®é¡ä¼¼åº¦ã‚’3ã¤ã®æŒ‡æ¨™ã§æ¯”è¼ƒã—ã¾ã™ï¼š\n\n"
                           "ãƒ»RBOï¼šä¸Šä½ã®æ¤œç´¢çµæœãŒã©ã‚Œã ã‘ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©ä¸Šä½ã®çµæœãŒåŒã˜ï¼‰\n"
                           "ãƒ»Kendall Tauï¼šé †ä½ã®ä¸¦ã³ãŒã©ã‚Œã ã‘ä¼¼ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©é †ä½ã®ä¸¦ã³ãŒåŒã˜ï¼‰\n"
                           "ãƒ»Overlap Ratioï¼šå…¨ä½“ã®æ¤œç´¢çµæœãŒã©ã‚Œã ã‘é‡è¤‡ã—ã¦ã„ã‚‹ã‹ï¼ˆ1ã«è¿‘ã„ã»ã©åŒã˜URLãŒå¤šã„ï¼‰", icon="â„¹ï¸")

                    # è©•åˆ¤çµæœã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
                    reputation_metrics = similarity_data.get("reputation_results_metrics", {})
                    if reputation_metrics and "error" not in reputation_metrics:
                        fig = plot_ranking_similarity(reputation_metrics, f"{title} - è©•åˆ¤çµæœ")
                        st.pyplot(fig, use_container_width=True)

                        # è©•åˆ¤çµæœã®è§£èª¬ã‚’è¡¨ç¤º
                        _display_ranking_interpretation(reputation_metrics, "è©•åˆ¤çµæœ")
                    else:
                        st.warning("è©•åˆ¤çµæœã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")

                with tab4:
                    st.markdown("**å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒ**")
                    st.info("Googleæ¤œç´¢ã¨Perplexityã®æ¤œç´¢çµæœã«ãŠã‘ã‚‹å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã®éœ²å‡ºæ¯”ç‡ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚\n\n"
                           "ãƒ»Googleå…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡ï¼šGoogleæ¤œç´¢çµæœä¸­ã®å…¬å¼ã‚µã‚¤ãƒˆæ¯”ç‡\n"
                           "ãƒ»Perplexityå…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ç‡ï¼šPerplexityå¼•ç”¨ä¸­ã®å…¬å¼ã‚µã‚¤ãƒˆæ¯”ç‡\n"
                           "ãƒ»ãƒã‚¤ã‚¢ã‚¹ãƒ‡ãƒ«ã‚¿ï¼šä¸¡è€…ã®å·®åˆ†ï¼ˆæ­£ã®å€¤ã¯GoogleãŒå…¬å¼ã‚µã‚¤ãƒˆã‚’å¤šãè¡¨ç¤ºï¼‰", icon="â„¹ï¸")

                    # å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨è¡¨ç¤º
                    if selected_category in citations_data and selected_subcategory in citations_data[selected_category]:
                        subcat_comparison_data = citations_data[selected_category][selected_subcategory]
                        official_data = subcat_comparison_data.get("official_domain_analysis", {})

                        if official_data:
                            fig = plot_official_domain_comparison(official_data, title)
                            st.pyplot(fig, use_container_width=True)

                            # ã‚°ãƒ©ãƒ•è§£èª¬
                            st.markdown("**ğŸ“Š ã‚°ãƒ©ãƒ•è§£èª¬**")

                            google_ratio = official_data.get("google_official_ratio", 0)
                            citations_ratio = official_data.get("citations_official_ratio", 0)
                            bias_delta = official_data.get("official_bias_delta", 0)

                            # è§£é‡ˆã®ç”Ÿæˆ
                            if bias_delta > 0.1:
                                interpretation = f"Googleæ¤œç´¢ï¼ˆ{google_ratio:.1%}ï¼‰ãŒPerplexityï¼ˆ{citations_ratio:.1%}ï¼‰ã‚ˆã‚Šå…¬å¼ã‚µã‚¤ãƒˆã‚’å¤šãè¡¨ç¤ºã—ã¦ãŠã‚Šã€Googleå´ã«å…¬å¼ã‚µã‚¤ãƒˆéœ²å‡ºã®ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚Šã¾ã™"
                            elif bias_delta < -0.1:
                                interpretation = f"Perplexityï¼ˆ{citations_ratio:.1%}ï¼‰ãŒGoogleæ¤œç´¢ï¼ˆ{google_ratio:.1%}ï¼‰ã‚ˆã‚Šå…¬å¼ã‚µã‚¤ãƒˆã‚’å¤šãè¡¨ç¤ºã—ã¦ãŠã‚Šã€Perplexityå´ã«å…¬å¼ã‚µã‚¤ãƒˆéœ²å‡ºã®ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚Šã¾ã™"
                            else:
                                interpretation = f"Googleæ¤œç´¢ï¼ˆ{google_ratio:.1%}ï¼‰ã¨Perplexityï¼ˆ{citations_ratio:.1%}ï¼‰ã®å…¬å¼ã‚µã‚¤ãƒˆè¡¨ç¤ºã¯å‡è¡¡ã—ã¦ãŠã‚Šã€å¤§ããªãƒã‚¤ã‚¢ã‚¹ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“"

                            st.markdown(f"**è§£é‡ˆ**: {interpretation}")

                            # ãƒã‚¤ã‚¢ã‚¹ãƒ‡ãƒ«ã‚¿ã®è©³ç´°èª¬æ˜
                            if abs(bias_delta) > 0.2:
                                st.markdown(f"**ãƒã‚¤ã‚¢ã‚¹å¼·åº¦**: å¼·ã„ï¼ˆå·®åˆ†: {bias_delta:.1%}ï¼‰")
                            elif abs(bias_delta) > 0.1:
                                st.markdown(f"**ãƒã‚¤ã‚¢ã‚¹å¼·åº¦**: ä¸­ç¨‹åº¦ï¼ˆå·®åˆ†: {bias_delta:.1%}ï¼‰")
                            else:
                                st.markdown(f"**ãƒã‚¤ã‚¢ã‚¹å¼·åº¦**: å¼±ã„ï¼ˆå·®åˆ†: {bias_delta:.1%}ï¼‰")
                        else:
                            st.info("å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    else:
                        st.info("å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                with tab3:
                    st.markdown("**æ„Ÿæƒ…åˆ†ææ¯”è¼ƒ**")
                    st.info("Googleæ¤œç´¢ã¨Perplexityã®æ¤œç´¢çµæœã«ãŠã‘ã‚‹æ„Ÿæƒ…åˆ†æçµæœã®åˆ†å¸ƒã‚’æ¯”è¼ƒã—ã¾ã™ã€‚\n\n"

                           "ãƒ»ä¸æ˜ï¼šæ„Ÿæƒ…åˆ†æãŒã§ããªã„çµæœã®æ¯”ç‡", icon="â„¹ï¸")

                    # æ„Ÿæƒ…åˆ†ææ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨è¡¨ç¤º
                    if selected_category in citations_data and selected_subcategory in citations_data[selected_category]:
                        subcat_comparison_data = citations_data[selected_category][selected_subcategory]
                        sentiment_data = subcat_comparison_data.get("sentiment_comparison", {})

                        if sentiment_data:
                            fig = plot_sentiment_comparison(sentiment_data, title)
                            st.pyplot(fig, use_container_width=True)

                            # ã‚°ãƒ©ãƒ•è§£èª¬
                            st.markdown("**ğŸ“Š ã‚°ãƒ©ãƒ•è§£èª¬**")

                            google_dist = sentiment_data.get("google_sentiment_distribution", {})
                            citations_dist = sentiment_data.get("citations_sentiment_distribution", {})
                            sentiment_correlation = sentiment_data.get("sentiment_correlation", 0)

                            # ä¸»è¦æ„Ÿæƒ…ã®æ¯”è¼ƒ
                            google_positive = google_dist.get("positive", 0)
                            google_negative = google_dist.get("negative", 0)
                            citations_positive = citations_dist.get("positive", 0)
                            citations_negative = citations_dist.get("negative", 0)

                            # æ„Ÿæƒ…å‚¾å‘ã®è§£é‡ˆ
                            if sentiment_correlation > 0.7:
                                correlation_interpretation = "éå¸¸ã«é¡ä¼¼"
                            elif sentiment_correlation > 0.5:
                                correlation_interpretation = "é¡ä¼¼"
                            elif sentiment_correlation > 0.3:
                                correlation_interpretation = "ã‚„ã‚„é¡ä¼¼"
                            else:
                                correlation_interpretation = "ç•°ãªã‚‹"

                            # ãƒã‚¸ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã®æ¯”è¼ƒ
                            positive_diff = google_positive - citations_positive
                            if abs(positive_diff) > 0.1:
                                if positive_diff > 0:
                                    positive_interpretation = f"Googleæ¤œç´¢ï¼ˆ{google_positive:.1%}ï¼‰ãŒPerplexityï¼ˆ{citations_positive:.1%}ï¼‰ã‚ˆã‚Šãƒã‚¸ãƒ†ã‚£ãƒ–ãªçµæœã‚’å¤šãè¡¨ç¤º"
                                else:
                                    positive_interpretation = f"Perplexityï¼ˆ{citations_positive:.1%}ï¼‰ãŒGoogleæ¤œç´¢ï¼ˆ{google_positive:.1%}ï¼‰ã‚ˆã‚Šãƒã‚¸ãƒ†ã‚£ãƒ–ãªçµæœã‚’å¤šãè¡¨ç¤º"
                            else:
                                positive_interpretation = f"ä¸¡è€…ã®ãƒã‚¸ãƒ†ã‚£ãƒ–æ„Ÿæƒ…æ¯”ç‡ã¯å‡è¡¡ï¼ˆGoogle: {google_positive:.1%}, Perplexity: {citations_positive:.1%}ï¼‰"

                            st.markdown(f"**æ„Ÿæƒ…ç›¸é–¢**: {sentiment_correlation:.3f}ï¼ˆ{correlation_interpretation}ï¼‰")
                            st.markdown(f"**ãƒã‚¸ãƒ†ã‚£ãƒ–æ„Ÿæƒ…æ¯”è¼ƒ**: {positive_interpretation}")

                            # ãƒã‚¬ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã®æ¯”è¼ƒ
                            negative_diff = google_negative - citations_negative
                            if abs(negative_diff) > 0.1:
                                if negative_diff > 0:
                                    negative_interpretation = f"Googleæ¤œç´¢ï¼ˆ{google_negative:.1%}ï¼‰ãŒPerplexityï¼ˆ{citations_negative:.1%}ï¼‰ã‚ˆã‚Šãƒã‚¬ãƒ†ã‚£ãƒ–ãªçµæœã‚’å¤šãè¡¨ç¤º"
                                else:
                                    negative_interpretation = f"Perplexityï¼ˆ{citations_negative:.1%}ï¼‰ãŒGoogleæ¤œç´¢ï¼ˆ{google_negative:.1%}ï¼‰ã‚ˆã‚Šãƒã‚¬ãƒ†ã‚£ãƒ–ãªçµæœã‚’å¤šãè¡¨ç¤º"
                            else:
                                negative_interpretation = f"ä¸¡è€…ã®ãƒã‚¬ãƒ†ã‚£ãƒ–æ„Ÿæƒ…æ¯”ç‡ã¯å‡è¡¡ï¼ˆGoogle: {google_negative:.1%}, Perplexity: {citations_negative:.1%}ï¼‰"

                            st.markdown(f"**ãƒã‚¬ãƒ†ã‚£ãƒ–æ„Ÿæƒ…æ¯”è¼ƒ**: {negative_interpretation}")
                        else:
                            st.info("æ„Ÿæƒ…åˆ†ææ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    else:
                        st.info("æ„Ÿæƒ…åˆ†ææ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                # è©³ç´°åˆ†æãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºï¼ˆå…¨ã‚¿ãƒ–å…±é€šï¼‰
                with st.expander("è©³ç´°åˆ†æãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰", expanded=True):
                    if selected_category in citations_data and selected_subcategory in citations_data[selected_category]:
                        subcat_comparison_data = citations_data[selected_category][selected_subcategory]
                        st.json(subcat_comparison_data, expanded=False)
                    else:
                        st.info("è©³ç´°åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                st.info("ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            st.warning("Perplexity-Googleæ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    elif viz_type_detail == "çµ±åˆåˆ†æ":
        # çµ±åˆåˆ†æã®è¡¨ç¤º
        st.subheader("ğŸ“Š çµ±åˆåˆ†æçµæœ")
        cross_data = analysis_data.get("cross_analysis_insights", {})

        # æ–°ã‚¿ãƒ–æ§‹æˆ
        main_tabs = st.tabs(["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªåˆ†æ", "å…¨ä½“çµ±åˆåˆ†æ", "å¸‚å ´åˆ†æ"])

        # é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢ã®èª¬æ˜æ–‡ã‚’å¤‰æ•°ã«ã¾ã¨ã‚ã¦ä½¿ã„å›ã™
        severity_info_text = """
**é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—å¼**
`severity = abs_bi Ã— |cliffs_delta| Ã— (1 - på€¤) Ã— å®‰å®šæ€§ã‚¹ã‚³ã‚¢`ï¼ˆæœ€å¤§10ã§ä¸¸ã‚ã‚‹ï¼‰
- **abs_bi**: ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ï¼ˆnormalized_bias_indexç­‰ï¼‰
- **cliffs_delta**: åŠ¹æœé‡ï¼ˆCliff's deltaç­‰ï¼‰
- **på€¤**: çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆå°ã•ã„ã»ã©é‡ã¿å¤§ï¼‰
- **å®‰å®šæ€§ã‚¹ã‚³ã‚¢**: ã°ã‚‰ã¤ããŒå°ã•ã„ã»ã©é‡ã¿å¤§

ã“ã‚Œã«ã‚ˆã‚Šã€Œå¤§ãããƒ»æœ‰æ„ã§ãƒ»å®‰å®šã—ãŸãƒã‚¤ã‚¢ã‚¹ã®ã¿ãŒé«˜ã‚¹ã‚³ã‚¢ã€ã¨ãªã‚Šã¾ã™ã€‚
"""

        # --- ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªåˆ†æã‚¿ãƒ– ---
        with main_tabs[0]:
            st.markdown(f"### ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªåˆ†æ: {selected_category} / {selected_subcategory}")

            # é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—å¼ã‚’expanderã§è¡¨ç¤º
            with st.expander("é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—å¼", expanded=False):
                st.markdown(severity_info_text)
            # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå˜ä½ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            sentiment_data = analysis_data.get("sentiment_bias_analysis", {})
            subcat_data = sentiment_data.get(selected_category, {}).get(selected_subcategory, {})
            entities = subcat_data.get("entities", {})
            # Noneã‚„dictä»¥å¤–ã‚’é™¤å¤–
            entities = {k: v for k, v in entities.items() if isinstance(v, dict)}

            # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªé‡ç¯¤åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            st.markdown("#### é‡ç¯¤åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå†…ï¼‰")
            if not entities:
                st.info("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå†…ã®é‡ç¯¤åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                ranking_rows = []
                for entity, entity_data in entities.items():
                    if not isinstance(entity_data, dict):
                        continue  # dictå‹ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
                    # severity = entity_data.get("severity_score", {}).get("severity_score", 0)
                    severity = (entity_data.get("severity_score") or {}).get("severity_score", 0)
                    ranking_rows.append({
                        "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£": entity,
                        "é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢": f"{severity:.3f}",
                        "é‡ç¯¤åº¦ãƒ¬ãƒ™ãƒ«": "é‡ç¯¤" if severity > 3.0 else "è»½å¾®"
                    })
                df_severity = pd.DataFrame(ranking_rows).sort_values(by="é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢", ascending=False)
                st.dataframe(df_severity, use_container_width=True, hide_index=True)

            # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªæ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢
            st.markdown("#### æ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢ï¼ˆã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå†…ï¼‰")

            # è¨ˆç®—æ–¹æ³•ã®èª¬æ˜ã‚’è¿½åŠ 
            with st.expander("ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—æ–¹æ³•", expanded=False):
                st.markdown("""
                **è¨ˆç®—å¯¾è±¡**: æ„Ÿæƒ…åˆ†æã®normalized_bias_index vs ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æã®avg_rank

                **è¨ˆç®—æ‰‹é †**:
                1. å…±é€šã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æŠ½å‡ºï¼ˆæ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æã®ä¸¡æ–¹ã«å­˜åœ¨ã™ã‚‹ä¼æ¥­ï¼‰
                2. ãƒ©ãƒ³ã‚­ãƒ³ã‚°å€¤ã®é€†è»¢å‡¦ç†ï¼ˆæ•°å€¤ãŒå¤§ãã„ã»ã©ä¸Šä½ã«ãªã‚‹ã‚ˆã†ã«ï¼‰
                   - å…ƒã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°: 1ä½=1, 2ä½=2, 3ä½=3...
                   - é€†è»¢å¾Œ: 1ä½=5, 2ä½=4, 3ä½=3...ï¼ˆ5ç¤¾ã®å ´åˆï¼‰
                3. Pearsonç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—
                4. Spearmanç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—

                **è§£é‡ˆ**:
                - æ­£ã®ç›¸é–¢: æ„Ÿæƒ…åˆ†æã§å¥½æ„çš„ãªä¼æ¥­ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã‚‚ä¸Šä½
                - è² ã®ç›¸é–¢: æ„Ÿæƒ…åˆ†æã§å¥½æ„çš„ãªä¼æ¥­ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã¯ä¸‹ä½
                - ç›¸é–¢ãªã—: æ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒç‹¬ç«‹ã—ãŸè©•ä¾¡
                """)

            cross_insights = analysis_data.get("cross_analysis_insights", {})
            sentiment_corr = cross_insights.get("sentiment_ranking_correlation", {})
            subcat_corr = sentiment_corr.get(selected_category, {}).get(selected_subcategory, {})
            if subcat_corr:
                corr = subcat_corr.get('correlation', 0)
                pval = subcat_corr.get('p_value', None)
                spearman = subcat_corr.get('spearman', None)
                n_entities = subcat_corr.get('n_entities', None)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Pearsonç›¸é–¢ä¿‚æ•°", f"{corr:.3f}")
                with col2:
                    if spearman is not None:
                        st.metric("Spearmanç›¸é–¢ä¿‚æ•°", f"{spearman:.3f}")
                with col3:
                    if n_entities is not None:
                        st.metric("åˆ†æå¯¾è±¡ä¼æ¥­æ•°", f"{n_entities}")

                if pval is not None:
                    st.markdown(f"**på€¤**: {pval:.3f}")
                    if pval < 0.05:
                        sig_text = "çµ±è¨ˆçš„ã«æœ‰æ„ (p < 0.05)"
                    else:
                        sig_text = "çµ±è¨ˆçš„ã«æœ‰æ„ã§ãªã„ (p >= 0.05)"
                else:
                    sig_text = "på€¤ãƒ‡ãƒ¼ã‚¿ãªã—"

                if corr > 0.3:
                    corr_text = "æ­£ã®ç›¸é–¢ï¼šæ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§åŒã˜ä¼æ¥­ãŒå„ªé‡ã•ã‚Œã‚‹å‚¾å‘"
                elif corr < -0.3:
                    corr_text = "è² ã®ç›¸é–¢ï¼šæ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ç•°ãªã‚‹è©•ä¾¡åŸºæº–ãŒåƒãå‚¾å‘"
                else:
                    corr_text = "ç›¸é–¢ãªã—ï¼šæ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒç‹¬ç«‹ã—ãŸè©•ä¾¡"
                st.markdown(f"**è§£é‡ˆ**: {corr_text} / {sig_text}")
            else:
                st.info("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå†…ã®ç›¸é–¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒã‚¤ã‚¢ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
            st.markdown("#### ãƒã‚¤ã‚¢ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»è§£èª¬ï¼ˆã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå†…ï¼‰")
            cat_analysis = subcat_data.get("category_level_analysis", {})
            bias_dist = cat_analysis.get("bias_distribution", {})
            if bias_dist:
                st.markdown(f"- æ­£ã®ãƒã‚¤ã‚¢ã‚¹æ•°: {bias_dist.get('positive_bias_count', 'N/A')}")
                st.markdown(f"- è² ã®ãƒã‚¤ã‚¢ã‚¹æ•°: {bias_dist.get('negative_bias_count', 'N/A')}")
                st.markdown(f"- ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ•°: {bias_dist.get('neutral_count', 'N/A')}")
                st.markdown(f"- ãƒã‚¤ã‚¢ã‚¹ç¯„å›²: {bias_dist.get('bias_range', 'N/A')}")
            else:
                st.info("ãƒã‚¤ã‚¢ã‚¹åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            # è§£é‡ˆï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
            if 'interpretation' in cat_analysis:
                st.markdown(f"è§£é‡ˆ: {cat_analysis['interpretation']}")
            else:
                st.info("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå˜ä½ã®è§£é‡ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

        # --- å…¨ä½“çµ±åˆåˆ†æã‚¿ãƒ– ---
        with main_tabs[1]:
            st.markdown("### å…¨ä½“çµ±åˆåˆ†æ")

            # é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—å¼ã‚’expanderã§è¡¨ç¤º
            with st.expander("é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—å¼", expanded=False):
                st.markdown(severity_info_text)

            # å…¨ä½“é‡ç¯¤åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            st.markdown("#### å…¨ä½“é‡ç¯¤åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            def extract_severity_ranking(analysis_data):
                severity_data = {}
                relative_bias = analysis_data.get("relative_bias_analysis", {})
                for category, category_data in relative_bias.items():
                    for subcategory, subcat_data in category_data.items():
                        entities = subcat_data.get("entities", {})
                        for entity, entity_data in entities.items():
                            if not isinstance(entity_data, dict):
                                continue
                            severity_score = (entity_data.get("severity_score") or {}).get("severity_score", 0)
                            severity_data[f"{category}/{subcategory}/{entity}"] = severity_score
                return dict(sorted(severity_data.items(), key=lambda x: x[1], reverse=True))
            severity_ranking = extract_severity_ranking(analysis_data)
            if severity_ranking:
                ranking_rows = []
                for i, (entity_path, severity) in enumerate(severity_ranking.items(), 1):
                    path_parts = entity_path.split('/')
                    if len(path_parts) >= 3:
                        category = path_parts[0]
                        subcategory = path_parts[1]
                        entity = path_parts[2]
                        display_name = f"{category}/{subcategory} - {entity}"
                    else:
                        display_name = entity_path
                    ranking_rows.append({
                        "é †ä½": i,
                        "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£": display_name,
                        "é‡ç¯¤åº¦ã‚¹ã‚³ã‚¢": f"{severity:.3f}",
                        "é‡ç¯¤åº¦ãƒ¬ãƒ™ãƒ«": "é‡ç¯¤" if severity > 3.0 else "è»½å¾®"
                    })
                df_severity = pd.DataFrame(ranking_rows)
                st.dataframe(df_severity, use_container_width=True, hide_index=True)
            else:
                st.info("å…¨ä½“é‡ç¯¤åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # å…¨ä½“æ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢
            st.markdown("#### å…¨ä½“æ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢")

            # è¨ˆç®—æ–¹æ³•ã®èª¬æ˜ã‚’è¿½åŠ 
            with st.expander("å…¨ä½“ç›¸é–¢ã®è¨ˆç®—æ–¹æ³•", expanded=False):
                st.markdown("""
                **è¨ˆç®—å¯¾è±¡**: å…¨ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã®æ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢ã®å¹³å‡å€¤

                **è¨ˆç®—æ‰‹é †**:
                1. å„ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã§æ„Ÿæƒ…åˆ†æã®normalized_bias_index vs ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æã®avg_rankã®ç›¸é–¢ã‚’è¨ˆç®—
                2. ãƒ©ãƒ³ã‚­ãƒ³ã‚°å€¤ã®é€†è»¢å‡¦ç†ã‚’å„ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã§å®Ÿè¡Œ
                3. å…¨ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã®Pearsonç›¸é–¢ä¿‚æ•°ã®å¹³å‡å€¤ã‚’ç®—å‡º
                4. å…¨ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã®på€¤ã®å¹³å‡å€¤ã‚’ç®—å‡º

                **è§£é‡ˆ**:
                - æ­£ã®ç›¸é–¢: å…¨ä½“çš„ã«æ„Ÿæƒ…åˆ†æã§å¥½æ„çš„ãªä¼æ¥­ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã‚‚ä¸Šä½
                - è² ã®ç›¸é–¢: å…¨ä½“çš„ã«æ„Ÿæƒ…åˆ†æã§å¥½æ„çš„ãªä¼æ¥­ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã¯ä¸‹ä½
                - ç›¸é–¢ãªã—: å…¨ä½“çš„ã«æ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒç‹¬ç«‹ã—ãŸè©•ä¾¡
                """)

            cross_insights = analysis_data.get("cross_analysis_insights", {})
            sentiment_corr_data = cross_insights.get('sentiment_ranking_correlation', {})
            all_correlations = []
            all_pvals = []
            all_spearman = []
            all_n_entities = []
            for category_data in sentiment_corr_data.values():
                for subcategory_data in category_data.values():
                    if 'correlation' in subcategory_data:
                        all_correlations.append(subcategory_data['correlation'])
                    if 'p_value' in subcategory_data:
                        all_pvals.append(subcategory_data['p_value'])
                    if 'spearman' in subcategory_data:
                        all_spearman.append(subcategory_data['spearman'])
                    if 'n_entities' in subcategory_data:
                        all_n_entities.append(subcategory_data['n_entities'])

            if all_correlations:
                avg_correlation = sum(all_correlations) / len(all_correlations)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å¹³å‡Pearsonç›¸é–¢ä¿‚æ•°", f"{avg_correlation:.3f}")
                with col2:
                    if all_spearman:
                        avg_spearman = sum(all_spearman) / len(all_spearman)
                        st.metric("å¹³å‡Spearmanç›¸é–¢ä¿‚æ•°", f"{avg_spearman:.3f}")
                with col3:
                    if all_n_entities:
                        total_entities = sum(all_n_entities)
                        st.metric("ç·åˆ†æå¯¾è±¡ä¼æ¥­æ•°", f"{total_entities}")

                if all_pvals:
                    avg_pval = sum(all_pvals) / len(all_pvals)
                    st.markdown(f"**å¹³å‡på€¤**: {avg_pval:.3f}")
                    if avg_pval < 0.05:
                        sig_text = "çµ±è¨ˆçš„ã«æœ‰æ„ (p < 0.05)"
                    else:
                        sig_text = "çµ±è¨ˆçš„ã«æœ‰æ„ã§ãªã„ (p >= 0.05)"
                else:
                    sig_text = "på€¤ãƒ‡ãƒ¼ã‚¿ãªã—"

                if avg_correlation > 0.3:
                    corr_text = "æ­£ã®ç›¸é–¢ï¼šæ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§åŒã˜ä¼æ¥­ãŒå„ªé‡ã•ã‚Œã‚‹å‚¾å‘"
                elif avg_correlation < -0.3:
                    corr_text = "è² ã®ç›¸é–¢ï¼šæ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ç•°ãªã‚‹è©•ä¾¡åŸºæº–ãŒåƒãå‚¾å‘"
                else:
                    corr_text = "ç›¸é–¢ãªã—ï¼šæ„Ÿæƒ…åˆ†æã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒç‹¬ç«‹ã—ãŸè©•ä¾¡"
                st.markdown(f"**è§£é‡ˆ**: {corr_text} / {sig_text}")
            else:
                st.metric("å¹³å‡ç›¸é–¢ä¿‚æ•°", "N/A")

            # å…¨ä½“ãƒã‚¤ã‚¢ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
            st.markdown("#### å…¨ä½“ãƒã‚¤ã‚¢ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»è§£èª¬")
            pattern_name = cross_data.get('overall_bias_pattern', 'unknown')
            def translate_pattern_name(pattern_name):
                pattern_labels = {
                    "strong_large_enterprise_favoritism": "å¼·ã„å¤§ä¼æ¥­å„ªé‡",
                    "weak_small_enterprise_penalty": "å¼±ã„ä¸­å°ä¼æ¥­ãƒšãƒŠãƒ«ãƒ†ã‚£",
                    "neutral_balanced_distribution": "ä¸­ç«‹ãƒãƒ©ãƒ³ã‚¹åˆ†å¸ƒ",
                    "unknown": "ä¸æ˜"
                }
                return pattern_labels.get(pattern_name, pattern_name)
            translated_pattern = translate_pattern_name(pattern_name)
            st.metric("ãƒ‘ã‚¿ãƒ¼ãƒ³", translated_pattern)
            # å¿…è¦ã«å¿œã˜ã¦è§£èª¬è¿½åŠ 

            # --- å¸‚å ´åˆ†æã‚¿ãƒ– ---
            with main_tabs[2]:
                # å·¥äº‹ä¸­ãƒãƒŠãƒ¼ï¼ˆå¸‚å ´åˆ†æã‚¿ãƒ–å†…ã§ã®ã¿è¡¨ç¤ºï¼‰
                st.markdown("""
                <div style="background-color: #ff6b6b; color: white; padding: 10px; border-radius: 5px; text-align: center; margin-bottom: 20px;">
                    <h3>ğŸš§ å·¥äº‹ä¸­ ğŸš§</h3>
                    <p><strong>ç¾åœ¨ã€å‹•çš„å¸‚å ´ã‚·ã‚§ã‚¢æ¨å®šã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ä¸­ã§ã™</strong></p>
                    <p>HHIåˆ†ææ©Ÿèƒ½ã¯å‹•ä½œã¯ã—ã¾ã™ãŒã€ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã¤ä¸€éƒ¨æ©Ÿèƒ½ãŒé–‹ç™ºä¸­ã§ã™</p>
                </div>
                """, unsafe_allow_html=True)

                # å·¥äº‹çŠ¶æ³ã®è©³ç´°èª¬æ˜
                with st.expander("â„¹ï¸ ç¾åœ¨ã®é–‹ç™ºçŠ¶æ³", expanded=False):
                    st.markdown("""
                    **ğŸ”„ é€²è¡Œä¸­ã®é–‹ç™º**
                    - å‹•çš„å¸‚å ´ã‚·ã‚§ã‚¢æ¨å®šã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
                    - ã‚µãƒ¼ãƒ“ã‚¹ã‚·ã‚§ã‚¢æ¨å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é–‹ç™º
                    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ HHIè¨ˆç®—æ©Ÿèƒ½ã®æ§‹ç¯‰

                    **âœ… åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½**
                    - æ—¢å­˜ã®ãƒã‚¤ã‚¢ã‚¹åˆ†æï¼ˆRaw Deltaã€BIã€æœ‰æ„æ€§ç­‰ï¼‰
                    - é™çš„HHIåˆ†æï¼ˆä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹å¸‚å ´é›†ä¸­åº¦ï¼‰
                    - å¸‚å ´é›†ä¸­åº¦-ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢åˆ†æ

                    **ğŸš§ é–‹ç™ºä¸­ã®æ©Ÿèƒ½**
                    - å‹•çš„å¸‚å ´ã‚·ã‚§ã‚¢æ¨å®š
                    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ HHIè¨ˆç®—
                    - ãƒã‚¤ã‚¢ã‚¹äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
                    - æ—©æœŸè­¦å‘Šã‚·ã‚¹ãƒ†ãƒ 

                    **ğŸ“… äºˆå®š**
                    - Phase 1: åŸºç›¤æ§‹ç¯‰ï¼ˆé€²è¡Œä¸­ï¼‰
                    - Phase 2: ä¼æ¥­ã‚·ã‚§ã‚¢æ¨å®šï¼ˆäºˆå®šï¼‰
                    - Phase 3: ã‚µãƒ¼ãƒ“ã‚¹ã‚·ã‚§ã‚¢æ¨å®šï¼ˆäºˆå®šï¼‰
                    - Phase 4: çµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆäºˆå®šï¼‰
                    """)

                # === å¸‚å ´æ”¯é…ãƒ»å…¬å¹³æ€§åˆ†æï¼ˆmarket_dominance_analysisï¼‰ ===
                st.subheader("ğŸ¢ å¸‚å ´æ”¯é…ãƒ»å…¬å¹³æ€§åˆ†æ")
                relative_bias = analysis_data.get("relative_bias_analysis", {})
                mda = None
                if selected_category in relative_bias and selected_subcategory in relative_bias[selected_category]:
                    mda = relative_bias[selected_category][selected_subcategory].get("market_dominance_analysis", None)
                if not mda:
                    st.info("å¸‚å ´æ”¯é…ãƒ»å…¬å¹³æ€§åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    # --- ã‚µãƒãƒªãƒ¼ã‚«ãƒ¼ãƒ‰ ---
                    integrated = mda.get("integrated_fairness", {})
                    score = integrated.get("integrated_score", "-")
                    confidence = integrated.get("confidence", "-")
                    interpretation = integrated.get("interpretation", "-")
                    # ç²’åº¦ã”ã¨ã«è¡¨ç¤ºå†…å®¹ã‚’åˆ‡ã‚Šæ›¿ãˆ
                    if selected_category == "ä¼æ¥­" or selected_category == "å¤§å­¦":
                        # ä¼æ¥­ã¨å¤§å­¦ã¯åŒæ§˜ã®åˆ†æã‚’è¡¨ç¤º
                        category_label = "ä¼æ¥­ãƒ¬ãƒ™ãƒ«" if selected_category == "ä¼æ¥­" else "å¤§å­¦ãƒ¬ãƒ™ãƒ«"
                        st.markdown(f"#### {category_label}å…¬å¹³æ€§ã‚¹ã‚³ã‚¢")
                        st.metric(label=f"{category_label}å…¬å¹³æ€§ã‚¹ã‚³ã‚¢", value=score)
                        # st.markdown(f"<span style='font-size:2em; font-weight:bold; color:#2ca02c'>{score}</span>", unsafe_allow_html=True)
                        st.caption(f"{category_label}ã§ã€å¸‚å ´ã‚·ã‚§ã‚¢ã«å¯¾ã—ã¦AIæ¤œç´¢çµæœã®éœ²å‡ºåº¦ãŒã©ã‚Œã ã‘å…¬å¹³ã‹ã‚’ç¤ºã™æŒ‡æ¨™ã§ã™ã€‚1ã«è¿‘ã„ã»ã©å¸‚å ´ã‚·ã‚§ã‚¢é€šã‚Šã«å…¬å¹³ã€1ã‚ˆã‚Šå¤§ãã„ã¨éå‰°éœ²å‡ºã€1æœªæº€ã ã¨éœ²å‡ºä¸è¶³ã‚’æ„å‘³ã—ã¾ã™ã€‚\n\n**è¨ˆç®—æ–¹æ³•:** {category_label}ã”ã¨ã®ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ»å¸‚å ´ã‚·ã‚§ã‚¢ãƒ»éšå±¤é–“æ ¼å·®ãƒ»ãƒã‚¤ã‚¢ã‚¹åˆ†æ•£ãªã©ã‚’ç·åˆè©•ä¾¡ï¼ˆè©³ç´°ã¯docs/bias_metrics_specification.mdå‚ç…§ï¼‰")
                        st.markdown(f"**ä¿¡é ¼åº¦**: {confidence}")
                        st.markdown(f"**è§£é‡ˆ**: {interpretation}")
                        # è£œåŠ©æŒ‡æ¨™ï¼ˆä¾‹ï¼šæ™‚ä¾¡ç·é¡ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç›¸é–¢ï¼‰
                        ent = mda.get("enterprise_level", {})
                        corr = ent.get("correlation_analysis", {})
                        if corr and corr.get("available"):
                            st.markdown("#### å¸‚å ´ã‚·ã‚§ã‚¢ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç›¸é–¢ï¼ˆè£œåŠ©æŒ‡æ¨™ï¼‰")
                            st.markdown(f"- **è¨ˆç®—æ–¹æ³•:** å„{category_label}ã®æ™‚ä¾¡ç·é¡ã¨ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã®ãƒšã‚¢ã§Pearsonç›¸é–¢ä¿‚æ•°ã‚’ç®—å‡º\n- **æ„å‘³:** å¸‚å ´ã‚·ã‚§ã‚¢ãŒå¤§ãã„{category_label}ã»ã©AIã§å„ªé‡ã•ã‚Œã‚‹å‚¾å‘ãŒã‚ã‚‹ã‹ã‚’ç¤ºã™ã€‚å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ã¨ã¯ç›´æ¥ã®è¨ˆç®—é–¢ä¿‚ã¯ãªãã€è£œåŠ©çš„ãªåˆ†ææŒ‡æ¨™ã§ã™ã€‚\n- **ç›¸é–¢ä¿‚æ•°:** {corr.get('correlation_coefficient', '-')}")
                            st.info(corr.get("interpretation", ""))
                    else:
                        st.markdown("#### ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«å…¬å¹³æ€§ã‚¹ã‚³ã‚¢")
                        st.metric(label="ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«å…¬å¹³æ€§ã‚¹ã‚³ã‚¢", value=score)
                        # st.markdown(f"<span style='font-size:2em; font-weight:bold; color:#1f77b4'>{score}</span>", unsafe_allow_html=True)
                        st.caption("ã‚µãƒ¼ãƒ“ã‚¹ç²’åº¦ã§ã€å¸‚å ´ã‚·ã‚§ã‚¢ã«å¯¾ã—ã¦AIæ¤œç´¢çµæœã®éœ²å‡ºåº¦ãŒã©ã‚Œã ã‘å…¬å¹³ã‹ã‚’ç¤ºã™æŒ‡æ¨™ã§ã™ã€‚1ã«è¿‘ã„ã»ã©å¸‚å ´ã‚·ã‚§ã‚¢é€šã‚Šã«å…¬å¹³ã€1ã‚ˆã‚Šå¤§ãã„ã¨éå‰°éœ²å‡ºã€1æœªæº€ã ã¨éœ²å‡ºä¸è¶³ã‚’æ„å‘³ã—ã¾ã™ã€‚\n\n**è¨ˆç®—æ–¹æ³•:** å„ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ»å¸‚å ´ã‚·ã‚§ã‚¢ãƒ»Fair Share Ratioãƒ»ãƒã‚¤ã‚¢ã‚¹åˆ†æ•£ãƒ»å¸‚å ´é›†ä¸­åº¦ãªã©ã‚’ç·åˆè©•ä¾¡ï¼ˆè©³ç´°ã¯docs/bias_metrics_specification.mdå‚ç…§ï¼‰")
                        st.markdown(f"**ä¿¡é ¼åº¦**: {confidence}")
                        st.markdown(f"**è§£é‡ˆ**: {interpretation}")
                        # è£œåŠ©æŒ‡æ¨™ï¼ˆç›¸é–¢ãƒ»æ©Ÿä¼šå‡ç­‰ã‚¹ã‚³ã‚¢ï¼‰
                        svc = mda.get("service_level", {})
                        overall_corr = svc.get("overall_correlation", {})
                        eq_score = svc.get("equal_opportunity_score", "-")
                        if overall_corr and overall_corr.get("correlation_coefficient") is not None:
                            st.markdown("#### å¸‚å ´ã‚·ã‚§ã‚¢ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç›¸é–¢ï¼ˆè£œåŠ©æŒ‡æ¨™ï¼‰")
                            st.markdown(f"- **è¨ˆç®—æ–¹æ³•:** å„ã‚µãƒ¼ãƒ“ã‚¹ã®å¸‚å ´ã‚·ã‚§ã‚¢ã¨ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã®ãƒšã‚¢ã§Pearsonç›¸é–¢ä¿‚æ•°ã‚’ç®—å‡º\n- **æ„å‘³:** å¸‚å ´ã‚·ã‚§ã‚¢ãŒå¤§ãã„ã‚µãƒ¼ãƒ“ã‚¹ã»ã©AIã§å„ªé‡ã•ã‚Œã‚‹å‚¾å‘ãŒã‚ã‚‹ã‹ã‚’ç¤ºã™ã€‚å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ã¨ã¯ç›´æ¥ã®è¨ˆç®—é–¢ä¿‚ã¯ãªãã€è£œåŠ©çš„ãªåˆ†ææŒ‡æ¨™ã§ã™ã€‚\n- **ç›¸é–¢ä¿‚æ•°:** {overall_corr.get('correlation_coefficient', '-')}")
                            st.info(overall_corr.get("interpretation", ""))
                        if eq_score != "-":
                            st.markdown("#### æ©Ÿä¼šå‡ç­‰ã‚¹ã‚³ã‚¢ï¼ˆè£œåŠ©æŒ‡æ¨™ï¼‰")
                            st.markdown("- **è¨ˆç®—æ–¹æ³•:** å„ã‚µãƒ¼ãƒ“ã‚¹ã®Fair Share Ratioï¼ˆæœŸå¾…éœ²å‡ºåº¦Ã·å¸‚å ´ã‚·ã‚§ã‚¢ï¼‰ãŒ1.0ã«è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ã€‚å…¨ã‚µãƒ¼ãƒ“ã‚¹ã®å¹³å‡å€¤ã‚’è¡¨ç¤º\n- **æ„å‘³:** å…¨ã‚µãƒ¼ãƒ“ã‚¹ãŒå‡ç­‰ã«AIæ¤œç´¢ã§éœ²å‡ºã™ã‚‹ç†æƒ³çŠ¶æ…‹ã¨ã®ä¹–é›¢ã‚’ç¤ºã™ã€‚å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ã¨ã¯ç›´æ¥ã®è¨ˆç®—é–¢ä¿‚ã¯ãªãã€è£œåŠ©çš„ãªæŒ‡æ¨™ã§ã™ã€‚\n- **æ©Ÿä¼šå‡ç­‰ã‚¹ã‚³ã‚¢:** " + str(eq_score))
                    st.markdown("---")

                    # --- ä¼æ¥­ãƒ¬ãƒ™ãƒ«åˆ†æ ---
                    # ent = mda.get("enterprise_level", {})
                    # svc = mda.get("service_level", {})
                    # ent_favor = None  # ã“ã“ã§å¿…ãšåˆæœŸåŒ–
                    # if (selected_category == "ä¼æ¥­" or selected_category == "å¤§å­¦") and ent:
                    #     category_label = "ä¼æ¥­" if selected_category == "ä¼æ¥­" else "å¤§å­¦"
                    #     st.markdown(f"### {category_label}ãƒ¬ãƒ™ãƒ«åˆ†æï¼ˆ{category_label}ç²’åº¦ï¼‰")
                    #     ent_score = ent.get("fairness_score", "-")
                    #     ent_favor = ent.get("tier_analysis", {}).get("favoritism_interpretation", "-")
                    #     ent_corr = ent.get("correlation_analysis", {}).get("interpretation", "-")
                    #     ent_corr_coef = ent.get("correlation_analysis", {}).get("correlation_coefficient", "-")
                    #     st.markdown(f"- å…¬å¹³æ€§ã‚¹ã‚³ã‚¢: {ent_score}")
                    #     st.markdown(f"- å„ªé‡å‚¾å‘: {ent_favor}")
                    #     st.markdown(f"- ç›¸é–¢: {ent_corr}ï¼ˆ{ent_corr_coef}ï¼‰")
                    #     # æ£’ã‚°ãƒ©ãƒ•: {category_label}è¦æ¨¡ã”ã¨ã®ãƒã‚¤ã‚¢ã‚¹åˆ†å¸ƒ
                    #     tier_stats = ent.get("tier_analysis", {}).get("tier_statistics", {})
                    #     entities = mda.get("entities", {})
                    #     import matplotlib.pyplot as plt
                    #     from src.utils.plot_utils import plot_enterprise_bias_bar, plot_marketcap_bias_scatter
                    #     if tier_stats and entities:
                    #         fig = plot_enterprise_bias_bar(tier_stats, entities)
                    #         st.pyplot(fig, use_container_width=True)
                    #         plt.close(fig)
                    #     else:
                    #         st.info(f"{category_label}è¦æ¨¡ã”ã¨ã®ãƒã‚¤ã‚¢ã‚¹åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    #     # æ•£å¸ƒå›³: æ™‚ä¾¡ç·é¡ã¨ãƒã‚¤ã‚¢ã‚¹
                    #     marketcap_bias = []
                    #     for ename, edata in entities.items():
                    #         mc = edata.get("market_cap")
                    #         bi = edata.get("bias_index")
                    #         if mc is not None and bi is not None:
                    #             marketcap_bias.append((ename, mc, bi))
                    #     if marketcap_bias:
                    #         fig = plot_marketcap_bias_scatter(marketcap_bias)
                    #         st.pyplot(fig, use_container_width=True)
                    #         plt.close(fig)
                    #     else:
                    #         st.info("æ™‚ä¾¡ç·é¡ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç›¸é–¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    #     st.markdown("---")
                    # --- ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ ---
                    # if selected_category not in ["ä¼æ¥­", "å¤§å­¦"] and svc:
                    #     st.markdown("### ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æï¼ˆã‚µãƒ¼ãƒ“ã‚¹ç²’åº¦ï¼‰")
                    #     cat_fairness = svc.get("category_fairness", {})
                    #     overall_corr = svc.get("overall_correlation", {})
                    #     eq_score = svc.get("equal_opportunity_score", "-")
                    #     # å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ•°å€¤ã®ã¿ã€è©³ç´°ã¯ä¸Šéƒ¨å‚ç…§ï¼‰
                    #     st.markdown(f"- å…¬å¹³æ€§ã‚¹ã‚³ã‚¢: {cat_fairness}")
                    #     st.caption("â€»å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ã®è©³ç´°ãªèª¬æ˜ã¯ä¸Šéƒ¨ã®çµ±åˆå¸‚å ´å…¬å¹³æ€§ã‚¹ã‚³ã‚¢æ¬„ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")
                    #     # å¸‚å ´ã‚·ã‚§ã‚¢ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç›¸é–¢ï¼ˆæ•°å€¤ã®ã¿ï¼‰
                    #     st.markdown(f"- å¸‚å ´ã‚·ã‚§ã‚¢ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç›¸é–¢: {overall_corr.get('correlation_coefficient', '-')}")
                    #     # ç›¸é–¢è§£é‡ˆæ–‡ï¼ˆå‚¾å‘æ–‡ï¼‰ã‚’å¿…ãšè¡¨ç¤º
                    #     if overall_corr.get('interpretation'):
                    #         st.info(f"ç›¸é–¢è§£é‡ˆ: {overall_corr.get('interpretation')}")
                    #     # æ©Ÿä¼šå‡ç­‰ã‚¹ã‚³ã‚¢ï¼ˆæ•°å€¤ï¼‹è§£èª¬æ–‡ï¼‰
                    #     st.markdown(f"- æ©Ÿä¼šå‡ç­‰ã‚¹ã‚³ã‚¢: {eq_score}")
                    #     st.caption("å¸‚å ´ã‚·ã‚§ã‚¢ã«é–¢ä¿‚ãªãã€å…¨ã‚µãƒ¼ãƒ“ã‚¹ãŒå‡ç­‰ã«AIæ¤œç´¢ã§éœ²å‡ºã™ã‚‹ç†æƒ³çŠ¶æ…‹ã¨ã®ä¹–é›¢ã‚’ç¤ºã™æŒ‡æ¨™ã§ã™ã€‚0ã«è¿‘ã„ã»ã©æ©Ÿä¼šå‡ç­‰ã€å€¤ãŒå¤§ãã„ã»ã©ä¸€éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã«åã‚ŠãŒã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚")
                    #     # æ£’ã‚°ãƒ©ãƒ•ã¯å‰Šé™¤ã€æ•£å¸ƒå›³ã®ã¿æ®‹ã™
                    #     from src.utils.plot_utils import plot_service_share_bias_scatter
                    #     entities = mda.get("entities", {})
                    #     share_bias = []
                    #     for sname, sdata in entities.items():
                    #         share = sdata.get("market_share")
                    #         bi = sdata.get("bias_index")
                    #         if share is not None and bi is not None:
                    #             share_bias.append((sname, share, bi))
                    #     if share_bias:
                    #         fig = plot_service_share_bias_scatter(share_bias)
                    #         st.pyplot(fig, use_container_width=True)
                    #         plt.close(fig)
                    #     else:
                    #         st.info("ã‚µãƒ¼ãƒ“ã‚¹ã”ã¨ã®å¸‚å ´ã‚·ã‚§ã‚¢ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç›¸é–¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å®Ÿè¡Œå›æ•°ãŒå°‘ãªã„å ´åˆã‚„ãƒ‡ãƒ¼ã‚¿æ¬ ææ™‚ã¯ã‚°ãƒ©ãƒ•ãŒè¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ã€‚")
                    #     st.markdown("---")

                    # # --- ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ»æ¨å¥¨äº‹é … ---
                    # st.markdown("### ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ»æ¨å¥¨äº‹é …")
                    # insights = []
                    # # interpretationã¯ã“ã“ã§ã¯è¡¨ç¤ºã—ãªã„ï¼ˆã‚µãƒãƒªãƒ¼ã‚«ãƒ¼ãƒ‰ã§è¡¨ç¤ºæ¸ˆã¿ï¼‰
                    # for rec in mda.get("improvement_recommendations", []):
                    #     insights.append(f"æ”¹å–„æ¨å¥¨: {rec}")
                    # if insights:
                    #     for ins in insights:
                    #         st.markdown(f"- {ins}")
                    # else:
                    #     st.info("ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ»æ¨å¥¨äº‹é …ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    # st.markdown("---")

                    # # --- è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆexpanderï¼‰ ---
                    # with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆmarket_dominance_analysis JSONï¼‰", expanded=False):
                    #     st.json(mda, expanded=False)

                    # === HHIåˆ†æUIé–¢æ•°ç¾¤ ===
                    def render_market_concentration_analysis(analysis_data, selected_category, selected_subcategory):
                        """
                        å¸‚å ´é›†ä¸­åº¦åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
                        """
                        relative_bias = analysis_data.get("relative_bias_analysis", {})
                        market_concentration = None
                        if (selected_category in relative_bias and
                            selected_subcategory in relative_bias[selected_category]):
                            market_concentration = relative_bias[selected_category][selected_subcategory].get(
                                "market_concentration_analysis", None
                            )
                        if not market_concentration:
                            st.info("å¸‚å ´é›†ä¸­åº¦åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                            return
                        st.markdown("---")
                        st.subheader("ğŸ“Š å¸‚å ´é›†ä¸­åº¦åˆ†æ")
                        st.caption("å¸‚å ´æ§‹é€ ã®é›†ä¸­åº¦ã¨ãƒã‚¤ã‚¢ã‚¹ãƒªã‚¹ã‚¯ã®é–¢ä¿‚æ€§ã‚’åˆ†æã—ã¾ã™")

                        # HHIè¨ˆç®—å¼ã®èª¬æ˜ã‚’è¿½åŠ 
                        with st.expander("â„¹ï¸ HHIï¼ˆãƒãƒ¼ãƒ•ã‚£ãƒ³ãƒ€ãƒ¼ãƒ«ãƒ»ãƒãƒ¼ã‚·ãƒ¥ãƒãƒ³æŒ‡æ•°ï¼‰ã®è¨ˆç®—å¼", expanded=False):
                            st.markdown("""
                            **HHI = Î£(å„ä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã®å¸‚å ´ã‚·ã‚§ã‚¢)Â²**

                            **è¨ˆç®—ä¾‹:**
                            - ä¼æ¥­A: 40% â†’ 40Â² = 1,600
                            - ä¼æ¥­B: 30% â†’ 30Â² = 900
                            - ä¼æ¥­C: 20% â†’ 20Â² = 400
                            - ä¼æ¥­D: 10% â†’ 10Â² = 100

                            **HHI = 1,600 + 900 + 400 + 100 = 3,000**

                            **è§£é‡ˆ:**
                            - 1,500æœªæº€: ä½é›†ä¸­å¸‚å ´
                            - 1,500-2,500: ä¸­é›†ä¸­å¸‚å ´
                            - 2,500ä»¥ä¸Š: é«˜é›†ä¸­å¸‚å ´
                            """)

                        render_hhi_summary_metrics(market_concentration, selected_category)

                        # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ã¦è©³ç´°åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ‡ã‚Šæ›¿ãˆ
                        if selected_category == "ä¼æ¥­":
                            # ä¼æ¥­ã‚«ãƒ†ã‚´ãƒª: ä¼æ¥­ãƒ¬ãƒ™ãƒ«HHIåˆ†æã®ã¿è¡¨ç¤º
                            render_enterprise_hhi_analysis(market_concentration, selected_category)
                        else:
                            # ãã®ä»–ã®ã‚«ãƒ†ã‚´ãƒª: ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«HHIåˆ†æã®ã¿è¡¨ç¤º
                            render_service_hhi_analysis(market_concentration)

                        # ç›¸é–¢åˆ†æã¨ã‚¤ãƒ³ã‚µã‚¤ãƒˆã¯å…¨ã‚«ãƒ†ã‚´ãƒªã§è¡¨ç¤º
                        render_concentration_bias_correlation(market_concentration)
                        render_market_structure_insights(market_concentration)

                    def render_hhi_summary_metrics(market_concentration, selected_category):
                        st.markdown("#### ğŸ“ˆ å¸‚å ´é›†ä¸­åº¦æ¦‚è¦")
                        service_hhi = market_concentration.get("service_hhi", {})
                        service_score = service_hhi.get("hhi_score", 0.0)
                        service_level = service_hhi.get("concentration_level", "ä¸æ˜")
                        enterprise_hhi = market_concentration.get("enterprise_hhi", {})
                        enterprise_score = enterprise_hhi.get("hhi_score", 0.0)
                        enterprise_level = enterprise_hhi.get("concentration_level", "ä¸æ˜")
                        correlation = market_concentration.get("concentration_bias_correlation", {})
                        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ä¿®æ­£: correlation_analysiså†…ã‹ã‚‰å–å¾—
                        correlation_analysis = correlation.get("correlation_analysis", {})
                        # ç›¸é–¢ä¿‚æ•°ã®å€¤ã‚’å–å¾—
                        enterprise_corr = correlation_analysis.get("enterprise_hhi_bias_correlation", 0.0)
                        service_corr = correlation_analysis.get("service_hhi_bias_correlation", 0.0)

                        # ç›¸é–¢ä¿‚æ•°ã«åŸºã¥ã„ã¦é©åˆ‡ãªè§£é‡ˆã‚’ç”Ÿæˆ
                        def get_correlation_interpretation(corr_value):
                            if abs(corr_value) < 0.1:
                                return "ç›¸é–¢ãªã—"
                            elif corr_value >= 0.7:
                                return "å¼·ã„æ­£ã®ç›¸é–¢"
                            elif corr_value >= 0.3:
                                return "ä¸­ç¨‹åº¦ã®æ­£ã®ç›¸é–¢"
                            elif corr_value >= 0.1:
                                return "å¼±ã„æ­£ã®ç›¸é–¢"
                            elif corr_value <= -0.7:
                                return "å¼·ã„è² ã®ç›¸é–¢"
                            elif corr_value <= -0.3:
                                return "ä¸­ç¨‹åº¦ã®è² ã®ç›¸é–¢"
                            else:
                                return "å¼±ã„è² ã®ç›¸é–¢"

                        # ä¼æ¥­ãƒ»å¤§å­¦ã®å ´åˆã¯enterprise_corrã€ãã®ä»–ã¯service_corrã‚’ä½¿ç”¨
                        if selected_category in ["ä¼æ¥­", "å¤§å­¦"]:
                            correlation_strength = get_correlation_interpretation(enterprise_corr)
                        else:
                            correlation_strength = get_correlation_interpretation(service_corr)

                        # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ã¦è¡¨ç¤ºå†…å®¹ã‚’åˆ‡ã‚Šæ›¿ãˆ
                        if selected_category == "ä¼æ¥­" or selected_category == "å¤§å­¦":
                            # ä¼æ¥­ãƒ»å¤§å­¦ã‚«ãƒ†ã‚´ãƒª: çµ„ç¹”å¸‚å ´é›†ä¸­åº¦ã¨ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢ã‚’2ã‚«ãƒ©ãƒ ã§è¡¨ç¤º
                            category_label = "ä¼æ¥­" if selected_category == "ä¼æ¥­" else "å¤§å­¦"
                            col1, col2 = st.columns(2)
                            with col1:
                                if enterprise_score > 0:
                                    st.metric(label=f"{category_label}å¸‚å ´é›†ä¸­åº¦", value=f"{enterprise_score:.1f}", delta=enterprise_level)
                                else:
                                    st.metric(label=f"{category_label}å¸‚å ´é›†ä¸­åº¦", value="è¨ˆç®—ä¸å¯", delta="ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                            with col2:
                                st.metric(label="ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢å¼·åº¦", value=correlation_strength, delta="å¸‚å ´é›†ä¸­åº¦ã¨ã®é–¢ä¿‚")
                        else:
                            # ãã®ä»–ã®ã‚«ãƒ†ã‚´ãƒª: ã‚µãƒ¼ãƒ“ã‚¹å¸‚å ´é›†ä¸­åº¦ã¨ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢ã‚’2ã‚«ãƒ©ãƒ ã§è¡¨ç¤º
                            col1, col2 = st.columns(2)
                            with col1:
                                if service_score > 0:
                                    st.metric(label="ã‚µãƒ¼ãƒ“ã‚¹å¸‚å ´é›†ä¸­åº¦", value=f"{service_score:.1f}", delta=service_level)
                                else:
                                    st.metric(label="ã‚µãƒ¼ãƒ“ã‚¹å¸‚å ´é›†ä¸­åº¦", value="è¨ˆç®—ä¸å¯", delta="ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                            with col2:
                                st.metric(label="ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢å¼·åº¦", value=correlation_strength, delta="å¸‚å ´é›†ä¸­åº¦ã¨ã®é–¢ä¿‚")

                    def render_service_hhi_analysis(market_concentration):
                        service_hhi = market_concentration.get("service_hhi", {})
                        if not service_hhi or service_hhi.get("hhi_score", 0) == 0:
                            st.info("ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«HHIåˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                            return
                        st.markdown("#### ğŸ¢ ã‚µãƒ¼ãƒ“ã‚¹å¸‚å ´é›†ä¸­åº¦åˆ†æ")
                        hhi_score = service_hhi.get("hhi_score", 0)
                        concentration_level = service_hhi.get("concentration_level", "ä¸æ˜")
                        market_structure = service_hhi.get("market_structure", "ä¸æ˜")
                        if concentration_level == "é«˜é›†ä¸­å¸‚å ´":
                            color = "ğŸ”´"
                        elif concentration_level == "ä¸­ç¨‹åº¦é›†ä¸­å¸‚å ´":
                            color = "ğŸŸ¡"
                        else:
                            color = "ğŸŸ¢"
                        st.markdown(f"**HHIå€¤**: {hhi_score:.1f} ({color} {concentration_level})")
                        st.markdown(f"**å¸‚å ´æ§‹é€ **: {market_structure}")
                        top_services = service_hhi.get("top_services", [])
                        if top_services:
                            st.markdown("**ä¸Šä½ã‚µãƒ¼ãƒ“ã‚¹**:")
                            for service in top_services[:5]:
                                rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£"][service.get("rank", 1) - 1]
                                st.markdown(f"- {rank_emoji} {service.get('service', '')}: {service.get('share', 0):.1f}%")
                        effective_competitors = service_hhi.get("effective_competitors", 0)
                        if effective_competitors > 0:
                            st.markdown(f"**æœ‰åŠ¹ç«¶äº‰è€…æ•°**: {effective_competitors}ç¤¾")
                        share_dispersion = service_hhi.get("share_dispersion", 0)
                        if share_dispersion > 0:
                            st.markdown(f"**ã‚·ã‚§ã‚¢åˆ†æ•£**: {share_dispersion:.2f}")

                    def render_enterprise_hhi_analysis(market_concentration, selected_category):
                        enterprise_hhi = market_concentration.get("enterprise_hhi", {})
                        if not enterprise_hhi or enterprise_hhi.get("hhi_score", 0) == 0:
                            st.info("ä¼æ¥­ãƒ»å¤§å­¦ãƒ¬ãƒ™ãƒ«HHIåˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                            return

                        # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸã‚¿ã‚¤ãƒˆãƒ«è¨­å®š
                        if selected_category == "ä¼æ¥­":
                            st.markdown("#### ğŸ­ ä¼æ¥­å¸‚å ´é›†ä¸­åº¦åˆ†æ")
                            tier_labels = ["å¤§ä¼æ¥­", "ä¸­ä¼æ¥­", "å°ä¼æ¥­"]
                        elif selected_category == "å¤§å­¦":
                            st.markdown("#### ğŸ“ å¤§å­¦å¸‚å ´é›†ä¸­åº¦åˆ†æ")
                            tier_labels = ["å¤§è¦æ¨¡å¤§å­¦", "ä¸­è¦æ¨¡å¤§å­¦", "å°è¦æ¨¡å¤§å­¦"]
                        else:
                            st.markdown("#### ğŸ¢ çµ„ç¹”å¸‚å ´é›†ä¸­åº¦åˆ†æ")
                            tier_labels = ["å¤§è¦æ¨¡çµ„ç¹”", "ä¸­è¦æ¨¡çµ„ç¹”", "å°è¦æ¨¡çµ„ç¹”"]
                        hhi_score = enterprise_hhi.get("hhi_score", 0)
                        concentration_level = enterprise_hhi.get("concentration_level", "ä¸æ˜")
                        if concentration_level == "é«˜é›†ä¸­å¸‚å ´":
                            color = "ğŸ”´"
                        elif concentration_level == "ä¸­ç¨‹åº¦é›†ä¸­å¸‚å ´":
                            color = "ğŸŸ¡"
                        else:
                            color = "ğŸŸ¢"
                        st.markdown(f"**HHIå€¤**: {hhi_score:.1f} ({color} {concentration_level})")
                        enterprise_tiers = enterprise_hhi.get("enterprise_tiers", {})
                        if enterprise_tiers:
                            st.markdown("**è¦æ¨¡åˆ¥ã‚·ã‚§ã‚¢**:")
                            values = [
                                enterprise_tiers.get("large", 0),
                                enterprise_tiers.get("medium", 0),
                                enterprise_tiers.get("small", 0)
                            ]
                            non_zero_labels = []
                            non_zero_values = []
                            for label, value in zip(tier_labels, values):
                                if value > 0:
                                    non_zero_labels.append(label)
                                    non_zero_values.append(value)
                            if non_zero_values:
                                fig = go.Figure(data=[go.Pie(
                                    labels=non_zero_labels,
                                    values=non_zero_values,
                                    hole=0.3,
                                    marker_colors=['#ff7f0e', '#2ca02c', '#d62728']
                                )])
                                fig.update_layout(title="è¦æ¨¡åˆ¥ã‚·ã‚§ã‚¢åˆ†å¸ƒ", height=400)
                                st.plotly_chart(fig, use_container_width=True)
                            else:
                                st.info("è¦æ¨¡åˆ¥ã‚·ã‚§ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        market_power = enterprise_hhi.get("market_power_analysis", "")
                        if market_power:
                            st.markdown(f"**å¸‚å ´æ”¯é…åŠ›åˆ†æ**: {market_power}")
                        bias_risk = enterprise_hhi.get("bias_risk_assessment", "")
                        if bias_risk:
                            if "é«˜ã„" in bias_risk or "æ¥µã‚ã¦é«˜ã„" in bias_risk:
                                st.error(f"**ãƒã‚¤ã‚¢ã‚¹ãƒªã‚¹ã‚¯è©•ä¾¡**: {bias_risk}")
                            elif "ä¸­ç¨‹åº¦" in bias_risk:
                                st.warning(f"**ãƒã‚¤ã‚¢ã‚¹ãƒªã‚¹ã‚¯è©•ä¾¡**: {bias_risk}")
                            else:
                                st.success(f"**ãƒã‚¤ã‚¢ã‚¹ãƒªã‚¹ã‚¯è©•ä¾¡**: {bias_risk}")

                    def render_concentration_bias_correlation(market_concentration):
                        correlation = market_concentration.get("concentration_bias_correlation", {})
                        if not correlation:
                            st.info("é›†ä¸­åº¦-ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                            return
                        st.markdown("#### ğŸ“ˆ é›†ä¸­åº¦-ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢åˆ†æ")
                        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ä¿®æ­£: correlation_analysiså†…ã‹ã‚‰å–å¾—
                        correlation_analysis = correlation.get("correlation_analysis", {})
                        service_corr = correlation_analysis.get("service_hhi_bias_correlation", 0)
                        if service_corr != 0:
                            st.markdown(f"**ã‚µãƒ¼ãƒ“ã‚¹HHI-ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢**: {service_corr:.3f}")
                        enterprise_corr = correlation_analysis.get("enterprise_hhi_bias_correlation", 0)
                        if enterprise_corr != 0:
                            # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ã¦ãƒ©ãƒ™ãƒ«ã‚’å¤‰æ›´
                            if selected_category == "ä¼æ¥­":
                                st.markdown(f"**ä¼æ¥­HHI-ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢**: {enterprise_corr:.3f}")
                            elif selected_category == "å¤§å­¦":
                                st.markdown(f"**å¤§å­¦HHI-ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢**: {enterprise_corr:.3f}")
                            else:
                                st.markdown(f"**çµ„ç¹”HHI-ãƒã‚¤ã‚¢ã‚¹ç›¸é–¢**: {enterprise_corr:.3f}")
                        # ç›¸é–¢ä¿‚æ•°ã«åŸºã¥ã„ã¦é©åˆ‡ãªè§£é‡ˆã‚’ç”Ÿæˆ
                        def get_correlation_interpretation(corr_value):
                            if abs(corr_value) < 0.1:
                                return "ç›¸é–¢ãªã—"
                            elif corr_value >= 0.7:
                                return "å¼·ã„æ­£ã®ç›¸é–¢"
                            elif corr_value >= 0.3:
                                return "ä¸­ç¨‹åº¦ã®æ­£ã®ç›¸é–¢"
                            elif corr_value >= 0.1:
                                return "å¼±ã„æ­£ã®ç›¸é–¢"
                            elif corr_value <= -0.7:
                                return "å¼·ã„è² ã®ç›¸é–¢"
                            elif corr_value <= -0.3:
                                return "ä¸­ç¨‹åº¦ã®è² ã®ç›¸é–¢"
                            else:
                                return "å¼±ã„è² ã®ç›¸é–¢"

                        # ä¼æ¥­ãƒ»å¤§å­¦ã®å ´åˆã¯enterprise_corrã€ãã®ä»–ã¯service_corrã‚’ä½¿ç”¨
                        if selected_category in ["ä¼æ¥­", "å¤§å­¦"]:
                            correlation_strength = get_correlation_interpretation(enterprise_corr)
                        else:
                            correlation_strength = get_correlation_interpretation(service_corr)

                        if correlation_strength:
                            st.markdown(f"**ç›¸é–¢å¼·åº¦**: {correlation_strength}")
                        interpretation = correlation_analysis.get("interpretation", "")
                        if interpretation:
                            st.info(f"**ç›¸é–¢è§£é‡ˆ**: {interpretation}")
                        st.markdown("**ç›¸é–¢ä¿‚æ•°ã®è§£é‡ˆ**:")
                        st.markdown("- **0.7ä»¥ä¸Š**: å¼·ã„æ­£ã®ç›¸é–¢ï¼ˆå¸‚å ´é›†ä¸­åº¦ãŒé«˜ã„ã»ã©ãƒã‚¤ã‚¢ã‚¹ãŒå¼·ã„ï¼‰")
                        st.markdown("- **0.3-0.7**: ä¸­ç¨‹åº¦ã®æ­£ã®ç›¸é–¢")
                        st.markdown("- **0.0-0.3**: å¼±ã„æ­£ã®ç›¸é–¢")
                        st.markdown("- **0.0**: ç›¸é–¢ãªã—")
                        st.markdown("- **è² ã®å€¤**: é€†ç›¸é–¢ï¼ˆå¸‚å ´é›†ä¸­åº¦ãŒé«˜ã„ã»ã©ãƒã‚¤ã‚¢ã‚¹ãŒå¼±ã„ï¼‰")

                    def render_market_structure_insights(market_concentration):
                        insights = market_concentration.get("market_structure_insights", [])
                        if not insights:
                            st.info("å¸‚å ´æ§‹é€ ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                            return
                        st.markdown("#### ğŸ’¡ å¸‚å ´æ§‹é€ ã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
                        for i, insight in enumerate(insights, 1):
                            if "é«˜ã„" in insight or "æ¥µã‚ã¦" in insight or "é¡•è‘—" in insight:
                                icon = "ğŸ”´"
                            elif "ä¸­ç¨‹åº¦" in insight or "æœŸå¾…" in insight:
                                icon = "ğŸŸ¡"
                            else:
                                icon = "ğŸŸ¢"
                            st.markdown(f"{icon} **{i}.** {insight}")
                        st.markdown("**æ”¹å–„ææ¡ˆ**:")
                        st.markdown("- å¸‚å ´é›†ä¸­åº¦ã®ç›£è¦–å¼·åŒ–")
                        st.markdown("- ç«¶äº‰ä¿ƒé€²æ”¿ç­–ã®æ¤œè¨")
                        st.markdown("- ãƒã‚¤ã‚¢ã‚¹è»½æ¸›ç­–ã®å®Ÿè£…")

                    # === å¸‚å ´é›†ä¸­åº¦åˆ†æï¼ˆHHIï¼‰ ===
                    render_market_concentration_analysis(analysis_data, selected_category, selected_subcategory)

    elif viz_type_detail == "ãŠã™ã™ã‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æçµæœ":
        # perplexity_rankingsãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥å‚ç…§
        source_data = dashboard_data.get("source_data", {})
        perplexity_rankings = source_data.get("perplexity_rankings", {})

        if perplexity_rankings:
            # é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªãŒãŠã™ã™ã‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æçµæœã§åˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if selected_category not in perplexity_rankings:
                st.warning(f"é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒª '{selected_category}' ã¯ãŠã™ã™ã‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æçµæœã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                st.stop()

            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¸æŠæ©Ÿèƒ½ã‚’è¿½åŠ 
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—
            subcat_data = perplexity_rankings[selected_category][selected_subcategory]
            ranking_summary = subcat_data.get("ranking_summary", {})
            entities = ranking_summary.get("entities", {})
            all_entities = list(entities.keys())
            all_entities.sort()

            if all_entities:
                selected_entities = st.sidebar.multiselect(
                    "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
                    all_entities,
                    default=all_entities,  # å…¨ä»¶ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¡¨ç¤º
                    key="sentiment_entities"
                )
            else:
                selected_entities = []

            st.subheader(f"ãŠã™ã™ã‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æçµæœï½œ{selected_category}ï½œ{selected_subcategory}")

            # === 1. ãƒã‚¹ã‚¯ã‚ã‚Šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ãƒ¼ã‚¿å†…å®¹è¡¨ç¤º ===
            answer_list = subcat_data.get("answer_list", [])
            avg_ranking = ranking_summary.get("avg_ranking", [])

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±è¡¨ç¤º
            if "prompt" in subcat_data:
                with st.expander("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", expanded=False):
                    st.markdown(f"**æ¤œç´¢ã‚¯ã‚¨ãƒª**: {subcat_data['prompt']}")
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
            st.markdown("**ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«**:")
            if entities and avg_ranking:
                table_rows = []
                for i, entity_name in enumerate(avg_ranking):
                    # é¸æŠã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿ã‚’è¡¨ç¤º
                    if entity_name in entities and (not selected_entities or entity_name in selected_entities):
                        entity_data = entities[entity_name]
                        avg_rank = entity_data.get("avg_rank", "æœªãƒ©ãƒ³ã‚¯")
                        all_ranks = entity_data.get("all_ranks", [])
                        official_url = entity_data.get("official_url", "")

                        # é †ä½çµ±è¨ˆè¨ˆç®—
                        if all_ranks and isinstance(avg_rank, (int, float)):
                            rank_std = (sum((r - avg_rank) ** 2 for r in all_ranks) / len(all_ranks)) ** 0.5 if len(all_ranks) > 1 else 0
                            min_rank = min(all_ranks)
                            max_rank = max(all_ranks)
                            rank_variation = max_rank - min_rank
                        else:
                            rank_std = 0
                            min_rank = max_rank = rank_variation = "N/A"

                        table_rows.append({
                            "é †ä½": i + 1,
                            "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£": entity_name,
                            "å¹³å‡é †ä½": avg_rank if isinstance(avg_rank, (int, float)) else 0,
                            "é †ä½æ¨™æº–åå·®": rank_std if rank_std else 0,
                            "æœ€è‰¯é †ä½": min_rank,
                            "æœ€æ‚ªé †ä½": max_rank,
                            "é †ä½å¤‰å‹•": rank_variation,
                            "å…¬å¼URL": official_url
                        })

                if table_rows:
                    df_ranking = pd.DataFrame(table_rows)
                    df_ranking = df_ranking.sort_values(by="å¹³å‡é †ä½", ascending=True)
                    st.dataframe(df_ranking, use_container_width=True)
                else:
                    st.info("è¡¨ç¤ºå¯èƒ½ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                st.info("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # å®Ÿè¡Œçµ±è¨ˆè¡¨ç¤º
            with st.expander("**å®Ÿè¡Œçµ±è¨ˆ**", expanded=False):
                total_executions = len(answer_list)
                st.markdown(f"- ç·å®Ÿè¡Œå›æ•°: {total_executions}")

                if avg_ranking:
                    # é †ä½æ¨™æº–åå·®ã®è¨ˆç®—
                    rank_std_values = []
                    max_rank_variation = 0

                    for entity_name, entity_data in entities.items():
                        # é¸æŠã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
                        if not selected_entities or entity_name in selected_entities:
                            all_ranks = entity_data.get("all_ranks", [])
                            if all_ranks:
                                avg_rank = sum(all_ranks) / len(all_ranks)
                                rank_std = (sum((r - avg_rank) ** 2 for r in all_ranks) / len(all_ranks)) ** 0.5
                                rank_std_values.append(rank_std)
                                rank_variation = max(all_ranks) - min(all_ranks)
                                max_rank_variation = max(max_rank_variation, rank_variation)

                    avg_rank_std = sum(rank_std_values) / len(rank_std_values) if rank_std_values else 0

                    st.markdown(f"- å¹³å‡é †ä½æ¨™æº–åå·®: {avg_rank_std:.3f}")
                    st.markdown(f"- æœ€å¤§é †ä½å¤‰å‹•: {max_rank_variation}")
            with st.expander("**å–å¾—å›ç­”ã‚µãƒ³ãƒ—ãƒ«**", expanded=False):

                # å›ç­”ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
                if answer_list:
                    for i, answer_data in enumerate(answer_list[:3]):  # æœ€åˆã®3ã¤ã‚’è¡¨ç¤º
                        answer_text = answer_data.get("answer", "") if isinstance(answer_data, dict) else str(answer_data)
                        with st.expander(f"å›ç­” {i+1}", expanded=False):
                            st.text(answer_text[:500] + "..." if len(answer_text) > 500 else answer_text)
                # è©³ç´°JSONãƒ‡ãƒ¼ã‚¿
            with st.expander("**è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰**", expanded=True):
                st.json(subcat_data, expanded=False)

            # === 2. ä¸»è¦æŒ‡æ¨™ã‚µãƒãƒªãƒ¼è¡¨ã¨è©³ç´°è§£é‡ˆ ===

            # ranking_bias_analysisã‹ã‚‰åˆ†ææ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            analysis_data = dashboard_data.get("analysis_results", {})
            ranking_bias_data = analysis_data.get("ranking_bias_analysis", {})

            if ranking_bias_data and selected_category in ranking_bias_data and selected_subcategory in ranking_bias_data[selected_category]:
                category_bias_data = ranking_bias_data[selected_category][selected_subcategory]

                # category_summaryã‹ã‚‰å„ç¨®åˆ†æçµæœã‚’å–å¾—
                category_summary = category_bias_data.get("category_summary", {})
                stability_analysis = category_summary.get("stability_analysis", {})
                quality_analysis = category_summary.get("quality_analysis", {})
                category_level_analysis = category_summary.get("category_level_analysis", {})

            # ä¸»è¦æŒ‡æ¨™ã‚µãƒãƒªãƒ¼è¡¨ï¼ˆranking_bias_analysiså„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
            st.subheader("ğŸ“Š ä¸»è¦æŒ‡æ¨™ã‚µãƒãƒªãƒ¼è¡¨")

            if ranking_bias_data and selected_category in ranking_bias_data and selected_subcategory in ranking_bias_data[selected_category]:
                category_bias_data = ranking_bias_data[selected_category][selected_subcategory]
                category_summary = category_bias_data.get("category_summary", {})
                stability_analysis = category_summary.get("stability_analysis", {})
                quality_analysis = category_summary.get("quality_analysis", {})
                category_level_analysis = category_summary.get("category_level_analysis", {})

                # åˆ†æçµæœè§£èª¬ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
                def get_stability_result_explanation(score, std_val):
                    if score >= 0.9:
                        return "æ¥µã‚ã¦å®‰å®šã—ãŸçµæœã§ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«é«˜ã„ä¿¡é ¼æ€§ãŒã‚ã‚‹"
                    elif score >= 0.8:
                        return "å®‰å®šã—ãŸçµæœã§ã€åˆ†æã«é©ç”¨å¯èƒ½ãªä¿¡é ¼æ€§ã‚’æŒã¤"
                    elif score >= 0.6:
                        return "ä¸­ç¨‹åº¦ã®å®‰å®šæ€§ã§ã€å‚¾å‘æŠŠæ¡ã«ã¯æœ‰åŠ¹"
                    else:
                        return "å¤‰å‹•ãŒå¤§ããã€çµæœã®è§£é‡ˆã«ã¯æ³¨æ„ãŒå¿…è¦"

                def get_quality_result_explanation(score):
                    if score >= 0.9:
                        return "éå¸¸ã«é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã§ã€ç²¾å¯†ãªåˆ†æãŒå¯èƒ½"
                    elif score >= 0.8:
                        return "é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã§ã€ä¿¡é ¼æ€§ã®é«˜ã„åˆ†æçµæœã‚’æä¾›"
                    elif score >= 0.6:
                        return "ä¸€å®šå“è³ªã®ãƒ‡ãƒ¼ã‚¿ã§ã€åŸºæœ¬çš„ãªåˆ†æã¯å¯èƒ½"
                    else:
                        return "ãƒ‡ãƒ¼ã‚¿å“è³ªã«èª²é¡ŒãŒã‚ã‚Šã€çµæœè§£é‡ˆã«æ³¨æ„ãŒå¿…è¦"

                def get_competitive_balance_explanation(balance):
                    if balance == "é«˜":
                        return "å¤šæ•°ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒç«¶äº‰ã—ã€å¸‚å ´ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯å¥½"
                    elif balance == "ä¸­":
                        return "é©åº¦ãªç«¶äº‰ç’°å¢ƒã§ã€ã‚ã‚‹ç¨‹åº¦ã®å¸‚å ´ãƒãƒ©ãƒ³ã‚¹ã‚’ä¿æŒ"
                    else:
                        return "ç«¶äº‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒå°‘ãªãã€é™å®šçš„ãªå¸‚å ´æ§‹é€ "

                def get_ranking_spread_explanation(spread):
                    if spread == "full":
                        return "å…¨é †ä½ç¯„å›²ã‚’æ´»ç”¨ã—ã€æ˜ç¢ºãªåºåˆ—ãŒå­˜åœ¨"
                    elif spread == "partial":
                        return "éƒ¨åˆ†çš„ãªé †ä½ç¯„å›²ã®åˆ©ç”¨ã§ã€ä¸€éƒ¨é›†ä¸­å‚¾å‘ã‚ã‚Š"
                    else:
                        return "é™å®šçš„ãªé †ä½ç¯„å›²ã§ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®åˆ†æ•£åº¦ãŒä½ã„"

                # å®Ÿéš›ã®å€¤ã‚’å–å¾—
                overall_stability = stability_analysis.get('overall_stability', 0)
                avg_rank_std = stability_analysis.get('avg_rank_std', 0)
                stability_interpretation = stability_analysis.get("stability_interpretation", "æœªåˆ¤å®š")

                quality_metrics = quality_analysis.get('quality_metrics', {})
                completeness_score = quality_metrics.get('completeness_score', 0)
                consistency_score = quality_metrics.get('consistency_score', 0)
                overall_quality_score = quality_analysis.get('overall_quality_score', 0)
                quality_interpretation = quality_analysis.get("quality_interpretation", "æœªåˆ¤å®š")

                competitive_balance = category_level_analysis.get("competition_analysis", {}).get("competitive_balance", "æœªåˆ¤å®š")
                ranking_spread = category_level_analysis.get("competition_analysis", {}).get("ranking_spread", "æœªè©•ä¾¡")

                # å†è¨­è¨ˆç‰ˆï¼š9é …ç›®çµ±åˆãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ5ã‚«ãƒ©ãƒ æ§‹é€ ï¼šæŒ‡æ¨™æ¦‚è¦ + åˆ†æçµæœï¼‰
                summary_data = [
                    # å®‰å®šæ€§æŒ‡æ¨™ï¼ˆ3é …ç›®ï¼‰
                    ["å…¨ä½“å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆoverall_stabilityï¼‰", f"{overall_stability:.3f}", "å®‰å®šæ€§", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å…¨ä½“çš„ãªå®‰å®šæ€§ã‚’ç¤ºã™ã‚¹ã‚³ã‚¢ï¼ˆ1.0ãŒæœ€é«˜ï¼‰", get_stability_result_explanation(overall_stability, avg_rank_std)],
                    ["å¹³å‡é †ä½æ¨™æº–åå·®ï¼ˆavg_rank_stdï¼‰", f"{avg_rank_std:.3f}", "å®‰å®šæ€§", "è¤‡æ•°å®Ÿè¡Œã«ãŠã‘ã‚‹å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é †ä½å¤‰å‹•ã®å¹³å‡å€¤", f"é †ä½å¤‰å‹•ã®å¹³å‡ãŒ{avg_rank_std:.3f}ã§ã€{'å°ã•ãªå¤‰å‹•' if avg_rank_std < 1.0 else 'ä¸­ç¨‹åº¦ã®å¤‰å‹•' if avg_rank_std < 2.0 else 'å¤§ããªå¤‰å‹•'}ã‚’ç¤ºã™"],
                    ["å®‰å®šæ€§åˆ¤å®šçµæœï¼ˆstability_interpretationï¼‰", stability_interpretation, "å®‰å®šæ€§", "å®‰å®šæ€§ã‚¹ã‚³ã‚¢ã«åŸºã¥ãå®šæ€§çš„ãªåˆ¤å®šçµæœ", f"ç·åˆçš„ãªå®‰å®šæ€§è©•ä¾¡ã¨ã—ã¦ã€Œ{stability_interpretation}ã€ã¨åˆ¤å®š"],

                    # å“è³ªæŒ‡æ¨™ï¼ˆ4é …ç›®ï¼‰
                    ["ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ï¼ˆcompleteness_scoreï¼‰", f"{completeness_score:.3f}", "å“è³ª", "ãƒ‡ãƒ¼ã‚¿ã®æ¬ æã‚„ä¸æ•´åˆãŒãªã„ç¨‹åº¦ã‚’ç¤ºã™ã‚¹ã‚³ã‚¢", f"ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãŒ{completeness_score:.1%}ã§ã€{'å„ªç§€' if completeness_score >= 0.9 else 'è‰¯å¥½' if completeness_score >= 0.8 else 'è¦æ”¹å–„'}ãªãƒ¬ãƒ™ãƒ«"],
                    ["ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ï¼ˆconsistency_scoreï¼‰", f"{consistency_score:.3f}", "å“è³ª", "ãƒ‡ãƒ¼ã‚¿ã®è«–ç†çš„æ•´åˆæ€§ã¨ä¸€è²«æ€§ã‚’ç¤ºã™ã‚¹ã‚³ã‚¢", f"ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ãŒ{consistency_score:.1%}ã§ã€{'é«˜ã„ä¸€è²«æ€§' if consistency_score >= 0.9 else 'ä¸­ç¨‹åº¦ã®ä¸€è²«æ€§' if consistency_score >= 0.7 else 'ä¸€è²«æ€§ã«èª²é¡Œ'}ã‚’ç¤ºã™"],
                    ["ç·åˆå“è³ªè©•ä¾¡ï¼ˆoverall_quality_scoreï¼‰", f"{overall_quality_score:.3f}", "å“è³ª", "å®Œå…¨æ€§ãƒ»ä¸€è²«æ€§ãƒ»ä¿¡é ¼æ€§ã‚’çµ±åˆã—ãŸç·åˆå“è³ªã‚¹ã‚³ã‚¢", get_quality_result_explanation(overall_quality_score)],
                    ["å“è³ªåˆ¤å®šçµæœï¼ˆquality_interpretationï¼‰", quality_interpretation, "å“è³ª", "å“è³ªã‚¹ã‚³ã‚¢ã«åŸºã¥ãå®šæ€§çš„ãªåˆ¤å®šçµæœ", f"ç·åˆçš„ãªå“è³ªè©•ä¾¡ã¨ã—ã¦ã€Œ{quality_interpretation}ã€ã¨åˆ¤å®š"],

                    # ç«¶äº‰æ€§æŒ‡æ¨™ï¼ˆ2é …ç›®ï¼‰
                    ["ç«¶äº‰ãƒãƒ©ãƒ³ã‚¹è©•ä¾¡ï¼ˆcompetitive_balanceï¼‰", competitive_balance, "ç«¶äº‰æ€§", "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®ç«¶äº‰ã®å‡è¡¡æ€§ã«é–¢ã™ã‚‹è©•ä¾¡", get_competitive_balance_explanation(competitive_balance)],
                    ["ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç¯„å›²ï¼ˆranking_spreadï¼‰", ranking_spread, "ç«¶äº‰æ€§", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒåˆ©ç”¨ã™ã‚‹é †ä½ç¯„å›²ã®åºƒãŒã‚Š", get_ranking_spread_explanation(ranking_spread)]
                ]

                df_summary = pd.DataFrame(summary_data, columns=["æŒ‡æ¨™å", "å€¤", "æŒ‡æ¨™ã‚«ãƒ†ã‚´ãƒª", "åˆ†æçµæœ", "æŒ‡æ¨™æ¦‚è¦"])
                st.dataframe(df_summary, use_container_width=True, hide_index=True)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šperplexity_rankingsã‹ã‚‰ç›´æ¥è¨ˆç®—ã—ã¦ä»•æ§˜æ›¸é€šã‚Šã®9é …ç›®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
                ranking_summary = subcat_data.get("ranking_summary", {})
                entities = ranking_summary.get("entities", {})
                answer_list = subcat_data.get("answer_list", [])
                execution_count = len(answer_list)

                # å®‰å®šæ€§æŒ‡æ¨™ã®è¨ˆç®—
                rank_std_values = []
                overall_stability_scores = []

                for entity_name, entity_data in entities.items():
                    all_ranks = entity_data.get("all_ranks", [])
                    if all_ranks:
                        avg_rank = sum(all_ranks) / len(all_ranks)
                        rank_std = (sum((r - avg_rank) ** 2 for r in all_ranks) / len(all_ranks)) ** 0.5 if len(all_ranks) > 1 else 0
                        rank_std_values.append(rank_std)

                        # å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆé †ä½æ¨™æº–åå·®ã®é€†æ•°ã§è¨ˆç®—ã€æ­£è¦åŒ–ï¼‰
                        stability_score = 1 / (1 + rank_std) if rank_std > 0 else 1
                        overall_stability_scores.append(stability_score)

                avg_rank_std = sum(rank_std_values) / len(rank_std_values) if rank_std_values else 0
                overall_stability = sum(overall_stability_scores) / len(overall_stability_scores) if overall_stability_scores else 1.0

                # å®‰å®šæ€§è§£é‡ˆ
                if overall_stability >= 0.8:
                    stability_interpretation = "å®‰å®š"
                elif overall_stability >= 0.6:
                    stability_interpretation = "ä¸­ç¨‹åº¦"
                else:
                    stability_interpretation = "ä¸å®‰å®š"

                # å“è³ªæŒ‡æ¨™ã®ç°¡æ˜“è¨ˆç®—
                completeness_score = len([e for e in entities.values() if e.get("all_ranks")]) / len(entities) if entities else 0
                consistency_score = 1.0 - avg_rank_std / 5.0 if avg_rank_std <= 5.0 else 0.0  # æ¨™æº–åå·®ã‹ã‚‰ä¸€è²«æ€§è¨ˆç®—
                overall_quality_score = (completeness_score + consistency_score) / 2.0

                if overall_quality_score >= 0.8:
                    quality_interpretation = "é«˜å“è³ª"
                elif overall_quality_score >= 0.6:
                    quality_interpretation = "ä¸­å“è³ª"
                else:
                    quality_interpretation = "ä½å“è³ª"

                # ç«¶äº‰æ€§æŒ‡æ¨™ã®ç°¡æ˜“è¨ˆç®—
                total_entities = len(entities)
                if total_entities >= 5:
                    competitive_balance = "é«˜"
                elif total_entities >= 3:
                    competitive_balance = "ä¸­"
                else:
                    competitive_balance = "ä½"

                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç¯„å›²ã®è¨ˆç®—
                all_positions = set()
                for entity_data in entities.values():
                    all_ranks = entity_data.get("all_ranks", [])
                    all_positions.update(all_ranks)

                if len(all_positions) >= total_entities:
                    ranking_spread = "full"
                elif len(all_positions) >= total_entities * 0.7:
                    ranking_spread = "partial"
                else:
                    ranking_spread = "limited"

                # åˆ†æçµæœè§£èª¬ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
                def get_stability_result_explanation(score, std_val):
                    if score >= 0.9:
                        return "æ¥µã‚ã¦å®‰å®šã—ãŸçµæœã§ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«é«˜ã„ä¿¡é ¼æ€§ãŒã‚ã‚‹"
                    elif score >= 0.8:
                        return "å®‰å®šã—ãŸçµæœã§ã€åˆ†æã«é©ç”¨å¯èƒ½ãªä¿¡é ¼æ€§ã‚’æŒã¤"
                    elif score >= 0.6:
                        return "ä¸­ç¨‹åº¦ã®å®‰å®šæ€§ã§ã€å‚¾å‘æŠŠæ¡ã«ã¯æœ‰åŠ¹"
                    else:
                        return "å¤‰å‹•ãŒå¤§ããã€çµæœã®è§£é‡ˆã«ã¯æ³¨æ„ãŒå¿…è¦"

                def get_quality_result_explanation(score):
                    if score >= 0.9:
                        return "éå¸¸ã«é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã§ã€ç²¾å¯†ãªåˆ†æãŒå¯èƒ½"
                    elif score >= 0.8:
                        return "é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã§ã€ä¿¡é ¼æ€§ã®é«˜ã„åˆ†æçµæœã‚’æä¾›"
                    elif score >= 0.6:
                        return "ä¸€å®šå“è³ªã®ãƒ‡ãƒ¼ã‚¿ã§ã€åŸºæœ¬çš„ãªåˆ†æã¯å¯èƒ½"
                    else:
                        return "ãƒ‡ãƒ¼ã‚¿å“è³ªã«èª²é¡ŒãŒã‚ã‚Šã€çµæœè§£é‡ˆã«æ³¨æ„ãŒå¿…è¦"

                def get_competitive_balance_explanation(balance):
                    if balance == "é«˜":
                        return "å¤šæ•°ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒç«¶äº‰ã—ã€å¸‚å ´ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯å¥½"
                    elif balance == "ä¸­":
                        return "é©åº¦ãªç«¶äº‰ç’°å¢ƒã§ã€ã‚ã‚‹ç¨‹åº¦ã®å¸‚å ´ãƒãƒ©ãƒ³ã‚¹ã‚’ä¿æŒ"
                    else:
                        return "ç«¶äº‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒå°‘ãªãã€é™å®šçš„ãªå¸‚å ´æ§‹é€ "

                def get_ranking_spread_explanation(spread):
                    if spread == "full":
                        return "å…¨é †ä½ç¯„å›²ã‚’æ´»ç”¨ã—ã€æ˜ç¢ºãªåºåˆ—ãŒå­˜åœ¨"
                    elif spread == "partial":
                        return "éƒ¨åˆ†çš„ãªé †ä½ç¯„å›²ã®åˆ©ç”¨ã§ã€ä¸€éƒ¨é›†ä¸­å‚¾å‘ã‚ã‚Š"
                    else:
                        return "é™å®šçš„ãªé †ä½ç¯„å›²ã§ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®åˆ†æ•£åº¦ãŒä½ã„"

                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®9é …ç›®çµ±åˆãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ5ã‚«ãƒ©ãƒ æ§‹é€ ï¼šæŒ‡æ¨™æ¦‚è¦ + åˆ†æçµæœï¼‰
                summary_data = [
                    # å®‰å®šæ€§æŒ‡æ¨™ï¼ˆ3é …ç›®ï¼‰
                    ["å…¨ä½“å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆoverall_stabilityï¼‰", f"{overall_stability:.3f}", "å®‰å®šæ€§", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å…¨ä½“çš„ãªå®‰å®šæ€§ã‚’ç¤ºã™ã‚¹ã‚³ã‚¢ï¼ˆ1.0ãŒæœ€é«˜ï¼‰", get_stability_result_explanation(overall_stability, avg_rank_std)],
                    ["å¹³å‡é †ä½æ¨™æº–åå·®ï¼ˆavg_rank_stdï¼‰", f"{avg_rank_std:.3f}", "å®‰å®šæ€§", "è¤‡æ•°å®Ÿè¡Œã«ãŠã‘ã‚‹å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é †ä½å¤‰å‹•ã®å¹³å‡å€¤", f"é †ä½å¤‰å‹•ã®å¹³å‡ãŒ{avg_rank_std:.3f}ã§ã€{'å°ã•ãªå¤‰å‹•' if avg_rank_std < 1.0 else 'ä¸­ç¨‹åº¦ã®å¤‰å‹•' if avg_rank_std < 2.0 else 'å¤§ããªå¤‰å‹•'}ã‚’ç¤ºã™"],
                    ["å®‰å®šæ€§åˆ¤å®šçµæœï¼ˆstability_interpretationï¼‰", stability_interpretation, "å®‰å®šæ€§", "å®‰å®šæ€§ã‚¹ã‚³ã‚¢ã«åŸºã¥ãå®šæ€§çš„ãªåˆ¤å®šçµæœ", f"ç·åˆçš„ãªå®‰å®šæ€§è©•ä¾¡ã¨ã—ã¦ã€Œ{stability_interpretation}ã€ã¨åˆ¤å®š"],

                    # å“è³ªæŒ‡æ¨™ï¼ˆ4é …ç›®ï¼‰
                    ["ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ï¼ˆcompleteness_scoreï¼‰", f"{completeness_score:.3f}", "å“è³ª", "ãƒ‡ãƒ¼ã‚¿ã®æ¬ æã‚„ä¸æ•´åˆãŒãªã„ç¨‹åº¦ã‚’ç¤ºã™ã‚¹ã‚³ã‚¢", f"ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãŒ{completeness_score:.1%}ã§ã€{'å„ªç§€' if completeness_score >= 0.9 else 'è‰¯å¥½' if completeness_score >= 0.8 else 'è¦æ”¹å–„'}ãªãƒ¬ãƒ™ãƒ«"],
                    ["ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ï¼ˆconsistency_scoreï¼‰", f"{consistency_score:.3f}", "å“è³ª", "ãƒ‡ãƒ¼ã‚¿ã®è«–ç†çš„æ•´åˆæ€§ã¨ä¸€è²«æ€§ã‚’ç¤ºã™ã‚¹ã‚³ã‚¢", f"ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ãŒ{consistency_score:.1%}ã§ã€{'é«˜ã„ä¸€è²«æ€§' if consistency_score >= 0.9 else 'ä¸­ç¨‹åº¦ã®ä¸€è²«æ€§' if consistency_score >= 0.7 else 'ä¸€è²«æ€§ã«èª²é¡Œ'}ã‚’ç¤ºã™"],
                    ["ç·åˆå“è³ªè©•ä¾¡ï¼ˆoverall_quality_scoreï¼‰", f"{overall_quality_score:.3f}", "å“è³ª", "å®Œå…¨æ€§ãƒ»ä¸€è²«æ€§ãƒ»ä¿¡é ¼æ€§ã‚’çµ±åˆã—ãŸç·åˆå“è³ªã‚¹ã‚³ã‚¢", get_quality_result_explanation(overall_quality_score)],
                    ["å“è³ªåˆ¤å®šçµæœï¼ˆquality_interpretationï¼‰", quality_interpretation, "å“è³ª", "å“è³ªã‚¹ã‚³ã‚¢ã«åŸºã¥ãå®šæ€§çš„ãªåˆ¤å®šçµæœ", f"ç·åˆçš„ãªå“è³ªè©•ä¾¡ã¨ã—ã¦ã€Œ{quality_interpretation}ã€ã¨åˆ¤å®š"],

                    # ç«¶äº‰æ€§æŒ‡æ¨™ï¼ˆ2é …ç›®ï¼‰
                    ["ç«¶äº‰ãƒãƒ©ãƒ³ã‚¹è©•ä¾¡ï¼ˆcompetitive_balanceï¼‰", competitive_balance, "ç«¶äº‰æ€§", "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®ç«¶äº‰ã®å‡è¡¡æ€§ã«é–¢ã™ã‚‹è©•ä¾¡", get_competitive_balance_explanation(competitive_balance)],
                    ["ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç¯„å›²ï¼ˆranking_spreadï¼‰", ranking_spread, "ç«¶äº‰æ€§", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒåˆ©ç”¨ã™ã‚‹é †ä½ç¯„å›²ã®åºƒãŒã‚Š", get_ranking_spread_explanation(ranking_spread)]
                ]

                df_summary = pd.DataFrame(summary_data, columns=["æŒ‡æ¨™å", "å€¤", "æŒ‡æ¨™ã‚«ãƒ†ã‚´ãƒª", "æŒ‡æ¨™æ¦‚è¦", "åˆ†æçµæœ"])
                st.dataframe(df_summary, use_container_width=True, hide_index=True)



            # === 3. ã‚¿ãƒ–åˆ¥ã‚°ãƒ©ãƒ•è¡¨ç¤º ===
            st.subheader("ğŸ“ˆ è©³ç´°ã‚°ãƒ©ãƒ•åˆ†æ")

            # å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from src.utils.plot_utils import (
                plot_ranking_similarity_for_ranking_analysis,
                plot_bias_indices_bar_for_ranking_analysis,
                plot_ranking_variation_heatmap,
                plot_entity_stability_analysis
            )

            # ã‚¿ãƒ–ä½œæˆ
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™æ£’ã‚°ãƒ©ãƒ•", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¤‰å‹•ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
                "ãƒ©ãƒ³ã‚­ãƒ³ã‚°å®‰å®šæ€§åˆ†æ", "å®‰å®šæ€§ã‚¹ã‚³ã‚¢åˆ†å¸ƒ", "æ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢æ•£å¸ƒå›³"
            ])

            with tab1:
                st.markdown("**ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™æ£’ã‚°ãƒ©ãƒ•**")
                st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤ºã—ã¾ã™ã€‚\n\n"
                       "ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã¯ã€å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ä»–ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ã®é †ä½å·®ã®å¹³å‡å€¤ã¨ã—ã¦è¨ˆç®—ã•ã‚Œã¾ã™ã€‚\n"
                       "ä¾‹ï¼šAWSãŒä»–ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚ˆã‚Šå¹³å‡2.5ä½ä¸Šä½ã«ã„ã‚‹å ´åˆã€ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã¯+2.5ã¨ãªã‚Šã¾ã™ã€‚\n\n"
                       "ãƒ»æ­£ã®å€¤ï¼ˆèµ¤ï¼‰ï¼šä»–ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚ˆã‚Šå¹³å‡çš„ã«ä¸Šä½ã«ä½ç½®ã™ã‚‹å‚¾å‘\n"
                       "ãƒ»è² ã®å€¤ï¼ˆç·‘ï¼‰ï¼šä»–ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚ˆã‚Šå¹³å‡çš„ã«ä¸‹ä½ã«ä½ç½®ã™ã‚‹å‚¾å‘\n"
                       "ãƒ»å€¤ã®å¤§ãã•ï¼šé †ä½å·®ã®å¤§ãã•ã‚’ç¤ºã™ï¼ˆçµ¶å¯¾å€¤ãŒå¤§ãã„ã»ã©ã€ä»–ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ã®é †ä½å·®ãŒå¤§ãã„ï¼‰", icon="â„¹ï¸")

                if ranking_bias_data and selected_category in ranking_bias_data and selected_subcategory in ranking_bias_data[selected_category]:
                    try:
                        fig = plot_bias_indices_bar_for_ranking_analysis(ranking_bias_data, selected_category, selected_subcategory, selected_entities)
                        if fig:
                            st.pyplot(fig, use_container_width=True)
                            plt.close(fig)
                        else:
                            st.info("ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    except Exception as e:
                        st.error(f"ã‚°ãƒ©ãƒ•æç”»ã‚¨ãƒ©ãƒ¼: {str(e)}")
                else:
                    st.warning("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

            with tab2:
                st.markdown("**ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¤‰å‹•ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—**")
                st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é †ä½å¤‰å‹•ã‚’å®Ÿè¡Œå›æ•°ã”ã¨ã«ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§è¡¨ç¤ºã—ã¾ã™ã€‚\n"
                       "è‰²ãŒæ¿ƒã„ã»ã©é †ä½ãŒä½ãã€è–„ã„ã»ã©é †ä½ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚", icon="â„¹ï¸")

                if ranking_bias_data and selected_category in ranking_bias_data and selected_subcategory in ranking_bias_data[selected_category]:
                    try:
                        subcat_data = ranking_bias_data[selected_category][selected_subcategory]
                        entities_data = subcat_data.get("entities", {})

                        fig = plot_ranking_variation_heatmap(entities_data, selected_entities)
                        if fig:
                            st.pyplot(fig, use_container_width=True)
                            plt.close(fig)
                        else:
                            st.info("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    except Exception as e:
                        st.error(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ã‚¨ãƒ©ãƒ¼: {str(e)}")
                else:
                    st.warning("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

            with tab3:
                st.markdown("**ãƒ©ãƒ³ã‚­ãƒ³ã‚°å®‰å®šæ€§åˆ†æ**")
                st.info("å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°å®‰å®šæ€§ã‚’åˆ†æã—ã¾ã™ã€‚\n\n"
                       "ãƒ»é †ä½æ¨™æº–åå·®ï¼šå®Ÿè¡Œå›æ•°é–“ã§ã®é †ä½ã®ã°ã‚‰ã¤ãï¼ˆ0ã«è¿‘ã„ã»ã©å®‰å®šï¼‰\n"
                       "ãƒ»é †ä½ç¯„å›²ï¼šå®Ÿè¡Œå›æ•°é–“ã§ã®é †ä½ã®å¤‰å‹•å¹…ï¼ˆ0ã«è¿‘ã„ã»ã©å®‰å®šï¼‰\n"
                       "ãƒ»å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼šå…¨ä½“çš„ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å®‰å®šæ€§ï¼ˆ1ã«è¿‘ã„ã»ã©å®‰å®šï¼‰", icon="â„¹ï¸")

                if ranking_bias_data and selected_category in ranking_bias_data and selected_subcategory in ranking_bias_data[selected_category]:
                    try:
                        subcat_data = ranking_bias_data[selected_category][selected_subcategory]
                        stability_analysis = subcat_data.get("category_summary", {}).get("stability_analysis", {})

                        if stability_analysis.get("available", False):
                            # å®‰å®šæ€§åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                            st.subheader("å…¨ä½“å®‰å®šæ€§")
                            col1, col2, col3 = st.columns(3)

                            with col1:
                                st.metric("å…¨ä½“å®‰å®šæ€§ã‚¹ã‚³ã‚¢", f"{stability_analysis.get('overall_stability', 0):.3f}")
                            with col2:
                                st.metric("å®Ÿè¡Œå›æ•°", stability_analysis.get("execution_count", 0))
                            with col3:
                                st.metric("å¹³å‡é †ä½æ¨™æº–åå·®", f"{stability_analysis.get('avg_rank_std', 0):.3f}")

                            st.write(f"**è§£é‡ˆ**: {stability_analysis.get('stability_interpretation', 'N/A')}")

                            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åˆ¥å®‰å®šæ€§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                            rank_variance = stability_analysis.get("rank_variance", {})
                            if rank_variance:
                                st.subheader("ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åˆ¥å®‰å®šæ€§åˆ†æ")

                                # ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
                                fig = plot_entity_stability_analysis(rank_variance)
                                if fig:
                                    st.pyplot(fig, use_container_width=True)
                                    plt.close(fig)
                                else:
                                    st.info("å®‰å®šæ€§åˆ†æã‚°ãƒ©ãƒ•ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

                                # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚è¡¨ç¤ºï¼ˆæŠ˜ã‚ŠãŸãŸã¿å¯èƒ½ï¼‰
                                with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«"):
                                    stability_df = []
                                    for entity, data in rank_variance.items():
                                        stability_df.append({
                                            "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£": entity,
                                            "å¹³å‡é †ä½": f"{data.get('mean_rank', 0):.2f}",
                                            "é †ä½æ¨™æº–åå·®": f"{data.get('rank_std', 0):.3f}",
                                            "é †ä½ç¯„å›²": f"{data.get('rank_range', 0):.1f}"
                                        })

                                    if stability_df:
                                        df = pd.DataFrame(stability_df)
                                        st.dataframe(df, use_container_width=True)
                        else:
                            st.info("å®‰å®šæ€§åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    except Exception as e:
                        st.error(f"å®‰å®šæ€§åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
                else:
                    st.warning("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

            with tab4:
                st.markdown("**å®‰å®šæ€§ã‚¹ã‚³ã‚¢åˆ†å¸ƒ**")
                st.info("å…¨ã‚«ãƒ†ã‚´ãƒªã®å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆè¤‡æ•°å›ã®åˆ†æã§é †ä½ãŒã©ã‚Œã ã‘å¤‰ã‚ã‚‰ãªã„ã‹ã‚’ç¤ºã™æŒ‡æ¨™ï¼‰ã‚’æ•£å¸ƒå›³ã§è¡¨ç¤ºã—ã¾ã™ï¼š\n\n"
                       "ãƒ»Xè»¸ï¼šå®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€1ã«è¿‘ã„ã»ã©é †ä½ãŒå¤‰å‹•ã—ãªã„ï¼‰\n"
                       "ãƒ»Yè»¸ï¼šé †ä½æ¨™æº–åå·®ï¼ˆ0ã«è¿‘ã„ã»ã©é †ä½å¤‰å‹•ãŒå°ã•ã„ï¼‰\n", icon="â„¹ï¸")

                # å…¨åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç¾åœ¨ã®ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
                ranking_bias_data = analysis_data.get("ranking_bias_analysis", {})
                fig = plot_stability_score_distribution(ranking_bias_data, selected_category, selected_subcategory)
                if fig:
                    st.pyplot(fig, use_container_width=True)
                    plt.close(fig)
                else:
                    st.info("å®‰å®šæ€§ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            with tab5:
                st.markdown("**æ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢æ•£å¸ƒå›³**")
                st.info("æ„Ÿæƒ…åˆ†æã®æ­£è¦åŒ–ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ï¼ˆNBIï¼‰ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°é †ä½ã®ç›¸é–¢ã‚’æ•£å¸ƒå›³ã§è¡¨ç¤ºã—ã¾ã™ã€‚\n\n"
                       "ãƒ»Xè»¸ï¼šæ­£è¦åŒ–ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ï¼ˆæ„Ÿæƒ…åˆ†æã§ã®å„ªé‡åº¦ï¼‰\n"
                       "ãƒ»Yè»¸ï¼šé€†è»¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°é †ä½ï¼ˆæ•°å€¤ãŒå¤§ãã„ã»ã©ä¸Šä½ï¼‰\n"
                       "ãƒ»å›å¸°ç›´ç·šï¼šç›¸é–¢ã®æ–¹å‘æ€§ã¨å¼·åº¦ã‚’è¡¨ç¤º\n"
                       "ãƒ»æ­£ã®ç›¸é–¢ï¼šæ„Ÿæƒ…åˆ†æã§å¥½æ„çš„ãªä¼æ¥­ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã‚‚ä¸Šä½\n"
                       "ãƒ»è² ã®ç›¸é–¢ï¼šæ„Ÿæƒ…åˆ†æã§å¥½æ„çš„ãªä¼æ¥­ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã¯ä¸‹ä½", icon="â„¹ï¸")

                                # æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆdashboard_dataã‹ã‚‰ç›´æ¥å–å¾—ï¼‰
                analysis_data = dashboard_data.get("analysis_results", {})
                sentiment_analysis = analysis_data.get("sentiment_bias_analysis", {})

                if (sentiment_analysis and selected_category in sentiment_analysis and
                    selected_subcategory in sentiment_analysis[selected_category]):

                    sentiment_entities = sentiment_analysis[selected_category][selected_subcategory].get("entities", {})
                    ranking_entities = entities  # perplexity_rankingsã‹ã‚‰å–å¾—æ¸ˆã¿

                    if sentiment_entities and ranking_entities:
                        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                        sentiment_data = {}
                        ranking_data = {}

                        for entity, entity_data in sentiment_entities.items():
                            if entity in ranking_entities:
                                bias_index = entity_data.get("basic_metrics", {}).get("normalized_bias_index", 0)
                                avg_rank = ranking_entities[entity].get("avg_rank", 0)

                                if bias_index is not None and avg_rank is not None:
                                    sentiment_data[entity] = {"normalized_bias_index": bias_index}
                                    ranking_data[entity] = {"avg_rank": avg_rank}

                        if len(sentiment_data) >= 2:
                            try:
                                fig = plot_sentiment_ranking_correlation_scatter(
                                    sentiment_data,
                                    ranking_data,
                                    title=f"{selected_category}/{selected_subcategory} æ„Ÿæƒ…-ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç›¸é–¢"
                                )
                                st.plotly_chart(fig, use_container_width=True)
                            except Exception as e:
                                st.error(f"æ•£å¸ƒå›³æç”»ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        else:
                            st.info("æ•£å¸ƒå›³æç”»ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½2ã¤ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒå¿…è¦ï¼‰")
                    else:
                        st.info("æ„Ÿæƒ…åˆ†æã¾ãŸã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                else:
                    st.info("æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

        else:
            st.info("perplexity_rankingsãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    # æ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼é–‰ã˜ã‚¿ã‚°
    st.markdown("</div>", unsafe_allow_html=True)

# --- CSSèª¿æ•´ ---
# ï¼ˆmain-dashboard-areaã‚„block-containerç­‰ã®ã‚«ã‚¹ã‚¿ãƒ CSSãƒ»JSã¯å‰Šé™¤ï¼‰