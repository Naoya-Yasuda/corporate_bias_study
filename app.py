#!/usr/bin/env python
# coding: utf-8

"""
ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æ - ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

Streamlitã‚’ä½¿ç”¨ã—ã¦ã€ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æã®çµæœãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã™ã€‚
"""

import os
import json
import glob
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime
import matplotlib as mpl
import matplotlib.font_manager as fm
from dotenv import load_dotenv
from src.utils.storage_utils import get_s3_client, S3_BUCKET_NAME, get_latest_file, load_json
from src.analysis.ranking_metrics import get_exposure_market_data, compute_rank_metrics, MARKET_SHARES, get_timeseries_exposure_market_data
import importlib
serp_metrics = importlib.import_module('src.analysis.serp_metrics')

# åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’å„ªå…ˆçš„ã«å–å¾—
import matplotlib.pyplot as plt
import japanize_matplotlib

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ã®ãƒ‘ã‚¹ï¼ˆW3ã‚’ä¾‹ã«ï¼‰
font_path = '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc'
prop = fm.FontProperties(fname=font_path)
plt.rcParams['font.family'] = prop.get_name()
plt.rcParams['font.sans-serif'] = [prop.get_name()]
plt.rcParams['axes.unicode_minus'] = False

# ã‚°ãƒ©ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
def set_plot_style():
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams['figure.figsize'] = (6, 3)  # ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å°ã•ãè¨­å®š
    plt.rcParams['font.size'] = 9
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['axes.titlesize'] = 12
    plt.rcParams['xtick.labelsize'] = 8
    plt.rcParams['ytick.labelsize'] = 8
    plt.rcParams['legend.fontsize'] = 8
    plt.rcParams['figure.titlesize'] = 14
    plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã‚’æ­£ã—ãè¡¨ç¤º

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®šã‚’é©ç”¨
set_plot_style()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

def get_data_files():
    """S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSON/CSVï¼‰ã®ã¿ã‚’å–å¾—"""
    try:
        s3_client = get_s3_client()
        prefix = 'results/'
        response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)

        if 'Contents' not in response:
            st.warning("S3ãƒã‚±ãƒƒãƒˆã«ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        files = []
        for content in response['Contents']:
            file_path = content['Key']
            file_name = os.path.basename(file_path)

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡
            if not (file_name.endswith('.json') or file_name.endswith('.csv')):
                continue

            # æ—¥ä»˜ã‚’æŠ½å‡º (YYYYMMDDå½¢å¼)
            date_match = next((part for part in file_name.split('_') if len(part) == 8 and part.isdigit()), None)

            if date_match:
                try:
                    date_obj = datetime.strptime(date_match, "%Y%m%d")
                    date_str = date_obj.strftime("%Y-%m-%d")
                except:
                    date_str = "ä¸æ˜"
            else:
                date_str = "ä¸æ˜"

            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’åˆ¤æ–­
            if "perplexity_sentiment" in file_name:
                data_type = "Perplexity æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "perplexity_results" in file_name and "/sentiment/" in file_path:
                data_type = "Perplexity æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "perplexity_rankings" in file_name:
                data_type = "Perplexity ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
            elif "perplexity_results" in file_name and "/rankings/" in file_path:
                data_type = "Perplexity ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
            elif "perplexity_citations" in file_name:
                data_type = "Perplexity å¼•ç”¨ãƒªãƒ³ã‚¯"
            elif "perplexity_results" in file_name and "/citations/" in file_path:
                data_type = "Perplexity å¼•ç”¨ãƒªãƒ³ã‚¯"
            elif "openai_sentiment" in file_name:
                data_type = "OpenAI æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            elif "openai_results" in file_name:
                data_type = "OpenAI æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
            else:
                data_type = "ãã®ä»–"

            # è¤‡æ•°å›å®Ÿè¡Œã‹ã©ã†ã‹
            import re
            m = re.search(r'_(\d+)runs\.json', file_name)
            if m:
                runs_count = int(m.group(1))
                is_multi_run = True
                runs_suffix = f"ï¼ˆ{runs_count}å›å®Ÿè¡Œï¼‰"
            elif "_runs.json" in file_name:
                is_multi_run = False
                runs_suffix = "ï¼ˆå˜ä¸€å®Ÿè¡Œï¼‰"
            else:
                is_multi_run = False
                runs_suffix = "ï¼ˆå˜ä¸€å®Ÿè¡Œï¼‰"

            # è¡¨ç¤ºåã‚’ç”Ÿæˆ
            display_name = f"{date_str}: {data_type}{runs_suffix}"

            files.append({
                "path": f"s3://{S3_BUCKET_NAME}/{file_path}",
                "name": file_name,
                "date": date_str,
                "date_obj": date_obj if date_match else datetime.now(),
                "type": data_type,
                "is_multi_run": is_multi_run,
                "display_name": display_name,
                "date_raw": date_match
            })

        # æ—¥ä»˜ã®æ–°ã—ã„é †ï¼‹åŒæ—¥ä»˜ãªã‚‰å®Ÿè¡Œå›æ•°å¤šã„é †ã«ã‚½ãƒ¼ãƒˆ
        def extract_runs(file):
            m = re.search(r'_(\d+)runs\.json', file["name"])
            return int(m.group(1)) if m else 1
        files.sort(key=lambda x: (x["date_obj"], extract_runs(x)), reverse=True)
        return files

    except Exception as e:
        st.error(f"S3ã‹ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_image_files():
    """S3ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPNGï¼‰ã®ã¿ã‚’å–å¾—"""
    try:
        s3_client = get_s3_client()
        prefix = 'results/perplexity_analysis/'
        response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)

        if 'Contents' not in response:
            st.warning("S3ãƒã‚±ãƒƒãƒˆã«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        files = []
        for content in response['Contents']:
            file_path = content['Key']
            file_name = os.path.basename(file_path)

            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡
            if not file_name.endswith('.png'):
                continue

            # æ—¥ä»˜ã‚’æŠ½å‡º (YYYYMMDDå½¢å¼)
            date_match = next((part for part in file_path.split('/') if len(part) == 8 and part.isdigit()), None)

            if date_match:
                try:
                    date_obj = datetime.strptime(date_match, "%Y%m%d")
                    date_str = date_obj.strftime("%Y-%m-%d")
                except:
                    date_str = "ä¸æ˜"
            else:
                date_str = "ä¸æ˜"

            # ç”»åƒã‚¿ã‚¤ãƒ—ã‚’åˆ¤æ–­
            if "_exposure_market.png" in file_name:
                image_type = "éœ²å‡ºåº¦ãƒ»å¸‚å ´ã‚·ã‚§ã‚¢"
            elif "_rank_heatmap.png" in file_name:
                image_type = "ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†å¸ƒ"
            elif "_stability_matrix.png" in file_name:
                image_type = "å®‰å®šæ€§è¡Œåˆ—"
            else:
                image_type = "ãã®ä»–"

            # è¡¨ç¤ºåã‚’ç”Ÿæˆ
            display_name = f"{date_str}: {image_type} - {file_name}"

            files.append({
                "path": f"s3://{S3_BUCKET_NAME}/{file_path}",
                "name": file_name,
                "date": date_str,
                "date_obj": date_obj if date_match else datetime.now(),
                "type": image_type,
                "display_name": display_name,
                "date_raw": date_match
            })

        # æ—¥ä»˜ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        files.sort(key=lambda x: x["date_obj"], reverse=True)
        return files

    except Exception as e:
        st.error(f"S3ã‹ã‚‰ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# CSSã§ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’è¿½åŠ 
st.markdown("""
<style>
    body {
        font-family: 'Hiragino Sans', 'Meiryo', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ä¼æ¥­ãƒã‚¤ã‚¢ã‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
st.markdown("AIæ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã«ãŠã‘ã‚‹ä¼æ¥­å„ªé‡ãƒã‚¤ã‚¢ã‚¹ã®å¯è¦–åŒ–")

# --- åˆ†æã‚µãƒãƒªãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
st.header("åˆ†æã‚µãƒãƒªãƒ¼ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼‰")

def get_latest_integrated_dir():
    """corporate_bias_datasets/integrated/é…ä¸‹ã®æœ€æ–°æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿”ã™"""
    base_dir = "corporate_bias_datasets/integrated"
    dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.isdigit()]
    if not dirs:
        return None
    latest = sorted(dirs, reverse=True)[0]
    return os.path.join(base_dir, latest)

def get_bias_summary():
    """æœ€æ–°ã®bias_analysis_results.jsonã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä¸»è¦ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ã‚’æŠ½å‡ºã—DataFrameã§è¿”ã™"""
    latest_dir = get_latest_integrated_dir()
    if not latest_dir:
        return None, "ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    bias_path = os.path.join(latest_dir, "bias_analysis_results.json")
    if not os.path.exists(bias_path):
        return None, f"{bias_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    with open(bias_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    results = []
    sentiment = data.get("sentiment_bias_analysis", {})
    for category, subcats in sentiment.items():
        for subcat, subdata in subcats.items():
            entities = subdata.get("entities", {})
            for key, ent in entities.items():
                metrics = ent.get("basic_metrics", {})
                interp = ent.get("interpretation", {})
                stability = ent.get("stability_metrics", {})
                stat = ent.get("statistical_significance", {})
                results.append({
                    "ã‚«ãƒ†ã‚´ãƒª": category,
                    "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª": subcat,
                    "åˆ†æå˜ä½": key,
                    "ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™": metrics.get("normalized_bias_index"),
                    "ç”Ÿå·®åˆ†": metrics.get("raw_delta"),
                    "å®‰å®šæ€§": stability.get("stability_score"),
                    "ãƒã‚¤ã‚¢ã‚¹æ–¹å‘": interp.get("bias_direction"),
                    "ãƒã‚¤ã‚¢ã‚¹å¼·åº¦": interp.get("bias_strength"),
                    "æœ‰æ„æ€§": stat.get("available"),
                    "æ¨å¥¨": interp.get("recommendation")
                })
    df = pd.DataFrame(results)
    return df, None

df, err = get_bias_summary()
if err:
    st.warning(err)
else:
    # ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    st.subheader("ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé™é †ï¼‰")
    top = df.sort_values("ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™", ascending=False).head(20)
    st.dataframe(top)
    # ãƒã‚¤ã‚¢ã‚¹æ–¹å‘ãƒ»å¼·åº¦ã®åˆ†å¸ƒ
    st.subheader("ãƒã‚¤ã‚¢ã‚¹æ–¹å‘ãƒ»å¼·åº¦ã®åˆ†å¸ƒ")
    bias_dir_counts = df["ãƒã‚¤ã‚¢ã‚¹æ–¹å‘"].value_counts()
    bias_strength_counts = df["ãƒã‚¤ã‚¢ã‚¹å¼·åº¦"].value_counts()
    col1, col2 = st.columns(2)
    with col1:
        st.bar_chart(bias_dir_counts)
    with col2:
        st.bar_chart(bias_strength_counts)
    # æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    st.subheader("æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ï¼ˆp<0.05ç›¸å½“ï¼‰")
    sig = df[df["æœ‰æ„æ€§"] == True]
    if not sig.empty:
        st.dataframe(sig)
    else:
        st.info("æœ‰æ„ãªãƒã‚¤ã‚¢ã‚¹ã¯æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“")

# --- ãƒ‡ãƒ¼ã‚¿åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
st.header("è©³ç´°åˆ†æï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ»ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªå˜ä½ï¼‰")

def get_latest_dataset_path():
    latest_dir = get_latest_integrated_dir()
    if not latest_dir:
        return None
    dataset_path = os.path.join(latest_dir, "corporate_bias_dataset.json")
    if not os.path.exists(dataset_path):
        return None
    return dataset_path

dataset_path = get_latest_dataset_path()
if not dataset_path:
    st.warning("corporate_bias_dataset.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
else:
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    categories = list(dataset.keys())
    selected_category = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", categories)
    subcategories = list(dataset[selected_category].keys())
    selected_subcategory = st.selectbox("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", subcategories)
    subcat_data = dataset[selected_category][selected_subcategory]
    st.subheader(f"{selected_category} - {selected_subcategory} ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿")
    st.json(subcat_data)

# --- ç”»åƒæŒ‡æ¨™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
st.header("ç”»åƒæŒ‡æ¨™ï¼ˆintegratedä»•æ§˜ï¼‰")

def get_latest_visuals_dir():
    # integratedã®æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨åŒã˜æ—¥ä»˜ã®analysis_visuals/é…ä¸‹ã‚’å‚ç…§
    latest_dir = get_latest_integrated_dir()
    if not latest_dir:
        return None
    date_str = os.path.basename(latest_dir)
    visuals_dir = os.path.join("corporate_bias_datasets/analysis_visuals", date_str)
    if not os.path.exists(visuals_dir):
        return None
    return visuals_dir

visuals_dir = get_latest_visuals_dir()
if not visuals_dir:
    st.warning("ç”»åƒæŒ‡æ¨™ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
else:
    # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨ã«ç”»åƒã‚’è¡¨ç¤º
    for subdir in os.listdir(visuals_dir):
        subdir_path = os.path.join(visuals_dir, subdir)
        if os.path.isdir(subdir_path):
            st.subheader(f"{subdir} ã®ç”»åƒæŒ‡æ¨™")
            images = [f for f in os.listdir(subdir_path) if f.lower().endswith(".png")]
            for img in images:
                img_path = os.path.join(subdir_path, img)
                st.image(img_path, caption=img)