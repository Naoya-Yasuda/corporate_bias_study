#!/usr/bin/env python
# coding: utf-8

"""
SNSæŠ•ç¨¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒã‚¤ã‚¢ã‚¹å¤‰åŒ–ç›£è¦–ã¨SNSæŠ•ç¨¿æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import os
import sys
import logging
import yaml
from datetime import datetime, timedelta
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.sns import BiasMonitor, S3DataLoader, ContentGenerator, PostingManager
from src.analysis.hybrid_data_loader import HybridDataLoader

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/test_sns_posting.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    config_path = project_root / "config" / "sns_monitoring_config.yml"

    if not config_path.exists():
        logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        return None

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    return config.get("sns_monitoring", {})


def create_test_data():
    """ãƒ†ã‚¹ãƒˆç”¨ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    # ç¾åœ¨ã®æ—¥ä»˜
    current_date = datetime.now().strftime("%Y%m%d")

    # å…ˆé€±ã®æ—¥ä»˜
    last_week = (datetime.now() - timedelta(days=7)).strftime("%Y%m%d")

    # ãƒ†ã‚¹ãƒˆç”¨ã®åˆ†æãƒ‡ãƒ¼ã‚¿
    current_data = {
        "metadata": {
            "analysis_date": current_date,
            "execution_count": 5,
            "reliability_level": "å®Ÿç”¨åˆ†æ"
        },
        "categories": {
            "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹": {
                "service_level_fairness_score": 0.85,
                "enterprise_level_fairness_score": 0.78,
                "subcategories": {
                    "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°": {
                        "entities": [
                            {
                                "name": "AWS",
                                "category": "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹",
                                "normalized_bias_index": 1.8,  # å¤§å¹…ã«ä¸Šæ˜‡
                                "sign_test_p_value": 0.02,
                                "cliffs_delta": 0.45,
                                "google_rank": 1,
                                "perplexity_rank": 2
                            },
                            {
                                "name": "Azure",
                                "category": "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹",
                                "normalized_bias_index": 0.5,
                                "sign_test_p_value": 0.15,
                                "cliffs_delta": 0.25,
                                "google_rank": 3,
                                "perplexity_rank": 1
                            }
                        ]
                    }
                }
            }
        }
    }

    # å…ˆé€±ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤‰åŒ–ã‚’æ¤œçŸ¥ã™ã‚‹ãŸã‚ã®æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ï¼‰
    previous_data = {
        "metadata": {
            "analysis_date": last_week,
            "execution_count": 5,
            "reliability_level": "å®Ÿç”¨åˆ†æ"
        },
        "categories": {
            "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹": {
                "service_level_fairness_score": 0.95,  # å¤§å¹…ã«ä½ä¸‹
                "enterprise_level_fairness_score": 0.88,
                "subcategories": {
                    "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°": {
                        "entities": [
                            {
                                "name": "AWS",
                                "category": "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹",
                                "normalized_bias_index": 1.2,  # å¤‰åŒ–ã‚ã‚Š
                                "sign_test_p_value": 0.03,
                                "cliffs_delta": 0.38,
                                "google_rank": 2,  # é †ä½å¤‰åŒ–ã‚ã‚Š
                                "perplexity_rank": 3
                            },
                            {
                                "name": "Azure",
                                "category": "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹",
                                "normalized_bias_index": 0.5,
                                "sign_test_p_value": 0.15,
                                "cliffs_delta": 0.25,
                                "google_rank": 3,
                                "perplexity_rank": 1
                            }
                        ]
                    }
                }
            }
        }
    }

    return current_data, previous_data


def test_s3_data_loader():
    """S3DataLoaderã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== S3DataLoaderãƒ†ã‚¹ãƒˆé–‹å§‹ ===")

    try:
        loader = S3DataLoader()

        # åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã®å–å¾—ãƒ†ã‚¹ãƒˆ
        start_date = (datetime.now() - timedelta(days=30)).strftime("%Y%m%d")
        end_date = datetime.now().strftime("%Y%m%d")
        available_dates = loader.get_available_dates(start_date, end_date)

        logger.info(f"åˆ©ç”¨å¯èƒ½æ—¥ä»˜æ•°: {len(available_dates)}")

        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
        test_data, _ = create_test_data()
        is_valid = loader.validate_data_structure(test_data)
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æ§‹é€ æ¤œè¨¼çµæœ: {is_valid}")

        logger.info("S3DataLoaderãƒ†ã‚¹ãƒˆå®Œäº†")
        return True

    except Exception as e:
        logger.error(f"S3DataLoaderãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_content_generator():
    """ContentGeneratorã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== ContentGeneratorãƒ†ã‚¹ãƒˆé–‹å§‹ ===")

    try:
        generator = ContentGenerator()

        # ãƒ†ã‚¹ãƒˆç”¨ã®å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿
        test_change = {
            "type": "nbi_change",
            "entity": "AWS",
            "category": "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹",
            "change_rate": 25.5,
            "current_value": 1.8,
            "previous_value": 1.2,
            "direction": "up",
            "p_value": 0.02,
            "cliffs_delta": 0.45
        }

        current_date = datetime.now().strftime("%Y%m%d")

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        content = generator.generate_content(test_change, current_date)
        logger.info(f"ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„:\n{content}")

        # ã‚µãƒãƒªãƒ¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        changes = [test_change]
        summary_content = generator.generate_summary_content(changes, current_date)
        logger.info(f"ç”Ÿæˆã•ã‚ŒãŸã‚µãƒãƒªãƒ¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„:\n{summary_content}")

        logger.info("ContentGeneratorãƒ†ã‚¹ãƒˆå®Œäº†")
        return True

    except Exception as e:
        logger.error(f"ContentGeneratorãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_posting_manager():
    """PostingManagerã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== PostingManagerãƒ†ã‚¹ãƒˆé–‹å§‹ ===")

    try:
        manager = PostingManager()

        # æ—¥æ¬¡åˆ¶é™ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        can_post = manager.check_daily_limit()
        logger.info(f"æ—¥æ¬¡åˆ¶é™ãƒã‚§ãƒƒã‚¯çµæœ: {can_post}")

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        is_duplicate = manager.check_duplicate("AWS", "nbi_change")
        logger.info(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ: {is_duplicate}")

        # æŠ•ç¨¿å±¥æ­´å–å¾—ãƒ†ã‚¹ãƒˆ
        history = manager.get_post_history(days=7)
        logger.info(f"æŠ•ç¨¿å±¥æ­´ä»¶æ•°: {len(history)}")

        # æ—¥æ¬¡çµ±è¨ˆå–å¾—ãƒ†ã‚¹ãƒˆ
        stats = manager.get_daily_stats()
        logger.info(f"æ—¥æ¬¡çµ±è¨ˆ: {stats}")

        logger.info("PostingManagerãƒ†ã‚¹ãƒˆå®Œäº†")
        return True

    except Exception as e:
        logger.error(f"PostingManagerãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_bias_monitor():
    """BiasMonitorã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== BiasMonitorãƒ†ã‚¹ãƒˆé–‹å§‹ ===")

    try:
        # è¨­å®šã‚’èª­ã¿è¾¼ã¿
        config = load_config()
        if not config:
            logger.error("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        current_data, previous_data = create_test_data()

        # BiasMonitorã‚’åˆæœŸåŒ–
        monitor = BiasMonitor(config)

        # ç›´å‰ãƒ‡ãƒ¼ã‚¿ã‚’S3DataLoaderã«è¨­å®šï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        monitor.s3_loader._test_previous_data = previous_data

        # å¤‰åŒ–ç›£è¦–ãƒ†ã‚¹ãƒˆ
        current_date = datetime.now().strftime("%Y%m%d")
        changes = monitor.monitor_changes(current_date, current_data)

        logger.info(f"æ¤œçŸ¥ã•ã‚ŒãŸå¤‰åŒ–æ•°: {len(changes)}")
        for i, change in enumerate(changes):
            logger.info(f"å¤‰åŒ– {i+1}: {change}")

        logger.info("BiasMonitorãƒ†ã‚¹ãƒˆå®Œäº†")
        return True

    except Exception as e:
        logger.error(f"BiasMonitorãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_integration():
    """çµ±åˆãƒ†ã‚¹ãƒˆ"""
    logger.info("=== çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ ===")

    try:
        # è¨­å®šã‚’èª­ã¿è¾¼ã¿
        config = load_config()
        if not config:
            logger.error("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        current_data, previous_data = create_test_data()

        # BiasMonitorã‚’åˆæœŸåŒ–
        monitor = BiasMonitor(config)

        # ç›´å‰ãƒ‡ãƒ¼ã‚¿ã‚’S3DataLoaderã«è¨­å®šï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        monitor.s3_loader._test_previous_data = previous_data

        # çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        current_date = datetime.now().strftime("%Y%m%d")
        changes = monitor.monitor_changes(current_date, current_data)

        logger.info(f"çµ±åˆãƒ†ã‚¹ãƒˆçµæœ: {len(changes)}ä»¶ã®å¤‰åŒ–ã‚’æ¤œçŸ¥")

        # æŠ•ç¨¿å±¥æ­´ã‚’ç¢ºèª
        history = monitor.posting_manager.get_post_history(days=1)
        logger.info(f"ä»Šæ—¥ã®æŠ•ç¨¿æ•°: {len(history)}")

        logger.info("çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
        return True

    except Exception as e:
        logger.error(f"çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("SNSæŠ•ç¨¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")

    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    log_dir = project_root / "logs"
    log_dir.mkdir(exist_ok=True)

    # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    tests = [
        ("S3DataLoader", test_s3_data_loader),
        ("ContentGenerator", test_content_generator),
        ("PostingManager", test_posting_manager),
        ("BiasMonitor", test_bias_monitor),
        ("çµ±åˆãƒ†ã‚¹ãƒˆ", test_integration)
    ]

    results = {}

    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")
        logger.info(f"{'='*50}")

        try:
            result = test_func()
            results[test_name] = result
            status = "æˆåŠŸ" if result else "å¤±æ•—"
            logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆ: {status}")
        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã§ä¾‹å¤–ãŒç™ºç”Ÿ: {e}")
            results[test_name] = False

    # çµæœã‚µãƒãƒªãƒ¼
    logger.info(f"\n{'='*50}")
    logger.info("ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    logger.info(f"{'='*50}")

    for test_name, result in results.items():
        status = "æˆåŠŸ" if result else "å¤±æ•—"
        logger.info(f"{test_name}: {status}")

    success_count = sum(results.values())
    total_count = len(results)

    logger.info(f"\nç·åˆçµæœ: {success_count}/{total_count} ãƒ†ã‚¹ãƒˆæˆåŠŸ")

    if success_count == total_count:
        logger.info("ğŸ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
    else:
        logger.warning("âš ï¸ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")

    logger.info("SNSæŠ•ç¨¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆçµ‚äº†")


if __name__ == "__main__":
    main()