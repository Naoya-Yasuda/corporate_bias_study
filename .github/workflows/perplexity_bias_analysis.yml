name: AI Bias & Ranking Analysis (Weekly)

on:
  schedule:
    - cron: '0 21 * * 1'   # 毎週月曜日 06:00 JST (=21:00 UTC) に実行
  workflow_dispatch:  # 手動実行も可能
    inputs:
      runs_count:
        description: 'API実行回数'
        required: false
        default: '5'
        type: string

jobs:
  run-data-collection-analysis:
    runs-on: ubuntu-latest
    env:
      PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
      AWS_ACCESS_KEY: ${{ secrets.AWS_ACCESS_KEY }}
      AWS_SECRET_KEY: ${{ secrets.AWS_SECRET_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
      S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
      TODAY_DATE: $(date +%Y%m%d)
      # GitHubアクションのログ詳細度設定
      PYTHONUNBUFFERED: 1  # Pythonの出力バッファリングを無効化（リアルタイムログ出力）
      ACTIONS_STEP_DEBUG: true  # GitHubアクション詳細ログを有効化
      # 実行回数設定（デフォルトは5回、手動実行時には入力値を使用）
      RUNS_COUNT: ${{ github.event.inputs.runs_count || '5' }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-github-actions.txt
          # CloudWatch連携用のAWS SDKをインストール
          pip install boto3 watchtower
      - name: Add src to PYTHONPATH
        run: |
          echo "PYTHONPATH=$PYTHONPATH:$(pwd)" >> $GITHUB_ENV

      # 日付文字列の取得
      - name: Set today's date
        run: echo "TODAY_DATE=$(date +%Y%m%d)" >> $GITHUB_ENV

      # ログファイルの初期設定
      - name: Setup logging
        run: |
          mkdir -p logs
          echo "開始時刻: $(date)" > logs/execution.log
          echo "実行環境: GitHub Actions" >> logs/execution.log
          echo "日付: ${{ env.TODAY_DATE }}" >> logs/execution.log
          echo "実行回数: ${{ env.RUNS_COUNT }}" >> logs/execution.log
          echo "----------------------------------------" >> logs/execution.log

      # 1. データ取得ステップ
      - name: Run Perplexity sentiment data collection (${{ env.RUNS_COUNT }} runs)
        run: |
          echo "開始: Perplexity 感情スコアデータ取得 ($(date))" | tee -a logs/execution.log
          python -m src.perplexity_sentiment_loader --multiple --runs ${{ env.RUNS_COUNT }} --verbose | tee -a logs/perplexity_sentiment.log
          echo "完了: Perplexity 感情スコアデータ取得 ($(date))" | tee -a logs/execution.log

      - name: Run Perplexity ranking data collection (${{ env.RUNS_COUNT }} runs)
        run: |
          echo "開始: Perplexity ランキングデータ取得 ($(date))" | tee -a logs/execution.log
          python -m src.perplexity_ranking_loader --multiple --runs ${{ env.RUNS_COUNT }} --verbose | tee -a logs/perplexity_ranking.log
          echo "完了: Perplexity ランキングデータ取得 ($(date))" | tee -a logs/execution.log

      - name: Run Perplexity citations data collection (${{ env.RUNS_COUNT }} runs)
        run: |
          echo "開始: Perplexity 引用リンクデータ取得 ($(date))" | tee -a logs/execution.log
          python -m src.perplexity_citations_loader --multiple --runs ${{ env.RUNS_COUNT }} --verbose | tee -a logs/perplexity_citations.log
          echo "完了: Perplexity 引用リンクデータ取得 ($(date))" | tee -a logs/execution.log

      - name: Run Google SERP data collection
        run: |
          echo "開始: Google SERPデータ取得 ($(date))" | tee -a logs/execution.log
          python -m src.google_serp_loader --verbose | tee -a logs/google_serp.log
          echo "完了: Google SERPデータ取得 ($(date))" | tee -a logs/execution.log

      # 感情分析ステップ
      - name: Run Sentiment Analysis for Google SERP
        run: |
          echo "開始: Google SERP感情分析 ($(date))" | tee -a logs/execution.log
          python -m src.sentiment_analyzer --input-file results/google_serp/${{ env.TODAY_DATE }}/${{ env.TODAY_DATE }}_google_serp_results.json --date ${{ env.TODAY_DATE }} --verbose | tee -a logs/sentiment_analysis_google.log
          echo "完了: Google SERP感情分析 ($(date))" | tee -a logs/execution.log

      - name: Run Sentiment Analysis for Perplexity Citations
        run: |
          echo "開始: Perplexity Citations感情分析 ($(date))" | tee -a logs/execution.log
          python -m src.sentiment_analyzer --input-file results/perplexity_citations/${{ env.TODAY_DATE }}/${{ env.TODAY_DATE }}_perplexity_citations_${{ env.RUNS_COUNT }}runs.json --date ${{ env.TODAY_DATE }} --verbose | tee -a logs/sentiment_analysis_perplexity_citations.log
          echo "完了: Perplexity Citations感情分析 ($(date))" | tee -a logs/execution.log

      # 2. 分析ステップ
      # - name: Run Perplexity Sentiment Bias Analysis
      #   run: |
      #     echo "開始: Perplexity 感情スコア・バイアス指標分析 ($(date))" | tee -a logs/execution.log
      #     python -m src.analysis.bias_sentiment_metrics --date ${{ env.TODAY_DATE }} --rankings --verbose | tee -a logs/bias_metrics_perplexity.log
      #     echo "完了: Perplexity 感情スコア・バイアス指標分析 ($(date))" | tee -a logs/execution.log

      # - name: Run ranking metrics analysis
      #   run: |
      #     echo "開始: ランキング指標分析 ($(date))" | tee -a logs/execution.log
      #     python -m src.analysis.ranking_metrics --date ${{ env.TODAY_DATE }} --api perplexity --verbose | tee -a logs/ranking_metrics.log
      #     echo "完了: ランキング指標分析 ($(date))" | tee -a logs/execution.log

      # - name: Run SERP-Perplexity comparison analysis
      #   run: |
      #     echo "開始: Google SERPとPerplexityの比較分析 ($(date))" | tee -a logs/execution.log
      #     python src/analysis/serp_metrics.py --date ${{ env.TODAY_DATE }} --runs ${{ env.RUNS_COUNT }} --output results/perplexity_analysis/${{ env.TODAY_DATE }}
      #     echo "完了: Google SERPとPerplexityの比較分析 ($(date))" | tee -a logs/execution.log

      # # 統合指標分析（複数指標に基づく後段処理）
      # - name: Run integrated metrics analysis
      #   run: |
      #     echo "開始: 統合指標分析 ($(date))" | tee -a logs/execution.log
      #     python -m src.analysis.integrated_metrics --date ${{ env.TODAY_DATE }} --output results/integrated_metrics/${{ env.TODAY_DATE }} --verbose | tee -a logs/integrated_metrics.log
      #     echo "完了: 統合指標分析 ($(date))" | tee -a logs/execution.log

      # # 統合バイアス評価パイプライン
      # - name: Run bias ranking pipeline
      #   run: |
      #     echo "開始: 統合バイアス評価パイプライン ($(date))" | tee -a logs/execution.log
      #     # ランキングデータを使用した分析
      #     python -m src.analysis.bias_ranking_pipeline --data-type rankings --output results/bias_analysis/rankings --verbose | tee -a logs/bias_ranking_pipeline.log
      #     # 引用リンクデータを使用した分析
      #     python -m src.analysis.bias_ranking_pipeline --data-type citations --output results/bias_analysis/citations --verbose | tee -a logs/bias_ranking_pipeline.log
      #     echo "完了: 統合バイアス評価パイプライン ($(date))" | tee -a logs/execution.log
      #     echo "全プロセス完了: $(date)" | tee -a logs/execution.log

      # ログをCloudWatchにアップロード
      - name: Upload logs to CloudWatch
        if: ${{ env.AWS_ACCESS_KEY != '' && env.AWS_SECRET_KEY != '' }}
        run: |
          python -c "
          import boto3
          import os
          from datetime import datetime

          # AWS認証情報
          aws_region = os.environ.get('AWS_REGION', 'ap-northeast-1')
          # リージョンが空文字列の場合はデフォルト値を使用
          if not aws_region or aws_region.strip() == '':
              aws_region = 'ap-northeast-1'
              print(f'AWS_REGIONが未設定のため、デフォルト値を使用します: {aws_region}')

          aws_access_key = os.environ.get('AWS_ACCESS_KEY')
          aws_secret_key = os.environ.get('AWS_SECRET_KEY')

          # CloudWatchクライアント
          logs_client = boto3.client(
              'logs',
              region_name=aws_region,
              aws_access_key_id=aws_access_key,
              aws_secret_access_key=aws_secret_key
          )

          # ログループとストリームの設定
          log_group_name = '/corporate-bias-study/github-actions'
          log_stream_name = f'run-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'

          # ログストリームの作成（存在しない場合）
          try:
              logs_client.create_log_group(logGroupName=log_group_name)
              print(f'作成したロググループ: {log_group_name}')
          except logs_client.exceptions.ResourceAlreadyExistsException:
              print(f'ロググループは既に存在: {log_group_name}')

          # ログストリームの作成
          try:
              logs_client.create_log_stream(
                  logGroupName=log_group_name,
                  logStreamName=log_stream_name
              )
              print(f'作成したログストリーム: {log_stream_name}')
          except Exception as e:
              print(f'ログストリーム作成エラー: {e}')
              exit(1)

          # ログファイル一覧
          log_files = [f for f in os.listdir('logs') if f.endswith('.log')]

          # ログをCloudWatchにアップロード
          sequence_token = None
          for log_file in log_files:
              with open(f'logs/{log_file}', 'r') as f:
                  log_content = f.read()

                  events = [{
                      'timestamp': int(datetime.now().timestamp() * 1000),
                      'message': f'=== {log_file} ===\\n{log_content}'
                  }]

                  try:
                      if sequence_token:
                          response = logs_client.put_log_events(
                              logGroupName=log_group_name,
                              logStreamName=log_stream_name,
                              logEvents=events,
                              sequenceToken=sequence_token
                          )
                      else:
                          response = logs_client.put_log_events(
                              logGroupName=log_group_name,
                              logStreamName=log_stream_name,
                              logEvents=events
                          )

                      sequence_token = response.get('nextSequenceToken')
                      print(f'アップロード成功: {log_file}')

                  except Exception as e:
                      print(f'ログアップロードエラー: {e}')

          print('CloudWatchへのログアップロード完了')
          "

      # ログファイルもアーティファクトとして保存
      - name: Upload logs as artifact
        uses: actions/upload-artifact@v4
        with:
          name: execution_logs
          path: logs/
          retention-days: 7

      # 結果をアーティファクトとしてアップロード
      - name: Upload results as artifact
        uses: actions/upload-artifact@v4
        with:
          name: ai_bias_analysis_results
          path: results/
          retention-days: 7
